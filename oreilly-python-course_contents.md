# Repository Contents: oreilly-python-course

Generated: 2026-01-29 17:44:22

**Files included:** `.md`, `.py`, `.ipynb`

**Total files:** 93

---

## Table of Contents

1. [CLAUDE.md](#claude-md)
2. [INSTRUCTOR_NOTES.md](#instructor-notes-md)
3. [QUICK_START_WINDOWS.md](#quick-start-windows-md)
4. [README.md](#readme-md)
5. [WINDOWS_SETUP.md](#windows-setup-md)
6. [creating_aliases_workflow.md](#creating-aliases-workflow-md)
7. [notebooks/01-python-fundamentals/01-python-basics.ipynb](#notebooks-01-python-fundamentals-01-python-basics-ipynb)
8. [notebooks/01-python-fundamentals/02-working-with-data.ipynb](#notebooks-01-python-fundamentals-02-working-with-data-ipynb)
9. [notebooks/01-python-fundamentals/03-packages-apis.ipynb](#notebooks-01-python-fundamentals-03-packages-apis-ipynb)
10. [notebooks/01-python-fundamentals/Untitled.ipynb](#notebooks-01-python-fundamentals-untitled-ipynb)
11. [notebooks/01-python-fundamentals/Untitled1.ipynb](#notebooks-01-python-fundamentals-untitled1-ipynb)
12. [notebooks/01-python-fundamentals/live-demo-part1-2-3.ipynb](#notebooks-01-python-fundamentals-live-demo-part1-2-3-ipynb)
13. [notebooks/01-python-fundamentals/live-demo-python-basics.ipynb](#notebooks-01-python-fundamentals-live-demo-python-basics-ipynb)
14. [notebooks/01-python-fundamentals/summary_file.md](#notebooks-01-python-fundamentals-summary-file-md)
15. [notebooks/01-python-fundamentals/summary_report.md](#notebooks-01-python-fundamentals-summary-report-md)
16. [notebooks/02-ai-apis/01-ai-apis-overview.ipynb](#notebooks-02-ai-apis-01-ai-apis-overview-ipynb)
17. [notebooks/02-ai-apis/02-ai-tools-hands-on.ipynb](#notebooks-02-ai-apis-02-ai-tools-hands-on-ipynb)
18. [notebooks/02-ai-apis/live-demo-apis-automations.ipynb](#notebooks-02-ai-apis-live-demo-apis-automations-ipynb)
19. [notebooks/02-ai-apis/live_demo_summarize_files.py](#notebooks-02-ai-apis-live-demo-summarize-files-py)
20. [notebooks/03-automation-projects/0-simple-examples-python-snippets.ipynb](#notebooks-03-automation-projects-0-simple-examples-python-snippets-ipynb)
21. [notebooks/03-automation-projects/01-file-management-automation.ipynb](#notebooks-03-automation-projects-01-file-management-automation-ipynb)
22. [notebooks/03-automation-projects/02-data-extraction.ipynb](#notebooks-03-automation-projects-02-data-extraction-ipynb)
23. [notebooks/03-automation-projects/03-data-analysis.ipynb](#notebooks-03-automation-projects-03-data-analysis-ipynb)
24. [notebooks/04-exercises/01-data-types-and-variables.ipynb](#notebooks-04-exercises-01-data-types-and-variables-ipynb)
25. [notebooks/04-exercises/02-functions.ipynb](#notebooks-04-exercises-02-functions-ipynb)
26. [notebooks/04-exercises/03-conditionals-and-files.ipynb](#notebooks-04-exercises-03-conditionals-and-files-ipynb)
27. [notebooks/04-exercises/04-day1-recap.ipynb](#notebooks-04-exercises-04-day1-recap-ipynb)
28. [notebooks/04-exercises/tutorial_learning_python_with_pdfs.ipynb](#notebooks-04-exercises-tutorial-learning-python-with-pdfs-ipynb)
29. [notebooks/05-how-to-learn-python/file.md](#notebooks-05-how-to-learn-python-file-md)
30. [notebooks/05-how-to-learn-python/how-to-learn-python.ipynb](#notebooks-05-how-to-learn-python-how-to-learn-python-ipynb)
31. [notebooks/05-how-to-learn-python/learn-python-talking-to-ai.ipynb](#notebooks-05-how-to-learn-python-learn-python-talking-to-ai-ipynb)
32. [notebooks/assets/automation_recipes_prompts/financial-analysis-prompt.md](#notebooks-assets-automation-recipes-prompts-financial-analysis-prompt-md)
33. [notebooks/assets/automation_recipes_prompts/outloook-send-emails-prompt.md](#notebooks-assets-automation-recipes-prompts-outloook-send-emails-prompt-md)
34. [notebooks/assets/automation_recipes_prompts/pdf-tasks-prompts.md](#notebooks-assets-automation-recipes-prompts-pdf-tasks-prompts-md)
35. [notebooks/assets/automation_recipes_prompts/ppt-automation-prompt.md](#notebooks-assets-automation-recipes-prompts-ppt-automation-prompt-md)
36. [notebooks/assets/automation_recipes_prompts/prompt_learn_ai_python.md](#notebooks-assets-automation-recipes-prompts-prompt-learn-ai-python-md)
37. [notebooks/assets/automation_recipes_prompts/uv-one-shot-prompt-to-script.md](#notebooks-assets-automation-recipes-prompts-uv-one-shot-prompt-to-script-md)
38. [notebooks/assets/documents/ai-tools.md](#notebooks-assets-documents-ai-tools-md)
39. [notebooks/assets/documents/trouble_shooting_setups.md](#notebooks-assets-documents-trouble-shooting-setups-md)
40. [presentation/presentation_update.md](#presentation-presentation-update-md)
41. [scripts/06-browser-automation.md](#scripts-06-browser-automation-md)
42. [scripts/ai_stuff.py](#scripts-ai-stuff-py)
43. [scripts/ai_tools.py](#scripts-ai-tools-py)
44. [scripts/browser_automation_add_movie_to_watchlist_letterboxd.py](#scripts-browser-automation-add-movie-to-watchlist-letterboxd-py)
45. [scripts/chat_with_pdfs_extract.py](#scripts-chat-with-pdfs-extract-py)
46. [scripts/dashboard_guidelines_example.py](#scripts-dashboard-guidelines-example-py)
47. [scripts/data_analysis_stock.py](#scripts-data-analysis-stock-py)
48. [scripts/data_extraction_invoices.py](#scripts-data-extraction-invoices-py)
49. [scripts/data_extraction_receipt_data.py](#scripts-data-extraction-receipt-data-py)
50. [scripts/data_transformation_md_to_jupyter.py](#scripts-data-transformation-md-to-jupyter-py)
51. [scripts/demo_utils.py](#scripts-demo-utils-py)
52. [scripts/download_papers_from_arxiv.py](#scripts-download-papers-from-arxiv-py)
53. [scripts/extract_pdf_data.py](#scripts-extract-pdf-data-py)
54. [scripts/file_management_backup.py](#scripts-file-management-backup-py)
55. [scripts/file_management_dir_org_with_visuals.py](#scripts-file-management-dir-org-with-visuals-py)
56. [scripts/file_management_image_categorization_ai.py](#scripts-file-management-image-categorization-ai-py)
57. [scripts/generate_image_with_openai.py](#scripts-generate-image-with-openai-py)
58. [scripts/generate_ppt_from_paper.py](#scripts-generate-ppt-from-paper-py)
59. [scripts/generic_presentation_maker.py](#scripts-generic-presentation-maker-py)
60. [scripts/live_demo_organize_files.py](#scripts-live-demo-organize-files-py)
61. [scripts/paper_download_summarize.py](#scripts-paper-download-summarize-py)
62. [scripts/repo_to_markdown.py](#scripts-repo-to-markdown-py)
63. [scripts/repo_tree.py](#scripts-repo-tree-py)
64. [scripts/roster_app.py](#scripts-roster-app-py)
65. [scripts/roster_app_html.py](#scripts-roster-app-html-py)
66. [scripts/summarize_pdf_live_demo.py](#scripts-summarize-pdf-live-demo-py)
67. [scripts/additional-scripts/AH_news_summarizer.py](#scripts-additional-scripts-ah-news-summarizer-py)
68. [scripts/additional-scripts/BD_timesheet_automation.py](#scripts-additional-scripts-bd-timesheet-automation-py)
69. [scripts/additional-scripts/JG_api_data_storage.py](#scripts-additional-scripts-jg-api-data-storage-py)
70. [scripts/additional-scripts/LA_file_organizer.py](#scripts-additional-scripts-la-file-organizer-py)
71. [scripts/additional-scripts/LC_email_formatter.py](#scripts-additional-scripts-lc-email-formatter-py)
72. [scripts/additional-scripts/MM_email_subscriptions.py](#scripts-additional-scripts-mm-email-subscriptions-py)
73. [scripts/additional-scripts/MM_powerpoint_generator.py](#scripts-additional-scripts-mm-powerpoint-generator-py)
74. [scripts/additional-scripts/MS_dashboard_patterns.py](#scripts-additional-scripts-ms-dashboard-patterns-py)
75. [scripts/additional-scripts/PL_etl_automation.py](#scripts-additional-scripts-pl-etl-automation-py)
76. [scripts/additional-scripts/SW_email_filter_urgency.py](#scripts-additional-scripts-sw-email-filter-urgency-py)
77. [scripts/additional-scripts/TT_data_ingestion.py](#scripts-additional-scripts-tt-data-ingestion-py)
78. [scripts/additional-scripts/TW_doc_converter.py](#scripts-additional-scripts-tw-doc-converter-py)
79. [scripts/additional-scripts/VK_clinical_trials_parser.py](#scripts-additional-scripts-vk-clinical-trials-parser-py)
80. [scripts/additional-scripts/VK_log_analyzer.py](#scripts-additional-scripts-vk-log-analyzer-py)
81. [scripts/additional-scripts/VN_user_test_summarizer.py](#scripts-additional-scripts-vn-user-test-summarizer-py)
82. [scripts/additional-scripts/browser_automation_get_reviews_from_letterboxd.py](#scripts-additional-scripts-browser-automation-get-reviews-from-letterboxd-py)
83. [scripts/additional-scripts/browser_automation_imdb.py](#scripts-additional-scripts-browser-automation-imdb-py)
84. [scripts/additional-scripts/browser_automation_securing_login.py](#scripts-additional-scripts-browser-automation-securing-login-py)
85. [scripts/additional-scripts/data_extraction_pdf_example_live.py](#scripts-additional-scripts-data-extraction-pdf-example-live-py)
86. [scripts/additional-scripts/data_extraction_simple_pdf.py](#scripts-additional-scripts-data-extraction-simple-pdf-py)
87. [scripts/additional-scripts/generate_synthetic_excel_data.py](#scripts-additional-scripts-generate-synthetic-excel-data-py)
88. [scripts/additional-scripts/pdf_analysis.py](#scripts-additional-scripts-pdf-analysis-py)
89. [scripts/additional-scripts/slide_deck_automation_ppt_example.py](#scripts-additional-scripts-slide-deck-automation-ppt-example-py)
90. [scripts/additional-scripts/slide_deck_automation_ppt_financial.py](#scripts-additional-scripts-slide-deck-automation-ppt-financial-py)
91. [scripts/additional-scripts/live-demo/summarize_ai.py](#scripts-additional-scripts-live-demo-summarize-ai-py)
92. [scripts/additional-scripts/live-demo/vibe_script_test.py](#scripts-additional-scripts-live-demo-vibe-script-test-py)
93. [scripts/summaries/3f5ee243547dee91fbd053c1c4a845aa.md](#scripts-summaries-3f5ee243547dee91fbd053c1c4a845aa-md)

---

## CLAUDE.md

<details>
<summary>View CLAUDE.md</summary>

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an O'Reilly Live Training course repository for "Automate Tasks with Python + AI". It's an educational project focused on teaching Python automation using AI APIs, particularly OpenAI and Anthropic APIs.

## Development Setup

The project uses `uv` for dependency management. To set up the development environment:

```bash
uv sync
uv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=oreilly-automate-py
playwright install
```

## Common Commands

### Running Jupyter Lab
```bash
uv run --with jupyter jupyter lab
```

### Running Python scripts
```bash
uv run python main.py
uv run python scripts/<script_name>.py
```

### Installing Playwright (for web automation)
```bash
playwright install
```

## Project Structure

- `notebooks/` - Main course content organized in 4 sections:
  - `01-python-fundamentals/` - Core Python concepts
  - `02-ai-apis/` - Working with AI services
  - `03-automation-projects/` - Real-world automation projects
  - `04-exercises/` - Practice problems and solutions
- `scripts/` - Standalone Python automation scripts and utilities
- `assets/` - Supporting files, sample data, and resources
- `main.py` - Basic entry point (minimal functionality)

## Key Dependencies

- **AI Libraries**: `openai`, `anthropic`, `ollama`
- **Data Science**: `pandas`, `matplotlib`, `numpy`
- **Web Scraping**: `requests`, `beautifulsoup4`, `playwright`
- **Jupyter**: `jupyterlab`, `ipykernel`, `ipywidgets`

## Architecture Notes

### AI Tools Module
The `scripts/ai_tools.py` module provides a unified interface for interacting with different AI providers:
- `ask_ai()` - Works with OpenAI (default) and Anthropic models
- `ask_local_ai()` - Works with local Ollama models
- Automatically handles API client initialization

### Demo Utilities
The `scripts/demo_utils.py` provides Jupyter notebook display utilities:
- `display_chat_message()` - Formats chat messages for notebook display
- `display_comparison()` - Shows side-by-side AI responses

### Environment Configuration
- Copy `.env.example` to `.env` and add API keys:
  ```
  OPENAI_API_KEY=<your openai api key>
  ANTHROPIC_API_KEY=<your claude api key>
  ```

## Working with Notebooks

- Notebooks are self-contained but build on previous concepts
- Run notebooks in numerical order within each section
- The custom kernel `oreilly-automate-py` should be used for consistent environment
- Sample data and resources are in the `assets/` folder

## Testing and Quality

This is an educational project without formal testing infrastructure. When making changes:
- Test scripts individually using `uv run python scripts/<script_name>.py`
- Verify notebooks run end-to-end in Jupyter Lab
- Ensure API keys are properly configured for AI-related functionality

</details>

---

## INSTRUCTOR_NOTES.md

<details>
<summary>View INSTRUCTOR_NOTES.md</summary>

# Instructor Notes - Windows Setup Resources

## What's Been Created

I've created a comprehensive Windows setup system for your students to address common issues:

### 1. **WINDOWS_SETUP.md** - Complete Beginner's Guide
   - **Purpose**: Detailed step-by-step instructions for absolute beginners
   - **Length**: ~30 minutes to read and follow
   - **Covers**:
     - Python installation with PATH setup
     - Git installation with proper settings
     - UV installation with firewall/antivirus handling
     - Complete troubleshooting section for every common error
   - **Use when**: Students are setting up for the first time or having issues

### 2. **setup-windows.ps1** - Automated Setup Script
   - **Purpose**: One-click setup that handles most common issues
   - **Features**:
     - Checks prerequisites (Python, Git)
     - Auto-installs UV if missing
     - Provides helpful error messages with solutions
     - Handles all setup steps automatically
   - **Use when**: Students want the easiest path
   - **Run with**: `.\setup-windows.ps1`

### 3. **diagnose-setup.ps1** - Diagnostic Tool
   - **Purpose**: Identifies what's wrong with a broken setup
   - **Checks**:
     - Python version and installation
     - Git availability
     - UV installation and PATH
     - Virtual environment status
     - Network connectivity
     - Firewall issues
   - **Use when**: Students report "it's not working" but you need specifics
   - **Run with**: `.\diagnose-setup.ps1`

### 4. **QUICK_START_WINDOWS.md** - 5-Minute Reference
   - **Purpose**: Quick reference card for experienced users
   - **Length**: Can be skimmed in 1-2 minutes
   - **Use when**: Students just need a command reminder

### 5. **Updated README.md** - Main Entry Point
   - Now prominently points Windows users to these resources
   - Clear distinction between automated and manual setup
   - Links to diagnostic tool

---

## Common Student Issues & Solutions

### Issue 1: "UV not found" (Most Common)
**Why it happens**:
- Students don't restart terminal after UV install
- UV installer didn't add to PATH due to permissions
- Corporate antivirus blocked the installer

**Solutions provided**:
- Setup script checks this and provides clear instructions
- Diagnostic script identifies if UV exists but isn't in PATH
- WINDOWS_SETUP.md has step-by-step PATH addition instructions

### Issue 2: Firewall Blocking Downloads
**Why it happens**:
- Corporate/school networks block Python package downloads
- Windows Defender flags unknown installers
- Antivirus software blocks file creation in project folder

**Solutions provided**:
- Setup script warns about this upfront
- Detailed firewall configuration in WINDOWS_SETUP.md
- Diagnostic script tests connectivity to PyPI, GitHub, etc.

### Issue 3: Wrong Python Version
**Why it happens**:
- Students have Python 2.7 or 3.8 from old installations
- Multiple Python versions creating PATH conflicts
- Python installed but "Add to PATH" wasn't checked

**Solutions provided**:
- Setup script checks version before proceeding
- Clear version check instructions in guide
- Diagnostic script identifies version issues

### Issue 4: PowerShell Execution Policy
**Why it happens**:
- Default Windows security blocks running downloaded scripts
- Students try to run .ps1 files without enabling scripts

**Solutions provided**:
- Setup script uses ByPass for UV installation
- WINDOWS_SETUP.md includes execution policy setup
- Diagnostic script checks current policy

### Issue 5: UV Sync Appears Frozen
**Why it happens**:
- Large packages (numpy, pandas) take time to compile on Windows
- Slow internet makes it seem frozen
- Students don't realize it's normal to take 5-10 minutes

**Solutions provided**:
- Setup script shows progress expectations ("This takes 3-10 minutes...")
- WINDOWS_SETUP.md explains what to expect
- Troubleshooting section on when to actually worry

---

## Recommended Student Flow

### For Beginners (Most Students)
1. **Pre-class**: Email link to WINDOWS_SETUP.md
2. **Start here**: Run `setup-windows.ps1`
3. **If issues**: Check WINDOWS_SETUP.md troubleshooting
4. **Still stuck**: Run `diagnose-setup.ps1` and share output

### For Experienced Users
1. Point to QUICK_START_WINDOWS.md
2. They can use manual commands

### For Problem-Solving During Class
1. Student reports issue
2. Ask them to run: `.\diagnose-setup.ps1`
3. They share the output (screenshot or copy/paste)
4. You immediately see what's wrong (Python version, missing UV, etc.)

---

## Pre-Class Email Template

Subject: Setting Up Your Python Environment - Please Do Before Class

Hi everyone,

Before our O'Reilly Python course, please set up your environment:

**Windows Users (3 options - choose one):**

**Option 1 - Easiest (Recommended):**
1. Install Python 3.13: https://www.python.org/downloads/ (CHECK "Add to PATH"!)
2. Install Git: https://git-scm.com/download/win
3. Clone the repo: `git clone https://github.com/EnkrateiaLucca/oreilly-python-course`
4. Run our setup script: `.\setup-windows.ps1`

**Option 2 - Step-by-step guide:**
Follow WINDOWS_SETUP.md in the repository (great for beginners)

**Option 3 - Quick start:**
See QUICK_START_WINDOWS.md if you're comfortable with command line

**Mac/Linux Users:**
Follow the README.md instructions

**Having problems?**
1. Run `.\diagnose-setup.ps1` to identify issues
2. Check the troubleshooting section in WINDOWS_SETUP.md
3. Email me the output from diagnose-setup.ps1 if still stuck

**To verify it worked:**
Run `uv run --with jupyter jupyter lab` - if Jupyter opens in your browser, you're ready!

See you in class!

---

## During-Class Support Strategy

### If someone has setup issues during class:

**Quick fix attempt (2 minutes):**
```powershell
.\diagnose-setup.ps1
```
This immediately shows what's wrong.

**If it's a quick fix** (UV not in PATH, wrong kernel, etc.):
- Reference the specific section in WINDOWS_SETUP.md
- Or run the setup script: `.\setup-windows.ps1`

**If it's a complex issue** (Python not installed, major firewall issues):
- Suggest they pair with someone who has it working
- Provide cloud environment alternative (Google Colab, etc.)
- Help them fix it during a break

---

## Files Reference

| File | Size | Use Case |
|------|------|----------|
| WINDOWS_SETUP.md | ~15 pages | Complete reference, troubleshooting |
| setup-windows.ps1 | Automated | One-click setup |
| diagnose-setup.ps1 | Diagnostic | Problem identification |
| QUICK_START_WINDOWS.md | 1 page | Quick reference |
| README.md | Updated | Main entry point |

---

## Testing the Scripts

Before distributing to students, test in a Windows VM:

1. **Fresh Windows 10/11 VM** (or test machine)
2. Run `setup-windows.ps1`
3. Verify it handles:
   - Missing Python gracefully
   - UV installation
   - Firewall prompts
4. Test `diagnose-setup.ps1` on:
   - Clean install (should pass all checks)
   - Broken install (remove UV from PATH, etc.)

---

## Maintenance Notes

**Before each course:**
- Update Python version requirements in all docs if needed (currently 3.13+)
- Test setup script on fresh Windows install
- Check if UV installation URL changed

**If pyproject.toml changes:**
- Setup scripts don't need updates (they just run `uv sync`)
- But verify new dependencies don't cause Windows-specific issues

---

## Additional Tips

**Zoom/Screen Sharing:**
- If helping students remotely, ask them to share diagnostic output first
- Much faster than watching them navigate settings

**Common Quick Wins:**
- "Close terminal and open new one" solves 30% of PATH issues
- "Run PowerShell as Admin" solves most permission issues
- "Turn off VPN" sometimes needed for package downloads

**Time Estimates:**
- Clean install with good internet: 10-15 minutes
- With troubleshooting: 20-30 minutes
- With major issues (firewall, no Python): 45+ minutes

**Fallback Options:**
If a student can't get local setup working:
- Google Colab (cloud-based Jupyter)
- Replit (cloud-based Python environment)
- Pair programming with working setup

---

Good luck with your course! These resources should handle 90%+ of Windows setup issues.


</details>

---

## QUICK_START_WINDOWS.md

<details>
<summary>View QUICK_START_WINDOWS.md</summary>

# Windows Quick Start - 5 Minute Setup

**New to this?** See [WINDOWS_SETUP.md](WINDOWS_SETUP.md) for detailed instructions.

## Prerequisites

‚úÖ Python 3.13+ installed ‚Üí [Download](https://www.python.org/downloads/)
‚úÖ Git installed ‚Üí [Download](https://git-scm.com/download/win)
‚úÖ Internet connection

## Setup Steps

### Option A: Automated (Recommended)

Open PowerShell and run:

```powershell
# Clone the project
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course

# Run setup script (handles everything)
.\setup-windows.ps1
```

**That's it!** The script will install UV, sync packages, and set everything up.

---

### Option B: Manual

```powershell
# 1. Install UV
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# 2. Close PowerShell and open a NEW one

# 3. Clone project
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course

# 4. Install packages (takes 5-10 min)
uv sync

# 5. Setup Jupyter
uv run ipython kernel install --user --env VIRTUAL_ENV "$PWD\.venv" --name=oreilly-automate-py

# 6. Install browsers (optional)
uv run playwright install
```

---

## Start Learning

```powershell
uv run --with jupyter jupyter lab
```

Opens in your browser automatically!

---

## Setup API Keys

1. Copy `.env.example` ‚Üí `.env`
2. Get keys:
   - OpenAI: https://platform.openai.com/
   - Anthropic: https://console.anthropic.com/
3. Add them to `.env`

---

## Common Issues

### "Command not found" errors
‚Üí Close terminal and open a **NEW** one

### UV won't install
‚Üí Run PowerShell **as Administrator**
‚Üí Run: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

### Antivirus blocking
‚Üí Temporarily disable during setup
‚Üí Or add project folder to exceptions

### Setup taking too long
‚Üí Normal! Package installation takes 5-10 minutes
‚Üí See progress messages to confirm it's working

### Still stuck?
```powershell
.\diagnose-setup.ps1  # Identifies issues
```

---

## Daily Usage

Start working:
```powershell
cd oreilly-python-course
uv run --with jupyter jupyter lab
```

That's it!

---

**Need more help?** ‚Üí [WINDOWS_SETUP.md](WINDOWS_SETUP.md)
**Report issues** ‚Üí https://github.com/EnkrateiaLucca/oreilly-python-course/issues


</details>

---

## README.md

<details>
<summary>View README.md</summary>

# O'Reilly Live Training - Automate Tasks with Python + AI 

## Quick Setup

### 1. Install UV
**Linux/macOS:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows:**

> **ü™ü Windows Users:** For complete beginners or if you encounter any issues, see **[WINDOWS_SETUP.md](WINDOWS_SETUP.md)** for a comprehensive step-by-step guide with troubleshooting.

**Automated Setup (Recommended):**
```powershell
# 1. Clone the repository
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course

# 2. Run the automated setup script
.\setup-windows.ps1
```

**Manual Setup:**
```powershell
# Install UV first
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Then run these commands
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course
uv sync
uv run ipython kernel install --user --env VIRTUAL_ENV "$PWD\.venv" --name=oreilly-automate-py
uv run playwright install
```

**Having Issues?** Run the diagnostic tool:
```powershell
.\diagnose-setup.ps1
```

### 2. Clone and Setup Project (Linux/macOS)

```bash
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course
uv sync
source .venv/bin/activate
uv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=oreilly-automate-py
playwright install
echo "‚úÖ Setup complete! To execute the jupyter environment for the interactive notebooks run:"
uv run --with jupyter jupyter lab
```

### 3. Start Jupyter Lab
```bash
# Make sure you're in the project directory
uv run --with jupyter jupyter lab
```

## API Setup

### Get your API keys:
1. OpenAI [API key](https://platform.openai.com/)
2. Anthropic [API key](https://docs.anthropic.com/en/docs/get-started)

### Setup your .env file
Change the `.env.example` file to `.env` and add your API keys:

```bash
OPENAI_API_KEY=<your openai api key>
ANTHROPIC_API_KEY=<your claude api key>
```

## What's Included
Dependencies installed:
- **AI Libraries:** openai, anthropic, ollama
- **Data Science:** pandas, matplotlib
- **Web Scraping:** requests, beautifulsoup4, playwright
- **Jupyter:** jupyterlab, ipykernel, ipywidgets

# Course Notebooks

This directory contains all the Jupyter notebooks for the O'Reilly Live Training course organized into logical sections.

## Structure

### üìö notebooks/01-python-fundamentals/
Core Python concepts and syntax:
- **01-python-basics.ipynb** ‚Äì Python basics: numbers, strings, variables, operators
- **02-working-with-data.ipynb** ‚Äì Lists, dictionaries, file I/O, CSV handling
- **03-packages-apis.ipynb** ‚Äì Using libraries and working with APIs
- **file.txt** ‚Äì Sample text file for file operations
- **places_to_go.csv** ‚Äì Example CSV data for exercises

### ü§ñ 02-ai-apis/
Working with AI services:
- **01-ai-apis-overview.ipynb** - Introduction to AI APIs
- **02-ai-tools-hands-on.ipynb** - Practical AI tool usage

### ‚öôÔ∏è 03-automation-projects/
Real-world automation projects:
- **01-file-management-automation.ipynb** - Organizing and managing files
- **02-data-extraction-with-ai.ipynb** - Extracting data using LLMs
- **03-web-data-extraction.ipynb** - Web scraping and data collection
- **04-data-analysis-automation.ipynb** - Automated data analysis
- **05-presentation-automation.ipynb** - Generating slides automatically
- **06-browser-automation.ipynb** - Controlling web browsers
- **07-workflow-automation.ipynb** - Building automation workflows
- **08-email-assistant.ipynb** - Email automation and processing
- **09-ai-scheduler-agent.ipynb** - Intelligent scheduling systems
- **10-receipt-data-extraction.ipynb** - Processing receipts and invoices
- **11-custom-automation-scripts.ipynb** - Building your own scripts
- **12-practical-examples.ipynb** - Additional real-world examples

### üìù 04-exercises/
Practice problems and solutions:
- **01-data-types-and-variables.ipynb** - Basic Python practice
- **02-functions.ipynb** - Function creation exercises
- **03-conditionals-and-files.ipynb** - Logic and file handling
- **04-day1-recap.ipynb** - Summary and review

### üìÅ assets/
Supporting files, images, sample data, and resources used throughout the course.

## Getting Started

1. Navigate to `01-python-fundamentals/` if you're new to Python
2. Work through notebooks in numerical order within each section
3. Use `04-exercises/` to practice what you've learned
4. Move to `02-ai-apis/` and `03-automation-projects/` for advanced topics

## Tips

- Each notebook is self-contained but builds on previous concepts
- Sample data and resources are in the `assets/` folder
- Run notebooks in order for the best learning experience

</details>

---

## WINDOWS_SETUP.md

<details>
<summary>View WINDOWS_SETUP.md</summary>

# Windows Setup Guide - Complete Beginner's Edition

This guide will walk you through setting up the O'Reilly Python course on Windows, step-by-step. Don't worry if you're new to this - we'll cover everything!

## üìã Table of Contents
- [Prerequisites](#prerequisites)
- [Step 1: Install Python](#step-1-install-python)
- [Step 2: Install Git](#step-2-install-git)
- [Step 3: Install UV Package Manager](#step-3-install-uv-package-manager)
- [Step 4: Clone and Setup Project](#step-4-clone-and-setup-project)
- [Step 5: Install Playwright Browsers](#step-5-install-playwright-browsers)
- [Step 6: Setup API Keys](#step-6-setup-api-keys)
- [Step 7: Launch Jupyter Lab](#step-7-launch-jupyter-lab)
- [Common Issues & Solutions](#common-issues--solutions)

---

## Prerequisites

### What You'll Need
- A Windows 10 or Windows 11 computer
- Administrator access to your computer
- An internet connection
- About 30 minutes of time

### Important Notes Before Starting
- **Antivirus/Firewall**: You may need to temporarily allow installations or add exceptions for Python, UV, and Git
- **Windows Defender**: May flag installers as unknown - this is normal for development tools
- **Execution Policy**: Some commands require changing PowerShell execution policy (we'll cover this)

---

## Step 1: Install Python

### 1.1 Check If Python Is Already Installed

1. Press `Windows Key + R`
2. Type `cmd` and press Enter
3. In the black window that appears, type:
   ```cmd
   python --version
   ```
4. If you see `Python 3.13.x` or higher, **skip to Step 2**
5. If you see an error or a version lower than 3.13, continue below

### 1.2 Download Python 3.13

1. Go to: https://www.python.org/downloads/
2. Click the yellow "Download Python 3.13.x" button
3. **Important**: When the installer runs:
   - ‚úÖ **CHECK** the box that says "Add Python to PATH" (at the bottom)
   - Click "Install Now"
   - If asked for Administrator permission, click "Yes"

### 1.3 Verify Python Installation

1. **Close any open Command Prompt/PowerShell windows** (this is important!)
2. Press `Windows Key + R`
3. Type `cmd` and press Enter
4. Type:
   ```cmd
   python --version
   ```
5. You should see: `Python 3.13.x`

**‚úÖ Checkpoint**: If you see the Python version, you're ready for Step 2!

**‚ùå Troubleshooting**: If it says "Python is not recognized":
- Make sure you closed all old terminal windows
- Restart your computer
- If still not working, see [Common Issues](#python-not-found-error)

---

## Step 2: Install Git

### 2.1 Check If Git Is Already Installed

1. Press `Windows Key + R`
2. Type `cmd` and press Enter
3. Type:
   ```cmd
   git --version
   ```
4. If you see `git version x.x.x`, **skip to Step 3**
5. If you see an error, continue below

### 2.2 Download Git

1. Go to: https://git-scm.com/download/win
2. The download should start automatically
3. Run the installer (Git-2.x.x-64-bit.exe)

### 2.3 Git Installation Settings

**Most users can just click "Next" through all options.** Here are the key screens:

1. **Select Components**: Keep defaults ‚úÖ
2. **Default Editor**: Choose "Use Notepad" (or your preferred editor)
3. **PATH Environment**: Choose "Git from the command line and also from 3rd-party software" ‚úÖ
4. **Line Ending**: Choose "Checkout Windows-style, commit Unix-style" ‚úÖ
5. **Terminal Emulator**: Choose "Use Windows' default console window" ‚úÖ
6. Continue clicking "Next" and then "Install"

### 2.4 Verify Git Installation

1. **Close any open Command Prompt/PowerShell windows**
2. Press `Windows Key + R`
3. Type `cmd` and press Enter
4. Type:
   ```cmd
   git --version
   ```
5. You should see: `git version x.x.x`

**‚úÖ Checkpoint**: If you see the Git version, you're ready for Step 3!

---

## Step 3: Install UV Package Manager

This is where students commonly face issues. Let's go through it carefully.

### 3.1 Open PowerShell as Administrator

**Method 1** (Recommended):
1. Press `Windows Key`
2. Type `PowerShell`
3. Right-click on "Windows PowerShell"
4. Click "Run as administrator"
5. Click "Yes" if asked for permission

**Method 2**:
1. Press `Windows Key + X`
2. Select "Windows PowerShell (Admin)" or "Terminal (Admin)"

### 3.2 Allow Script Execution (May be required)

If you get an error about execution policy later, run this command first:

```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

When asked "Do you want to change the execution policy?", type `Y` and press Enter.

**What this does**: Allows you to run downloaded PowerShell scripts (required for UV installation)

### 3.3 Install UV

In the same PowerShell window (still as Administrator), run:

```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**What to expect**:
- You'll see text downloading and installing UV
- Takes about 30 seconds to 2 minutes
- You should see a message about UV being installed successfully

### 3.4 Handle Firewall/Antivirus Warnings

**Windows Defender SmartScreen**:
- If you see "Windows protected your PC":
  - Click "More info"
  - Click "Run anyway"

**Antivirus Software**:
- If your antivirus blocks the download:
  - Temporarily disable it, OR
  - Add an exception for the UV installer
  - You can re-enable it after installation

### 3.5 Restart PowerShell

**Important**: Close the PowerShell window and open a new one (you can use regular PowerShell now, doesn't need to be Admin)

### 3.6 Verify UV Installation

In a **new** PowerShell window:

```powershell
uv --version
```

You should see: `uv x.x.x`

**‚úÖ Checkpoint**: If you see the UV version, you're ready for Step 4!

**‚ùå Troubleshooting**: If it says "uv is not recognized":
- See [UV Not Found Error](#uv-not-found-error) in Common Issues
- May need to add UV to PATH manually (covered in troubleshooting)

---

## Step 4: Clone and Setup Project

### 4.1 Choose Your Project Location

1. Open File Explorer
2. Navigate to where you want the course folder (e.g., `C:\Users\YourName\Documents\`)
3. Right-click in the folder ‚Üí "Open in Terminal" or "Open PowerShell window here"
   - If you don't see this option: Hold Shift, then right-click ‚Üí "Open PowerShell window here"

### 4.2 Clone the Repository

In the PowerShell window at your chosen location:

```powershell
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
```

**What to expect**:
- You'll see: "Cloning into 'oreilly-python-course'..."
- Progress bars showing the download
- Takes about 1-3 minutes depending on internet speed

### 4.3 Navigate to Project Folder

```powershell
cd oreilly-python-course
```

### 4.4 Sync Dependencies with UV

**This is the main setup step - it will take several minutes:**

```powershell
uv sync
```

**What to expect**:
- UV will create a virtual environment (.venv folder)
- Download and install all Python packages (50+ packages!)
- Takes 3-10 minutes depending on internet speed
- You'll see lots of "Downloading..." and "Installing..." messages

**‚ö†Ô∏è Common Issues During Sync**:

**If you see "Failed to download" errors**:
- Check your internet connection
- If behind a corporate firewall, you may need to configure proxy settings
- Try again - sometimes downloads fail temporarily

**If you see "Permission denied" errors**:
- Your antivirus might be blocking file creation
- Try temporarily disabling antivirus
- Or add the `oreilly-python-course` folder to antivirus exceptions

**If UV seems stuck**:
- Wait 5-10 minutes - large packages like numpy/pandas take time
- You should see progress messages periodically
- If truly frozen (no messages for 10+ minutes), press `Ctrl + C` and try again

### 4.5 Install Jupyter Kernel

This creates a custom Python kernel for the notebooks:

```powershell
uv run ipython kernel install --user --env VIRTUAL_ENV "$PWD\.venv" --name=oreilly-automate-py
```

**What to expect**:
- You'll see: "Installed kernelspec oreilly-automate-py in..."
- This registers the environment with Jupyter

**‚úÖ Checkpoint**: If you see the "Installed kernelspec" message, continue to Step 5!

---

## Step 5: Install Playwright Browsers

Playwright is used for browser automation. It needs to download browser binaries.

### 5.1 Install Browsers

```powershell
uv run playwright install
```

**What to expect**:
- Downloads Chromium, Firefox, and WebKit browsers
- Takes 2-5 minutes
- Downloads about 400-500 MB

**‚ö†Ô∏è If you see "Access denied" or permission errors**:
- Run PowerShell as Administrator for this step
- Or add exception in antivirus

**‚ö†Ô∏è If download fails**:
- Check internet connection
- Check firewall settings
- Try running: `uv run playwright install chromium` (just Chrome browser)

**‚úÖ Checkpoint**: Setup is almost complete! Just need API keys and you're ready.

---

## Step 6: Setup API Keys

The course uses OpenAI and Anthropic AI services. You'll need API keys.

### 6.1 Get API Keys

**OpenAI**:
1. Go to: https://platform.openai.com/
2. Sign up or log in
3. Click on your profile ‚Üí "View API keys"
4. Click "Create new secret key"
5. **Copy the key immediately** (you won't see it again!)

**Anthropic (Claude)**:
1. Go to: https://console.anthropic.com/
2. Sign up or log in
3. Go to "API Keys"
4. Click "Create Key"
5. **Copy the key immediately**

### 6.2 Create .env File

1. In the `oreilly-python-course` folder, find the file `.env.example`
2. **Make a copy** of this file and rename it to `.env` (no .example)
   - **Note**: It's just `.env` - no name before the dot
3. Open `.env` in Notepad or any text editor
4. Replace the placeholder text with your actual keys:

```
OPENAI_API_KEY=your-actual-openai-key-here
ANTHROPIC_API_KEY=your-actual-anthropic-key-here
```

5. Save and close the file

**‚ö†Ô∏è Important**:
- Never share your .env file
- Never commit it to git (it's already in .gitignore)
- Keep your API keys secret

---

## Step 7: Launch Jupyter Lab

You're ready to start the course!

### 7.1 Start Jupyter Lab

From the PowerShell window in your `oreilly-python-course` folder:

```powershell
uv run --with jupyter jupyter lab
```

**What to expect**:
- You'll see several startup messages
- Your default web browser will open automatically
- You'll see the Jupyter Lab interface with all course notebooks

### 7.2 Verify Everything Works

In Jupyter Lab:
1. Navigate to `notebooks/01-python-fundamentals/`
2. Open `01-python-basics.ipynb`
3. In the top-right, verify the kernel says "oreilly-automate-py"
4. Run the first cell (Shift + Enter)

**‚úÖ Success!** If the cell runs without errors, you're all set up!

---

## Common Issues & Solutions

### Python Not Found Error

**Symptom**: `'python' is not recognized as an internal or external command`

**Solutions**:
1. Close all terminal windows and try again
2. Restart your computer
3. Manually add Python to PATH:
   - Press `Windows Key + R`
   - Type `sysdm.cpl` and press Enter
   - Click "Environment Variables"
   - Under "User variables", find "Path", click "Edit"
   - Click "New" and add: `C:\Users\YourUsername\AppData\Local\Programs\Python\Python313\`
   - Click "New" again and add: `C:\Users\YourUsername\AppData\Local\Programs\Python\Python313\Scripts\`
   - Click OK on all windows
   - Restart terminal

### UV Not Found Error

**Symptom**: `'uv' is not recognized as an internal or external command`

**Solutions**:
1. Close and reopen PowerShell (UV requires a fresh terminal)
2. Check if UV actually installed:
   - Look for: `C:\Users\YourUsername\.cargo\bin\uv.exe`
3. If UV exists but not recognized, add to PATH:
   - Press `Windows Key + R`
   - Type `sysdm.cpl` and press Enter
   - Click "Environment Variables"
   - Under "User variables", find "Path", click "Edit"
   - Click "New" and add: `C:\Users\YourUsername\.cargo\bin\`
   - Click OK, restart terminal
4. If UV doesn't exist at all, try alternative installation:
   ```powershell
   # Using winget (Windows 11)
   winget install --id=astral-sh.uv -e
   ```

### Firewall Blocking Downloads

**Symptom**: Downloads fail, "Connection timed out", "Access denied"

**Solutions**:
1. **Windows Defender Firewall**:
   - Press `Windows Key`
   - Type "Firewall"
   - Click "Windows Defender Firewall"
   - Click "Allow an app through firewall"
   - Find Python, UV, and Git - make sure both Private and Public are checked
   - If not listed, click "Allow another app" and browse to add them

2. **Corporate/School Firewall**:
   - May need to use a personal network
   - Or contact IT to allow astral.sh, github.com, python.org

3. **Antivirus Software** (Norton, McAfee, Avast, etc.):
   - Temporarily disable during installation
   - Or add exceptions for:
     - `C:\Users\YourUsername\.cargo\`
     - Your project folder
     - Python installation folder

### Execution Policy Error

**Symptom**: "cannot be loaded because running scripts is disabled on this system"

**Solution**:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

Type `Y` when asked to confirm.

### UV Sync Taking Forever

**Symptom**: `uv sync` seems frozen

**Solutions**:
1. Wait patiently - large packages take time (numpy, pandas can be 50+ MB each)
2. Check if progress messages are appearing every minute or so
3. If truly frozen (10+ minutes with no output):
   - Press `Ctrl + C`
   - Delete the `.venv` folder
   - Run `uv sync` again
4. Try verbose mode to see what's happening:
   ```powershell
   uv sync -v
   ```

### Jupyter Lab Won't Open

**Symptom**: Browser doesn't open, or shows "Connection refused"

**Solutions**:
1. Check if the terminal shows errors
2. Try a different browser
3. Manually open browser and go to the URL shown in terminal (usually http://localhost:8888)
4. Check if another Jupyter instance is running:
   ```powershell
   # Stop all Jupyter processes
   taskkill /f /im jupyter-lab.exe
   # Try again
   uv run --with jupyter jupyter lab
   ```

### Wrong Kernel in Jupyter

**Symptom**: Notebooks use wrong Python kernel

**Solutions**:
1. In Jupyter Lab, click the kernel name (top-right)
2. Click "Change kernel"
3. Select "oreilly-automate-py"
4. If not listed, go back to Step 4.5 and reinstall the kernel

### Import Errors in Notebooks

**Symptom**: `ModuleNotFoundError: No module named 'anthropic'` (or other modules)

**Solutions**:
1. Make sure you're using the "oreilly-automate-py" kernel
2. Verify `uv sync` completed successfully
3. Try reinstalling dependencies:
   ```powershell
   uv sync --reinstall
   ```

---

## Need More Help?

1. **Course Issues**: Open an issue at https://github.com/EnkrateiaLucca/oreilly-python-course/issues
2. **UV Issues**: Check UV docs at https://docs.astral.sh/uv/
3. **Python Issues**: Check Python docs at https://docs.python.org/3/

---

## Quick Reference

**Start working on the course**:
```powershell
cd path\to\oreilly-python-course
uv run --with jupyter jupyter lab
```

**Update dependencies**:
```powershell
uv sync
```

**Reinstall everything**:
```powershell
# Delete .venv folder first
uv sync
uv run ipython kernel install --user --env VIRTUAL_ENV "$PWD\.venv" --name=oreilly-automate-py
uv run playwright install
```

---

**üéâ Congratulations!** You've successfully set up your Python development environment. Time to start learning!


</details>

---

## creating_aliases_workflow.md

<details>
<summary>View creating_aliases_workflow.md</summary>

## Mac Workflow (your current approach)

1. **Python script** lives at a path (e.g., `~/scripts/my_script.py`)
2. **Command to run**: `uv run ~/scripts/my_script.py`
3. **Accept arguments**: `uv run ~/scripts/my_script.py "$@"` passes all args
4. **Create alias** in `~/.aliases`:
   ```bash
   alias myscript='uv run ~/scripts/my_script.py'
   ```
   Or as a function for arguments:
   ```bash
   myscript() { uv run ~/scripts/my_script.py "$@"; }
   ```
5. **Source it** from `.zshrc` or `.bashrc`: `source ~/.aliases`

---

## Windows Equivalent

1. **Python script** lives at a path (e.g., `C:\scripts\my_script.py`)

2. **Command to run**: `uv run C:\scripts\my_script.py`

3. **Create a PowerShell function** in your profile:
   - Open/create your profile: `notepad $PROFILE`
   - Add a function:
     ```powershell
     function myscript { uv run C:\scripts\my_script.py @args }
     ```
   - `@args` passes all arguments through (equivalent to `"$@"`)

4. **Reload profile**: `. $PROFILE` or restart terminal

5. **(Optional) Organize aliases**: Create a separate file like `C:\Users\<you>\aliases.ps1`, then source it from `$PROFILE`:
   ```powershell
   . "$HOME\aliases.ps1"
   ```

---

**Alternative (batch files)**: Create `myscript.cmd` in a folder that's in your PATH:
```cmd
@echo off
uv run C:\scripts\my_script.py %*
```
`%*` passes all arguments.

</details>

---

## notebooks/01-python-fundamentals/01-python-basics.ipynb

# Python Basics: Complete Foundation

This comprehensive notebook covers all the fundamental concepts of Python programming, including data types, variables, functions, lists, dictionaries, comparisons, and conditionals.

## Part 1: Data Types and Operations

In this section, we'll explore Python's fundamental data types and the operations we can perform with them.

### Basic Data Types

Let's start by exploring different data types in Python using the `type()` function:

```python
import sys
sys.path.append("../../scripts/")

from ai_tools import *
```

```python
"str"
```

```python
10
```

```python
1.5
```

```python
[1,2,3]
```

```python

```

```python
# Let's check some basic data types
type(42)        # integer
```

```python
print(type(3.14159))   # float
```

```python
print(type("Hello"))   # string
```

```python
print(type([1,2,3]))   # list
```

```python

```

```python

```

### Arithmetic Operations

```python
# Basic arithmetic
print(25 + 15)    # Addition: 40
print(50 - 30)    # Subtraction: 20
print(8 * 6)      # Multiplication: 48
print(100 / 5)    # Division: 20.0
```

```python
20*2 + (10+3) - 230
```

```python
# Special operations
print(17 % 5)     # Modulus (remainder): 2
print(17 // 5)    # Floor division: 3
print(2 ** 3)     # Exponentiation: 8
```

### String Operations

```python
# String concatenation
print("Hello" + " " + "World")    # Works!

# String multiplication
print("Python! " * 3)             # Repeats the string 3 times
```

```python
# Could we add a string to a number?

"lucas is a bald gorgeous man" + 100
```

```python

```

```python

```

### String Formatting with F-strings

```python
# Using f-strings for dynamic string creation
planet = "Mars"
```

```python
planet
```

```python
distance = 225.0
distance
```

```python
message = f"The distance to {planet} is approximately {distance} million kilometers"
print(message)
```

```python
planet = 'Venus'
distance = 170.0
message = f"The distance to {planet} is approximately {distance} million kilometers"
message
```

## Part 2: Variables

A variable in Python is a container that holds data. Think of it like a labeled box where you can store different types of information.

```python
# Creating our first variable
message = "Hello, Python!"
print(message)

# Variables can store different types of data
number = 42
decimal = 3.14
text = "Programming is fun"

print(f"Number: {number}")
print(f"Decimal: {decimal}")
print(f"Text: {text}")
```

### Working with Variables

```python
# Mathematical operations
first_number = 25
second_number = 15

sum_result = first_number + second_number
sum_result
```

```python
difference = first_number - second_number
difference
```

```python
product = first_number * second_number
product
```

```python
print(f"Sum: {sum_result}")
print(f"Difference: {difference}")
print(f"Product: {product}")
```

```python
printed_results = f"""
The sum, difference and product of {first_number} and {second_number} are:
- Sum: {sum_result}
- Difference: {difference}
- Product: {product}
"""
print(printed_results)
```

### Updating Variables

```python
# Changing variable values
counter = 1
print(f"Initial value: {counter}")

counter = counter + 1
print(f"After adding 1: {counter}")

counter += 5  # Shorthand for counter = counter + 5
print(f"After adding 5 more: {counter}")
```

# 1. Time Travel Calculator

- Ask the user for their current age (integer).

- Ask how many years into the future they want to travel (integer).

- Calculate and print their future age using f-strings.

üìù Example output:

You are 33 years old now.
In 20 years, you will be 53 years old!

```python
user_age = input("What is your age son?")

future_years = input("How many years into the future you would like to travel? (output an integer: e.g 10)")

user_age_int = int(user_age)

future_years_int = int(future_years)

future_age = user_age_int + future_years_int

print(f"Your future age will be: {future_age}")
```

# MM asked: how do you insert the new cell in the notebook to start testing ?

ESC + B

# VN: Could you have written int(input("Expecting an int"))?

```python
user_age = int(input("Your age"))

future_years = int(input("Years in the future"))

future_age = user_age + future_years

print(future_age)
```

## Part 3: Functions

Functions are essential building blocks that help us organize code, make it reusable, and break down complex problems into manageable pieces.

### Built-in Functions

```python
# len() - Calculates the length of sequences
name = "Alexander"
numbers = [1, 2, 3, 4, 5]

print(f"Length of name: {len(name)}")
print(f"Length of number list: {len(numbers)}")

# round() - Rounds numbers to specified decimals
pi = 3.14159
print(f"Pi rounded to 2 decimals: {round(pi, 2)}")
print(f"Pi rounded to whole number: {round(pi)}")

# abs() - Returns absolute value
temperature = -10
print(f"Absolute value: {abs(temperature)}")
```

### Creating Our Own Functions

```python
def greet_user(name, time_of_day):
    """
    Creates a personalized greeting based on time of day
    """
    greeting = f"Good {time_of_day}, {name}!"
    return greeting

# Using our function
morning_greeting = greet_user("Alice", "morning")
evening_greeting = greet_user("Bob", "evening")

print(morning_greeting)
print(evening_greeting)
```

### Functions with Multiple Parameters

```python
def calculate_statistics(numbers):
    """
    Calculate basic statistics for a list of numbers
    """
    total = sum(numbers)
    average = total / len(numbers)
    maximum = max(numbers)
    minimum = min(numbers)
    
    return {
        "sum": total,
        "average": average,
        "max": maximum,
        "min": minimum
    }

# Using our statistics function
scores = [85, 92, 78, 90, 88]
stats = calculate_statistics(scores)

print(f"Class Statistics:")
for key, value in stats.items():
    print(f"{key.capitalize()}: {value}")
```

## Part 4: Lists and Loops

Lists in Python are ordered collections that can store different types of data. For loops help us work with these collections efficiently.

```python
# Creating simple lists
students = ["Emma", "James", "Sofia", "Alex", "Nina"]
grades = [95, 87, 92, 88, 90]

# Combining related lists using zip
student_grades = list(zip(students, grades))
print("Student grades:", student_grades)

# Accessing elements
print(f"First student: {students[0]}")
print(f"Last grade: {grades[-1]}")
```

### List Operations and Methods

```python
# Creating a shopping list
shopping = ["milk", "bread", "eggs"]

# Adding items
shopping.append("cheese")               # Add single item
shopping.extend(["apples", "bananas"])  # Add multiple items
shopping.insert(1, "butter")            # Add at specific position

print("Shopping list:", shopping)

# Removing items
removed_item = shopping.pop()           # Remove and return last item
shopping.remove("milk")                 # Remove specific item
print("Updated list:", shopping)
```

### For Loops

```python
# Basic for loop with a task list
tasks = [
    "Review Python basics",
    "Complete coding exercises",
    "Write documentation",
    "Debug application",
    "Submit project"
]

print("Task Tracker:")
for index, task in enumerate(tasks, 1):
    print(f"{index}. [ ] {task}")
```

## Part 5: Dictionaries

Dictionaries are a powerful data structure for storing and managing key-value pairs.

```python
# Creating a simple contact dictionary
contacts = {
    "Alice": "555-0101",
    "Bob": "555-0202",
    "Charlie": "555-0303",
    "Diana": "555-0404"
}

# Accessing values
print(f"Alice's number: {contacts['Alice']}")
print(f"Bob's number: {contacts.get('Bob')}")
# Using get() with default value for safety
print(f"Eve's number: {contacts.get('Eve', 'Not found')}")
```

### Essential Dictionary Methods

```python
def demonstrate_dictionary_methods():
    inventory = {
        "apples": 50,
        "bananas": 30,
        "oranges": 25
    }
    
    # Getting all keys
    print("Available fruits:", list(inventory.keys()))
    
    # Getting all values
    print("Quantities:", list(inventory.values()))
    
    # Getting key-value pairs
    print("\nCurrent inventory:")
    for fruit, quantity in inventory.items():
        print(f"{fruit}: {quantity}")
    
    # Adding/updating items
    inventory.update({
        "apples": 45,  # Update existing
        "grapes": 40   # Add new
    })
    
    # Removing items
    removed_quantity = inventory.pop("bananas")
    print(f"\nRemoved {removed_quantity} bananas")
    
    return inventory

updated_inventory = demonstrate_dictionary_methods()
print("\nFinal inventory:", updated_inventory)
```

### Nested Dictionaries

```python
# Creating a nested user profile
user_profile = {
    "personal_info": {
        "name": "Sarah Wilson",
        "age": 28,
        "location": "Seattle"
    },
    "professional_info": {
        "title": "Software Engineer",
        "skills": ["Python", "JavaScript", "Docker"],
        "experience_years": 5
    },
    "preferences": {
        "theme": "dark",
        "notifications": True,
        "language": "English"
    }
}

def display_profile(profile):
    print("User Profile Summary:")
    print(f"Name: {profile['personal_info']['name']}")
    print(f"Title: {profile['professional_info']['title']}")
    print(f"Skills: {', '.join(profile['professional_info']['skills'])}")
    print(f"Settings: {profile['preferences']}")

display_profile(user_profile)
```

## Part 6: Comparisons and Logic

In this section, we'll explore how Python handles logical operations and comparisons.

### Boolean Values

```python
# Booleans are Python's way of representing True and False
is_python_fun = True
is_coding_hard = False

print(f"Type of boolean: {type(is_python_fun)}")
print(f"Is Python fun? {is_python_fun}")
print(f"Is coding hard? {is_coding_hard}")
```

### Comparison Operators

```python
# Let's look at various comparisons
score = 85
passing_grade = 70
perfect_score = 100

# Greater than and less than
is_passing = score > passing_grade
is_perfect = score >= perfect_score

# Equality comparisons
is_exactly_passing = score == passing_grade  # Equality check
has_improved = score != passing_grade        # Inequality check

print(f"Did I pass? {is_passing}")
print(f"Did I get perfect? {is_perfect}")
print(f"Did I just barely pass? {is_exactly_passing}")
print(f"Did I do better than passing? {has_improved}")
```

## Python Logical Operators

### `and` Operator

| A       | B       | A and B |
|---------|---------|---------|
| True    | True    | True    |
| True    | False   | False   |
| False   | True    | False   |
| False   | False   | False   |

### `or` Operator

| A       | B       | A or B  |
|---------|---------|---------|
| True    | True    | True    |
| True    | False   | True    |
| False   | True    | True    |
| False   | False   | False   |

### `not` Operator

| A       | not A   |
|---------|---------|
| True    | False   |
| False   | True    |

### Creating Complex Conditions

```python
def check_study_conditions(time_of_day, energy_level, is_weekend):
    """
    Determine if it's a good time to study based on multiple factors
    """
    # Check if it's an optimal time (between 9 AM and 8 PM)
    good_time = time_of_day >= 9 and time_of_day <= 20
    
    # Check if energy levels are sufficient (scale of 1-10)
    good_energy = energy_level >= 6
    
    # Determine if we should study
    should_study = (good_time and good_energy) or is_weekend
    
    return {
        "should_study": should_study,
        "time_is_good": good_time,
        "energy_is_good": good_energy
    }

# Test the function
result = check_study_conditions(time_of_day=14, energy_level=8, is_weekend=False)
print("Study Session Analysis:")
for condition, value in result.items():
    print(f"{condition}: {value}")
```

## Part 7: Conditionals and Decision Making

In programming, we often need to make decisions based on certain conditions. Python provides `if`, `elif`, and `else` statements for this purpose.

### Basic If Statements

```python
time_of_day = 10  # 24-hour format

if time_of_day < 12:
    print("Good morning!")
    
if time_of_day > 10:
    print("Time for work!")
```

### If-Else Statements

```python
battery_percentage = 15

if battery_percentage < 20:
    print("Warning: Battery low!")
    print("Please connect charger")
else:
    print("Battery level okay")
```

### If-Elif-Else Chains

```python
def check_file_status(file_size_mb):
    if file_size_mb == 0:
        print("Error: File is empty")
    elif file_size_mb < 1:
        print("Warning: File is very small")
    elif file_size_mb > 1000:
        print("Warning: File is very large")
    else:
        print("File size is acceptable")

check_file_status(0.5)
check_file_status(1500)
```

### Nested Conditionals

```python
def process_file(filename, size_mb):
    if filename.endswith('.txt'):
        if size_mb < 1:
            print(f"Processing small text file: {filename}")
            return True
        else:
            print(f"Text file too large: {filename}")
            return False
    else:
        print(f"Unsupported file type: {filename}")
        return False

process_file("report.txt", 0.5)
process_file("data.csv", 0.3)
process_file("video.txt", 10)
```

### Practical Example: Task Priority System

```python
def analyze_task(task_name, deadline_days, estimated_hours, importance):
    """
    Analyze a task and determine its priority level and handling strategy
    """
    is_urgent = deadline_days <= 3
    is_big_task = estimated_hours > 4
    is_important = importance >= 8  # Scale of 1-10
    
    # Determine priority level
    if is_urgent and is_important:
        priority = "Critical"
    elif is_urgent or (is_important and is_big_task):
        priority = "High"
    elif is_important:
        priority = "Medium"
    else:
        priority = "Low"
    
    # Determine handling strategy
    needs_immediate_action = is_urgent and (is_important or is_big_task)
    can_delegate = not is_important and is_big_task
    
    return {
        "task": task_name,
        "priority": priority,
        "needs_immediate_action": needs_immediate_action,
        "can_delegate": can_delegate
    }

# Test the system with different tasks
tasks = [
    {"name": "Client Presentation", "deadline": 2, "hours": 6, "importance": 9},
    {"name": "Email Responses", "deadline": 1, "hours": 2, "importance": 5},
    {"name": "Code Review", "deadline": 5, "hours": 3, "importance": 8}
]

print("Task Analysis:")
for task in tasks:
    result = analyze_task(task["name"], task["deadline"], 
                         task["hours"], task["importance"])
    print(f"\nTask: {result['task']}")
    print(f"Priority: {result['priority']}")
    print(f"Immediate Action: {result['needs_immediate_action']}")
    print(f"Can Delegate: {result['can_delegate']}")
```

## Summary and Key Takeaways

We've covered the fundamental building blocks of Python programming:

### Data Types and Operations
- Python has several basic data types: integers (`int`), floating-point numbers (`float`), strings (`str`), and more
- Different data types support different operations
- F-strings provide an easy way to create formatted strings

### Variables
- Variables are containers for storing data
- They can be updated or changed at any time
- Meaningful variable names make code easier to understand

### Functions
- Functions help organize code and make it reusable
- Python has many useful built-in functions
- We can create custom functions using the `def` keyword
- Functions can take parameters and return values

### Lists and Loops
- Lists are versatile data structures that can store any type of data
- List methods like append(), extend(), remove() help manage list contents
- For loops provide a way to iterate through lists
- List comprehensions offer a concise way to create and transform lists

### Dictionaries
- Dictionaries store key-value pairs for efficient data organization
- Keys must be immutable (strings, numbers, tuples)
- Common methods include keys(), values(), items(), get(), update(), pop()
- Nested dictionaries can store complex data structures

### Comparisons and Logic
- Boolean values (`True` and `False`) are fundamental for logical operations
- Comparison operators (`>`, `<`, `>=`, `<=`, `==`, `!=`) return boolean values
- `and` requires both conditions to be `True`
- `or` requires at least one condition to be `True`

### Conditionals
- Conditionals allow programs to make decisions based on conditions
- Basic syntax includes `if`, `elif`, and `else`
- Nested conditionals enable complex decision making
- Ternary operator provides simple conditional expressions

These concepts form the foundation for all Python programming. Master these, and you'll be ready to tackle more advanced topics!

---

## notebooks/01-python-fundamentals/02-working-with-data.ipynb

# Working with Data: Files and Tabular Data

This comprehensive notebook covers working with files and CSV data in Python, including reading/writing files, data manipulation with pandas, and leveraging AI tools for data analysis.

## Part 1: Working with Files

In Python, it's extremely simple to work with files like .txt or .md files. Let's explore file operations and how to leverage them for powerful workflows.

```python
import sys
sys.path.append('../../scripts')
from ai_tools import ask_ai
from IPython.display import Markdown
```

### Reading Files

```python
# To open a file
with open("./file.txt", "r") as f:
    data = f.read()

print(data)
```

### Creating and Writing Files

We can also create files easily in Python:

```python
content = "Lucas is teaching python basics!"
with open("summary-notes.txt", "w") as f:
    f.write(content)
```

```python
with open("./summary-notes.txt", "r") as f:
    print(f.read())
```

### File Modes

Below are more examples for the different modes of reading and writing files available via the built-in `open()` method:

```python
# Common file modes in Python's open() function:

# "a" - Append - Opens file for appending, creates new file if not exists
with open("summary-notes.txt", "a") as f:
    f.write("Lucas will also be teaching cool AI tricks!")
```

```python
with open("./summary-notes.txt", "r") as f:
    print(f.read())
```

```python
# "x" - Exclusive creation - Opens for writing, fails if file exists
try:
    with open("file.txt", "x") as f:
        f.write("new file content")
except FileExistsError:
    print("File already exists!")
```

```python
# "+" - Read and write mode
with open("file.txt", "r+") as f:  # Open for both reading and writing
    data = f.read()
    f.write("new data")
```

### Combining File Operations with AI

The cool stuff about being able to do this is that we can connect our ability of generating summaries of information with AI, along with our ability to read and write files in Python to create super powerful workflows.

For example, below we will write single sentence summaries for multiple files containing information about different papers.

```python
ask_ai("Tell me about yourself in one sentence")
```

```python
ask_ai("Write me a one paragraph story that ends with a funny twist")
```

```python
folder_with_papers = "../assets/papers/"
file_names = ["paper1.txt", "paper2.txt", "paper3.txt"]
```

```python
# We know how to loop over lists of names of things
# We have this special functionality 'ask_ai' that allows us to prompt an Ai model!
# Let's combine these 2 things into a function that can summarize all the papers!

def summarize_this_paper(paper_contents):
    summary_prompt = f"Summarize this paper\n\n: {paper_contents} in a couple of sentences."
    output_summary = ask_ai(summary_prompt)
    
    return output_summary
```

```python
paper_summaries_list = []
for file_name in file_names:
    file_path = folder_with_papers + file_name
    with open(file_path, "r") as f:
        contents_of_the_paper = f.read()
    
    paper_summary = summarize_this_paper(contents_of_the_paper)
    paper_summaries_list.append(paper_summary)
```

```python
# Display the markdown content in the notebook
markdown_content = "# Paper Summaries\n\n"
for i, summary in enumerate(paper_summaries_list, 1):
    markdown_content += f"## Paper {i}\n\n"
    markdown_content += f"{summary}\n\n"
        
Markdown(markdown_content)
```

We can do similar things to extract specific information from documents, imagine you have a bunch of differently formatted invoices from which you would like to organize the information extracting things like the amounts and dates.

## Part 2: Working with Tabular Data (CSV Files)

Let's learn about CSV files that structure data into rows and columns (tabular data!). Text files are great but sometimes you need a bit more organization and structure, that's where CSV files come into play.

```python
# Super popular library for working with tabular data
import pandas as pd
import sys
sys.path.append('../../scripts')
from ai_tools import ask_ai
```

### Reading CSV Files

Imagine you have a bunch of information about customer tickets organized in a .csv file that you would like to understand a bit more about.

```python
data_customer_tickets = pd.read_csv("../assets/extracted_ticket_issues.csv")

data_customer_tickets
```

The data contains 3 columns:
1. `customer_name` - names of the customers
2. `issue_description` - description of the issue they had
3. `priority` - reference to the level of priority of that task

### Filtering Data

We could use Python to get for example only the high priority issues:

```python
# == indicates equivalence!
data_customer_tickets["priority"]=="High"

```

```python
data_customer_tickets
```

```python
high_priority_issues = data_customer_tickets[data_customer_tickets["priority"]=="High"]

# Now we can take a look at the issues themselves:
high_priority_issues
```

### Using AI for Data Categorization

Awesome! What we could do now is for example use our `ask_ai` tool to categorize the issues for us to help organizing the information, and then feed that back into the table:

```python
categories_list = []
for issue in high_priority_issues["issue_description"]:
    print(f"Categorizing issue: {issue}")
    category = ask_ai(f"Categorize this issue in just one single word and OUTPUT ONLY THAT WORD:\n\n issue: {issue}\n category: \n")
    print(f"Category: {category}")
    categories_list.append(category)
```

Notice we use concepts we've learned before by looping over the issues, saving them to a list.

Now with that information in hand we can actually update the dataframe accordingly, first we create a new column in the dataframe:

```python
categories_list = []
for issue in data_customer_tickets["issue_description"]:
    print(f"Categorizing issue: {issue}")
    category = ask_ai(f"Categorize this issue in just one single word and OUTPUT ONLY THAT WORD:\n\n issue: {issue}\n category: \n")
    print(f"Category: {category}")
    categories_list.append(category)
```

```python
data_customer_tickets["issue_category"] = categories_list
```

```python
data_customer_tickets
```

## Creating and Managing Structured Data

Besides analysing data, we can also create our own tables with information we care about.

Let's start with a practical example - creating a camping trip gear checklist:

```python
# Create a camping gear checklist
camping_gear = {
    "item": [
        "Tent", "Sleeping Bag", "Backpack", "Hiking Boots",
        "Water Filter", "First Aid Kit", "Headlamp", "Camp Stove"
    ],
    "priority": [
        "Essential", "Essential", "Essential", "Essential",
        "High", "Essential", "High", "Medium"
    ],
    "estimated_cost": [
        299.99, 149.99, 199.99, 159.99,
        89.99, 49.99, 39.99, 79.99
    ],
    "packed": [
        False, False, False, False,
        False, False, False, False
    ]
}

# Convert to DataFrame
gear_df = pd.DataFrame(camping_gear)
print("Camping Gear Checklist:")
display(gear_df)
```

### Working with Data Filters

Let's demonstrate how to filter and analyze our data:

```python
def analyze_gear_requirements():
    # Filter essential items
    essential_gear = gear_df[gear_df['priority'] == 'Essential']
    
    # Calculate total cost of essential items
    essential_cost = essential_gear['estimated_cost'].sum()
    
    # Get unpacked essential items
    unpacked_essential = essential_gear[~essential_gear['packed']]
    
    print(f"Total cost of essential gear: ${essential_cost:.2f}")
    print("\nUnpacked essential items:")
    display(unpacked_essential[['item', 'estimated_cost']])

analyze_gear_requirements()
```

### Creating a Trip Itinerary

Let's create a more complex example with a detailed trip itinerary:

```python
def create_trip_itinerary():
    itinerary_data = {
        'day': range(1, 6),
        'date': pd.date_range('2024-06-01', periods=5),
        'activity': [
            'Arrival and Camp Setup',
            'Mountain Trail Hike',
            'Lake Exploration',
            'Forest Adventure',
            'Pack and Departure'
        ],
        'location': [
            'Basecamp Area',
            'Mountain Ridge Trail',
            'Crystal Lake',
            'Ancient Forest',
            'Basecamp Area'
        ],
        'distance_km': [2, 8, 5, 6, 2],
        'difficulty': [
            'Easy',
            'Hard',
            'Moderate',
            'Moderate',
            'Easy'
        ]
    }
    
    itinerary_df = pd.DataFrame(itinerary_data)
    return itinerary_df

# Create and display the itinerary
trip_itinerary = create_trip_itinerary()
print("Trip Itinerary:")
display(trip_itinerary)
```

### Analyzing Trip Statistics

Let's add some analysis to our trip planning:

```python
def analyze_trip_metrics(itinerary_df):
    # Calculate total distance
    total_distance = itinerary_df['distance_km'].sum()
    
    # Get difficulty breakdown
    difficulty_counts = itinerary_df['difficulty'].value_counts()
    
    # Find longest day
    longest_day = itinerary_df.loc[itinerary_df['distance_km'].idxmax()]
    
    print(f"Trip Analysis:")
    print(f"Total distance: {total_distance} km")
    print("\nDifficulty breakdown:")
    display(difficulty_counts)
    print(f"\nLongest day: Day {longest_day['day']} - {longest_day['activity']}")
    print(f"Distance: {longest_day['distance_km']} km")

analyze_trip_metrics(trip_itinerary)
```

### Exporting and Saving Data

Let's see how to save our data for later use:

```python
def export_trip_data(gear_df, itinerary_df, filename_prefix):
    # Export to CSV
    gear_df.to_csv(f"{filename_prefix}_gear.csv", index=False)
    itinerary_df.to_csv(f"{filename_prefix}_itinerary.csv", index=False)
    print(f"Data exported to {filename_prefix}_gear.csv and {filename_prefix}_itinerary.csv")

# Export our data
export_trip_data(gear_df, trip_itinerary, "camping_trip")
```

### Practical Exercise: Trip Budget Calculator

Let's create a budget calculator for our trip:

```python
def calculate_trip_budget(gear_df, itinerary_df):
    # Equipment costs
    total_gear_cost = gear_df['estimated_cost'].sum()
    
    # Daily expenses (example values)
    daily_expenses = {
        'food': 30,
        'fuel': 10,
        'miscellaneous': 15
    }
    
    num_days = len(itinerary_df)
    daily_total = sum(daily_expenses.values())
    total_daily_costs = daily_total * num_days
    
    # Create budget summary
    budget_summary = pd.DataFrame({
        'Category': ['Gear', 'Food', 'Fuel', 'Miscellaneous'],
        'Cost': [
            total_gear_cost,
            daily_expenses['food'] * num_days,
            daily_expenses['fuel'] * num_days,
            daily_expenses['miscellaneous'] * num_days
        ]
    })
    
    budget_summary['Percentage'] = (
        budget_summary['Cost'] / budget_summary['Cost'].sum() * 100
    ).round(1)
    
    return budget_summary

# Calculate and display budget
budget = calculate_trip_budget(gear_df, trip_itinerary)
print("Trip Budget Summary:")
display(budget)
```

## Key Takeaways

### Working with Files
- Always use `with` statements when working with files to ensure proper closure
- Different file modes serve different purposes:
  - `"r"` for reading
  - `"w"` for writing (creates new/overwrites)
  - `"a"` for appending
  - `"r+"` for reading and writing
- Always handle potential file-related exceptions
- File operations can be combined with data processing for powerful automation
- Consider creating helper functions for common file operations

### Working with CSV and Tabular Data
- Pandas provides powerful tools for working with tabular data
- DataFrames can be filtered and analyzed in various ways
- Data can be exported to different formats (CSV, Excel)
- Structured data makes analysis and planning easier
- Always consider data types when creating DataFrames
- Use appropriate column names and data organization
- Remember to handle missing data appropriately

### Combining AI with Data Processing
- AI tools can be integrated with file and data operations for enhanced automation
- Use AI for tasks like summarization, categorization, and information extraction
- Combine traditional data processing with AI capabilities for powerful workflows

In the next lesson, we'll explore Python packages, APIs, and more advanced data processing techniques!

---

## notebooks/01-python-fundamentals/03-packages-apis.ipynb

# Python Packages, APIs and Code Organization

In this lesson, we'll explore how to work with packages in Python, understand APIs, and learn best practices for organizing code across multiple files. We'll see how to leverage both built-in and third-party packages to extend Python's capabilities.

## Built-in Packages

Python comes with a rich set of built-in packages. Let's explore some common ones:

```python
# Import built-in packages
import math
import statistics
import random

# Generate some random data for our examples
data = [random.randint(1, 100) for _ in range(20)]

data
```

```python
# Using math functions
print(f"Square root of 16: {math.sqrt(16)}")
print(f"Pi value: {math.pi}")
print(f"Exponential of 2: {math.exp(2)}")
```

```python
# Using statistics functions
print(f"\nMean of data: {statistics.mean(data)}")
print(f"Median of data: {statistics.median(data)}")
print(f"Standard deviation: {statistics.stdev(data)}")
```

```python
# Using random functions
print(f"\nRandom choice from data: {random.choice(data)}")
```

```python
print(f"Random sample of 5 numbers: {random.sample(data, 5)}")
```

```python
import sys
sys.path.append('../../scripts')
from ai_tools import ask_ai, ask_local_ai #?????
```

```python
ask_local_ai("Explain to my friend AS who is learning about LLM apis why you don't need an API key to use local LLM models. ")
```

```python

```

## Working with Third-Party Packages

Third-party packages extend Python's functionality. Let's work with some popular ones:

```python
# Import third-party packages
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Create sample data
dates = pd.date_range(start='2024-01-01', end='2024-02-01', freq='D')
temperatures = np.random.normal(20, 5, size=len(dates))

# Create a DataFrame
weather_data = pd.DataFrame({
    'date': dates,
    'temperature': temperatures
})

# Basic data wrangling
weather_stats = weather_data.describe()
print("Weather Statistics:")
print(weather_stats)

# Create a visualization
plt.figure(figsize=(10, 6))
plt.plot(weather_data['date'], weather_data['temperature'])
plt.title('Daily Temperatures')
plt.xlabel('Date')
plt.ylabel('Temperature (¬∞C)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## Code Organization with Custom Modules

**WALKTHROUGH OF THE `./ai_tools.py` module:**

 The `ai_tools.py` module contains several key functions:

 1. `import` statements at the top:
     - `from openai import OpenAI`: For accessing OpenAI's API
     - `import anthropic`: For accessing Anthropic's Claude API
     - `import ollama`: For running local AI models
     - `import json`: For parsing JSON responses
 
 2. `ask_ai(prompt, model_name)`: Sends prompts to cloud AI models like GPT-4 and Claude
    - Uses OpenAI and Anthropic APIs
    - Returns text response from the AI

 3. `ask_local_ai(prompt, model_name)`: Sends prompts to locally-run AI models
    - Uses Ollama API for local LLMs
    - Can return structured JSON or plain text

 4. Helper functions for parsing AI outputs:
    - `parse_dates_list()`: Converts string output to Python list of dates
    - `parse_json_output()`: Safely parses JSON responses from AI

Let's create a separate module for weather-related functions. First, create a file called `weather_utils.py`:

```python
# weather_utils.py

def celsius_to_fahrenheit(celsius):
    """Convert Celsius to Fahrenheit"""
    return (celsius * 9/5) + 32

def calculate_heat_index(temperature, humidity):
    """Calculate heat index based on temperature (¬∞F) and humidity (%)"""
    if temperature < 80:
        return temperature
    
    heat_index = -42.379 + 2.04901523 * temperature + 10.14333127 * humidity
    heat_index -= 0.22475541 * temperature * humidity
    heat_index -= 6.83783e-3 * temperature**2
    heat_index -= 5.481717e-2 * humidity**2
    heat_index += 1.22874e-3 * temperature**2 * humidity
    heat_index += 8.5282e-4 * temperature * humidity**2
    heat_index -= 1.99e-6 * temperature**2 * humidity**2
    
    return round(heat_index, 2)
```

Now let's use our custom module:

```python
# Import the pandas module for data manipulation
import pandas as pd
import numpy as np
# Import our custom module
from weather_utils import celsius_to_fahrenheit, calculate_heat_index

weather_data = pd.DataFrame({
    'date': pd.date_range(start='2024-01-01', periods=100, freq='D'),
    'temperature': np.random.uniform(0, 30, size=100)  # Random temperatures in Celsius
})

# Convert our temperature data to Fahrenheit
weather_data['temperature_f'] = weather_data['temperature'].apply(celsius_to_fahrenheit)

# Add random humidity data
weather_data['humidity'] = np.random.uniform(30, 80, size=len(weather_data))

# Calculate heat index
weather_data['heat_index'] = weather_data.apply(
    lambda x: calculate_heat_index(x['temperature_f'], x['humidity']), 
    axis=1
)

print("\nSample of weather data with heat index:")
print(weather_data.head())
```

## Working with APIs

Let's explore how to work with APIs using the OpenWeatherMap API as an example:

```python
import requests
import json
import os
from datetime import datetime

def get_weather_data(city, api_key):
    """
    Fetch weather data for a given city using OpenWeatherMap API
    """
    base_url = "http://api.openweathermap.org/data/2.5/weather"
    params = {
        "q": city,
        "appid": api_key,
        "units": "metric"  # Use metric units
    }
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()  # Raise an exception for bad status codes
        
        data = response.json()
        
        # Extract relevant information
        weather_info = {
            "city": city,
            "temperature": data["main"]["temp"],
            "humidity": data["main"]["humidity"],
            "description": data["weather"][0]["description"],
            "timestamp": datetime.fromtimestamp(data["dt"]).strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return weather_info
    
    except requests.exceptions.RequestException as e:
        print(f"Error fetching weather data: {e}")
        return None

# Example usage (you'll need your own API key)
api_key = os.environ["OPEN_WEATHER_API_KEY"]  # Replace with your actual API key
cities = ["London", "New York", "Tokyo", "Sydney"]

weather_results = []
for city in cities:
    result = get_weather_data(city, api_key)
    if result:
        weather_results.append(result)

# Convert results to DataFrame
weather_df = pd.DataFrame(weather_results)
print("\nCurrent weather in different cities:")
print(weather_df)
```

## Installing Packages with pip

Here's how to install packages using pip (run these commands in your terminal):

# Install packages

```pip install requests
pip install pandas
pip install matplotlib
pip install beautifulsoup4

# Install specific versions
pip install requests==2.28.1

# Upgrade packages
pip install --upgrade requests

# List installed packages
pip list```

## Web Scraping Example

Let's create a simple web scraping example using BeautifulSoup:

```python
import requests
from bs4 import BeautifulSoup

def scrape_oreilly_ai_programming_news(url):
    """
    Scrape AI and programming-related content from O'Reilly Radar.
    """
    try:
        # Send request to the website
        response = requests.get(url)
        response.raise_for_status()
        
        # Parse HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find AI and Programming sections
        ai_section = soup.find('h2', string=lambda text: text and 'AI' in text)
        programming_section = soup.find('h2', string=lambda text: text and 'Programming' in text)
        
        news_items = []
        
        # Extract AI content
        if ai_section:
            ai_content = ai_section.find_next('ul')
            if ai_content:
                news_items.extend([li.text.strip() for li in ai_content.find_all('li')])
        
        # Extract Programming content
        if programming_section:
            programming_content = programming_section.find_next('ul')
            if programming_content:
                news_items.extend([li.text.strip() for li in programming_content.find_all('li')])
        
        return news_items
    
    except requests.exceptions.RequestException as e:
        print(f"Error scraping website: {e}")
        return []

# Example usage (replace with actual O'Reilly Radar URL)
oreilly_url = "https://www.oreilly.com/radar/radar-trends-to-watch-january-2025/"
ai_programming_news = scrape_oreilly_ai_programming_news(oreilly_url)

if ai_programming_news:
    print("\nLatest O'Reilly AI & Programming News:")
    for idx, news in enumerate(ai_programming_news, 1):
        print(f"{idx}. {news}")

```

```python
def create_html_news_display(news_items):
    """
    Create a formatted HTML display of news items with proper styling
    """
    html = """
    <div style="max-width: 800px; margin: 20px auto; font-family: Arial, sans-serif;">
        <h2 style="color: white; border-bottom: 2px solid red; padding-bottom: 10px;">
            O'Reilly AI & Programming News
        </h2>
        <ul style="list-style-type: none; padding: 0;">
    """
    
    for item in news_items:
        html += f"""
            <li style="margin: 15px 0; padding: 15px; background-color: black; 
                       border-left: 4px solid red; border-radius: 4px;">
                {item}
            </li>
        """
    
    html += """
        </ul>
    </div>
    """
    
    from IPython.display import HTML
    return HTML(html)

# Display the news in formatted HTML

create_html_news_display(ai_programming_news)
```

## Best Practices for Package Management

1. Always use virtual environments to isolate project dependencies
2. Keep a requirements.txt file with your project
3. Use semantic versioning in requirements.txt
4. Document any specific installation steps in README.md

You can create a requirements.txt file for your project:

```python
# Generate requirements.txt
import subprocess

def generate_requirements():
    """Generate requirements.txt file"""
    try:
        subprocess.run(["pip", "freeze", ">", "requirements.txt"], shell=True)
        print("Requirements.txt generated successfully")
    except subprocess.SubprocessError as e:
        print(f"Error generating requirements.txt: {e}")

generate_requirements()
```

---

## notebooks/01-python-fundamentals/Untitled.ipynb

```python
print("HI!")
```

# SM Question
I know we all are here to learn python. But whats so much buzz around vibe coding? Chatgpt, Grok and others if i know basics of programming, claude code , copilot all these new AI tools helping a lot. so do we still need to learn syntax and other things about python?


Yes! It is nice and useful to know at least the basic syntax of any programming language including Python.

# BA issue

abc@@dhcp-10-48-113-248 oreilly-python-course % playwright install                                                                             

zsh: command not found: playwright

DOn't worry about this right now! I'll send a solution during the break. Focus on following along.

```python

```

---

## notebooks/01-python-fundamentals/Untitled1.ipynb

```python

```

---

## notebooks/01-python-fundamentals/live-demo-part1-2-3.ipynb

# Data Types

```python
# strings
"this is a string because its enclosed by double quotes!"
```

```python
'strings can also be enclosed by single quotes!'
```

```python
# type is a special function in python that tells us the type of any data in Python!
type("Lucas is an instructor")
```

```python
type(10)
```

```python
type(1.2)
```

# Operations!

```python
# basic arithmetics!
10/2
```

```python
# division
20.0 / 3
```

```python
# multiplication
10*3
```

```python
# addition
10 + 20
```

```python
# subtraction
20 - 10
```

```python
# special operations
# modulus := 17 / 5 = 3*5 + 2
17%5
```

```python
# floor division
17//5 # returns the 3 because 17 = 3*5+2
```

```python
# Exponentiation?
3**2 # 3 * 3 = 9
```

## Operations on Strings!

```python
"Hello" + " my name is Lucas."
```

```python
# could we add a string to a number?
"lucas is bald and gorgeous" + 100
```

# THE MOST IMPORTANT OPERATION ON STRING THAT YOU WILL EVER LEARN EVER!!!!!

## STRING FORMATTING!!!

```python
# variable assigned to a string named: "Mars"
planet = "Mars"
```

```python
distance = 225.0
distance
```

```python
message = f"The distance to {planet} is approximately {distance} million kilometers"
message
```

```python
instructor_name = "George"
title_course = "Building robots"
welcome_message = f"What up folks my name is {instructor_name} and I want to welcome you to this course titled: {title_course}"
welcome_message
```

```python
# examplme of how we'll use string formatting in practice
# file_path = "./fake-location-path.pdf
# prompt_template = f"Summarize this pdf located here: {file_path}"
# ai(prompt_template)
```

# Variables!

```python
# a little container for data, think of it like a little labeled box where you can store different types of information!
```

```python
message = 'Hello, Python is awesome!'

number = 23

print(message)

print(number)
```

```python
n_1 = 10
n_2 = 20
add_operation = n_1 + n_2
add_operation
```

```python

course_title = "Python AI goodies"
topic = "automation scripts with AI and Python"
multiplier_automator = 100
printed_outputs = f"""
This course: {course_title} is about: {topic} and
will make you a {multiplier_automator} automator!
"""
printed_outputs
```

```python
# to remove things from a string we could use a method inside the string
# Data types have methods for each!
print(f"Before:")
printed_outputs
```

```python
print("After")
printed_outputs.strip("\n")
```

```python
# to make words bigger we can use .upper() method!
'lucas'
```

```python
'lucas'.upper()
```

# Time Travel Calculator Exercise!

- Ask the user for their current age (integer).

- Ask how many years into the future they want to travel (integer).

- Calculate and print their future age using f-strings.

üìù Example output:

You are 33 years old now.

In 20 years, you will be 53 years old!

# RM Submission

```python
# input is for taking input from the user when we run the code!
age = input('Enter your age :')
future = input('Enter the years you want to travel as a number :')
age = int(age)
future = int(future)
print(f'Your are {age} years old now and in {future} you will be {age+future} years old')
```

## Part 3: Functions

Functions are essential building blocks that help us organize code, make it reusable, and break down complex problems into manageable pieces.

```python
# I want to turn this piece of code into a reusable block! How do we do that?

def time_travel_calculator_from_awesome_student():
    age = input('Enter your age :')
    future = input('Enter the years you want to travel as a number :')
    age = int(age)
    future = int(future)
    print(f'Your are {age} years old now and in {future} you will be {age+future} years old')

time_travel_calculator_from_awesome_student()
```

```python
def time_travel_calculator_with_parameters(age, future_years):
    """
    Function declaration (piece of string that explains what the function does)
    """
    future_age = age + future_years
    print(f"Your age {age} years old now and in {future_years} you will be {future_age} years old")

# calling the function
time_travel_calculator_with_parameters(age=10, future_years=20)
```

```python
# a function that returns information
def calculate_multiplication_on_3_numbers(num1, num2, num3):
    """calculates a sum of 3 numbers"""
    return num1+num2+num3 # makes sure the function RETURNs an output!

output = calculate_multiplication_on_3_numbers(1,2,3) # here I'm not specifying the names because it's not necessary unless the parameters are super different
output
```

# Lists and Loops!

```python
list_numbers = [10, 20, 30] # list of numbers

for item in list_numbers: # looping over the list of numbers to do something with the contents of that list
    print(item)
```

```python
students = ["KS", "MP", "JL", "SN", "SK", "RM", "U1", "ZH", "GB", "GH"]
grades = [10, 10, 10, 5, 8, 10, 9, 3, 4, 6] 
for student, grade in zip(students, grades): # special keyword to loop over 2 lists together
    print(f"This student: {student} go this grade: {grade}") #string formatting to organize our prints
```

```python
# access specific values inside of a list
item_list = ["apple","orange","cookies"]

item_list[0] # index level access to elements of a list 
```

```python
item_list[1]
```

```python
item_list[2]
```

```python
item_list[3]
```

we get an error because lists in python start from 0 so if the full size is 3, it will 0, 1 and 2, etc....

```python
# Any data type in Python has SPECIAL methods remember???

item_list[item_list.index("cookies")]
```

```python
# slicing = accessing elements from a range

shopping = ["milk", "bread", "eggs", "banana", "cookies", "apple"]

shopping[0:2] # so milk, bread the last element is not returned
```

```python
# Add items to the list!!!
shopping.append("cheese")
```

```python
# extend the list by adding multiple items from another list!
shopping.extend(["fish","chips"])
```

```python
shopping
```

```python
# remove elements from a list!
shopping.remove("cheese")
```

```python
shopping
```

```python
# For loops are awesome! Even more awesome is the idea of iterating through elements and doing some indexing on the fly
tasks = ["teach python", "talk to students", "drink some beers", "sleep"]

print("Task Tracker")
for index_task, task in enumerate(tasks):
    print(f"{index_task}. [ ] {task}")
```

# Dictionaries!

```python
# create a simple contacts dictionary
contacts = {
    "Alice": "555-0101",
    "Bob": "555-0202",
    "Charlier": "555-1234"
}

contacts
```

```python
contacts['Alice'] # access elements by their keys!
```

```python
# dictionaries have keys and values
contacts.keys()
```

```python
# dictionaries also have their special methods like .keys() and .values()!!!
contacts.values()
```

```python
# what if we want everything?
contacts.items()
```

```python
# Let's take 2 lists and turn them into a dictionary!
list_names = ["Lucas", "John", "Berk"]
list_of_numbers = [1, 11, 13]

dictionary_names_numbers = dict(zip(list_names, list_of_numbers)) # zip creates a way to go through these 2 lists together, and the 
# dict allows to conver those 2 into a single dictionary where the keys are the first list and the values the second 
dictionary_names_numbers
```

```python
# using for loops we could've done this!
# create an empty dictionary first:
dictionary_names_numbers = {}

for name,num in zip(list_names, list_of_numbers):
    dictionary_names_numbers[name] = num

dictionary_names_numbers
```

# Comparisons and Logic

```python
# create a value that is either true or false
is_python_fun = True # boolean values which are just values that can be True or False
is_scripting_hard = False
is_software_engineering_hard = True

type(is_python_fun)
```

```python
type(is_scripting_hard)
```

```python
# exam with grades from 0-100
score = 85
passing_grade = 75
perfect_score = 100

# did you pass the exam?
is_passing = score > passing_grade
is_passing # because score is a bigger number than the passing_grade
```

```python
score = 70
is_passing = score>passing_grade
is_passing
```

# Conditionals and Decision Making

```python
score = 70
passing_grade = 80

if score>passing_grade:
    print("You passed!")
else:
    print("You didn't pass....")
```

```python
def student_pass(student_score):
    pass # pass just means doesn't return anything
```

If the student has a score higher than a number
return a True boolean
if not (or else....) return False boolean

```python
def student_pass(student_score, passing_grade):
    if student_score>passing_grade:
        print("You passed!")
        return True
    else:
        print("You didn't pass!")
        return False

student_pass(80,75) 
```

```python
student_pass(70, 75)
```

```python
# def student_pass(student_score, passing_grade):
#     return student_score>passing_grade

# student_pass(70, 80)
```

# Logical Operators

```python
True and True
```

```python
True and False
```

## Python Logical Operators

### `and` Operator

| A       | B       | A and B |
|---------|---------|---------|
| True    | True    | True    |
| True    | False   | False   |
| False   | True    | False   |
| False   | False   | False   |

### `or` Operator

| A       | B       | A or B  |
|---------|---------|---------|
| True    | True    | True    |
| True    | False   | True    |
| False   | True    | True    |
| False   | False   | False   |

### `not` Operator

| A       | not A   |
|---------|---------|
| True    | False   |
| False   | True    |

```python
score = 100
is_passing = 90

if score>is_passing and type(is_passing)==int:
    print("passed")
```

# Are we clear so far on these topics?

- Data types
- Operations
- Functions
- Lists
- Loops
- Dictionaries
- Comparison values
- Conditionals with if else statements?

Let's look a toy example automation that would process a file 
and return a message indicating if the file if too big or not

# Reading and Writing to files
    

```python
# file path: file.txt open mode: "r" which is read mode
with open("./file.txt", "r") as f:
    contents = f.read()

contents
```

```python
# what if we want to write a file?
with open("./sample_file.txt", "w") as f:
    f.write("this is the content of sample file")
```

```python
tasks = ["learn python", "learn about ai tools"]

with open("tasks.txt", "w") as f:
    for task in tasks:
        f.write(task)
        f.write("\n") # to skip lines

```

```python
with open("./tasks.txt", "r") as f:
    print(f.read())
```

TO check if a file exists in Python we can use the one and only 'os' package!!!! that comes with Python! (or pathlib...)

```python
import os # this imports the functionalites 
          # from this 'os' package into 
          # our code environment
```

```python
os.path.abspath("./file.txt")
```

```python
# returns all the files in the folder specified
os.listdir("./")
```

```python
# to check if a file exists we give it the path and use this:
file_path = "./file.txt"
os.path.exists(file_path)
```

```python
# the output is a boolean value! 
```

```python
def task_tracker(task, file_path):
    # if file exists we append to file
    if os.path.exists(file_path):
        with open(file_path, "a") as f: # a = append mode!!!
            f.write(task)
            f.write("\n")
    # else we write the file from scratch
    else:
        with open(file_path, "w") as f:
            f.write(task)

    return 'task written!'

task_tracker("learn python", "./lucas_tasks.txt")
task_tracker("learn french", "./lucas_tasks.txt")
task_tracker("practicing piano", "./lucas_tasks.txt")
```

# Are we clear so far on these topics?

- Data types
- Operations
- Functions
- Lists
- Loops
- Dictionaries
- Comparison values
- Conditionals with if else statements?
- Write / Read Files
- Use 'os' package to perform file system operations

# Recap of what we've covered so far!

```python
# data types and operations
10 + 20
```

```python
result = 10 * 2
result
```

```python
def reusable_block_of_code(a, b):
    print(a,b)
    return a, b

reusable_block_of_code(10, 'hello')
```

```python
item_list = [1,2,3,'apple']
for item in item_list:
    print(item)
```

```python
dict_key_values = {
    "key1": "value1",
    "key2": "value2"
}

dict_key_values["key1"]
```

```python
import pandas as pd # external package for working with tabular data
# csvs or excel files
# pd will be the keyword that we use to use the functionality
# from pandas
```

```python
table = pd.DataFrame({"colum1": [1,2,3]})

table
```

```python
score = 80
passing_grade = 75

passed = score>passing_grade
passed
```

```python
True and True
```

```python
True and False
```

```python
True or False
```

```python
# conditionals'
company_name = "pet store AI"
if "AI" in company_name:
    print("Millions in funding")
else:
    print("Put AI in your company name")
```

```python
def write_file(contents, file_path):
    file_mode = "w"
    with open(file_path, file_mode) as f:
        f.write(contents)

    return 'File written successfully!'

write_file("lucas is the best or the worst?", "./lucas-silly-notes.txt")
```

```python
import os # package we are importing to do file system
          # operations

```

```python
file_path = 'file.txt'
os.path.exists(file_path)
```

# Working with Data APIs and Packages

how to work well with csv files?

```python
import pandas as pd
```

Image you have a bunch of information about customer tickets organized in a .csv file (which is table just like an excel)
that youw ould like to understand a bit more about what
is going on inside.

```python
# pd:= using the functionalities from the pandas package
# read_csv = the method for uploading a csv to memory in
# our environment
data_customer_tickets = pd.read_csv("../assets/extracted_ticket_issues.csv")

data_customer_tickets
```

The data contains 3 columns:
1. `customer_name` - names of the customers
2. `issue_description` - description of the issue they had
3. `priority` - reference to the level of priority of that task

Filter out the data to get only high priority issues:

```python
data_customer_tickets["priority"]
```

```python
data_customer_tickets["priority"]=="High"
```

```python
index_of_high_priority_issues = data_customer_tickets["priority"]=="High"
```

```python
high_priority_issues = data_customer_tickets[index_of_high_priority_issues]
high_priority_issues
```

```python
import pandas as pd # to work with tabular data 

# Sample data: Monthly sales for a small shop
data = {
    "Month": ["Jan", "Feb", "Mar", "Apr", "May", "Jun"],
    "Sales": [120, 135, 150, 170, 160, 190],
    "Expenses": [80, 85, 90, 95, 100, 105],
}

df = pd.DataFrame(data)
df
```

```python
# Line plot
df.plot(x="Month", y=["Sales", "Expenses"], kind="line", marker="o", title="Sales vs Expenses")
```

# Implementing our First Automations!

IMplement some automations to help with our camping trip!

```python
# Create a camping gear checklist
camping_gear = {
    "item": [
        "Tent", "Sleeping Bag", "Backpack", "Hiking Boots",
        "Water Filter", "First Aid Kit", "Headlamp", "Camp Stove"
    ],
    "priority": [
        "Essential", "Essential", "Essential", "Essential",
        "High", "Essential", "High", "Medium"
    ],
    "estimated_cost": [
        299.99, 149.99, 199.99, 159.99,
        89.99, 49.99, 39.99, 79.99
    ],
    "packed": [
        False, False, False, False,
        False, False, False, False
    ]
}

# Convert to DataFrame
gear_df = pd.DataFrame(camping_gear)
print("Camping Gear Checklist:")
display(gear_df)
```

```python
def analyze_gear_requirements():
    # Filter essential items
    essential_gear = gear_df[gear_df['priority'] == 'Essential']
    
    # Calculate total cost of essential items
    essential_cost = essential_gear['estimated_cost'].sum()
    
    # Get unpacked essential items
    unpacked_essential = essential_gear[~essential_gear['packed']]
    
    print(f"Total cost of essential gear: ${essential_cost:.2f}")
    print("\nUnpacked essential items:")
    display(unpacked_essential[['item', 'estimated_cost']])

analyze_gear_requirements()
```

# Packages and Using LLM APIs!!!

Task: Summarize all the pdfs inside a folder and then organize that information inside of a table?

```python
# Super popular library for working with tabular data
import pandas as pd  # package for table data
import sys # this is for system operation actions
sys.path.append('../../scripts') # this folder should be considered as part of our environment (puts the path inside our env)
from ai_tools import ask_ai # 
```

```python
ask_ai("HI")
```

```python
# triple quotes like: """ are for multi-line strings

with open("article.txt", "r") as f:
    contents_of_article = f.read()

print(contents_of_article)
```

```python
output_summary = ask_ai(f"""Summarize this article for me:  
{contents_of_article} the output should be in organized bullet points.
""")
output_summary
```

```python
from IPython.display import Markdown # special functionality inside jupyter notebooks to display markdown content

Markdown(output_summary)
```

```python
with open("./summary_file.md", "w") as f:
    f.write(output_summary)
```

```python
def summarize_all_files(folder):
    summaries = []
    for file in os.listdir(folder):
        if file.endswith(".txt") or file.endswith(".md"):
            with open(file, "r") as f:
                contents = f.read()
            print("AI summarizing....")
            output_summary = ask_ai(f"""
            Summarize this content into super concise bullet points:
            {contents}
            """)
            summaries.append(output_summary)
        else:
            print(f"This is not a file: {file}")

    return summaries

output_summaries = summarize_all_files("./")
```

```python
with open("summary_report.md", "w") as f:
    for summary in output_summaries:
        f.write(summary)
        f.write("\n")

    print("summary with ai done!")
```

---

## notebooks/01-python-fundamentals/live-demo-python-basics.ipynb

# What have we seen so far?

Variables, data, operations...

```python
variable = 10

# how do we get the type of a variable?
type(variable)
```

```python
variable_str = "10"

type(variable_str)
```

```python
10 + 10
```

```python
result = 20 * 3
result
```

```python
# Truthfullness and falsehood?

20 == 10
```

```python
type(False)
```

```python
# The result of this operation is False which is stored inside the 
# bool_variable!
bool_variable = 20 == 10

bool_variable
```

```python
'L' in 'Lucas'
```

```python
character = 'x'

word = 'Extraordinary'

character in word
```

```python
number = 1000

another_number = 300

# What is the output going to be?
number > another_number
```

# What do we do when we want to organize operations together in re-usable blocks?

We use **FUNCTIONS**!

Python has a bunch of **built-in functions**

```python
name = "Lucas"
numbers = [1,2,3,4,5]

# built in function in Python to get the size of an 'iterable' which is an object
# that we can loop over
len(name)
```

```python
numbers = [1,2,3]
len(numbers)
```

```python
len(name)==len(numbers)
```

```python
decimal_number = 1.14547945728415472332

round(decimal_number, 3)
```

WE want to create our own functions Lucas!!!! how do we do that???

```python
def name_of_our_function():
    print("Welcome to functions!")
    print("Functions are fun!")

name_of_our_function()
```

```python
name_of_our_function()
```

I don't care! I want something useful!

```python
def calculate_stats(list_numbers):
    """
    Calculate basic statistics for a list of numbers
    """

    total_sum = sum(list_numbers)
    # Avg of any group of numbers is the total sum divided by how
    # many numbers are in the group! so Avg = total / #num of numbers
    average = total_sum / len(list_numbers)
    max_num = max(list_numbers)
    min_num = min(list_numbers)

    return [total_sum, average, max_num, min_num]

calculate_stats([1,2,3,4,5])
```

```python
def greet_couple(name1, name2):
    greeting_msg = f"Hello {name1} and {name2} and welcome to the 'Love is Kind of Blind' show!"
    return greeting_msg

greet_couple("Lucas", "Marta")
```

```python
print(greet_couple("Lucas", "Marta"))

print("Another message")
```

# How do we work with lots of data in an organized way?

Quick answer is to use lists and loops!

```python
list_names = ["Lucas", "Marcus", "John", "Pamela"]

for name in list_names:
    print(f"Hi! {name}")
```

```python
for index,name in enumerate(list_names):
    print(f"Id: {index}, person name: {name}.")
```

```python
list_names[0]
```

```python
list_names.index("Lucas")
```

# Dictionaries!!!!

```python
phone_book = {
    "Lucas": "555-0245",
    "John" : "555-6543",
    "Dumbledore": "0900-1234"
}
phone_book
```

```python
phone_book.keys()
```

```python
phone_book.values()
```

```python
phone_book_and_recipe_book = {
    "Lucas": "555-0245",
    "John" : "555-6543",
    "Dumbledore": "0900-1234",
    "pancake_recipe": """
    Mix flour, eggs, milk, sugar, 
    and a pinch of salt into a smooth batter, 
    pour onto a hot pan, flip when bubbles form, 
    and serve warm with your favorite topping."""
}
```

```python
def check_study_conditions(time_of_day, is_weekend):
    """
    Determne if it's a good time to study 
    based on multiple factors like the time of day,
    your energy levels and whether or not today
    is a weekend.
    """
    # checking if its between 9AM and 8PM
    good_time = time_of_day >=9 and time_of_day <= 20

    # checking energy levels
    energy_level = int(input("What is your energy level now between 1-10?"))
    good_energy = energy_level>=6

    # should you study? 
    should_study = (good_time and good_energy) or is_weekend

    return should_study
    
check_study_conditions(10, False)
```

```python
check_study_conditions(13,False)
```

# We want the ability to perform decisions! 

For that we need conditionals!

```python
def greetings_based_on_time_of_day(current_time):
    if current_time<12 and current_time>=5:
        print("Good morning!")
    # else if this other scenario
    elif current_time>=12 and current_time is current_time <=18:
        print("Good afternoon")
    else:
        print("Good evening!")

    
greetings_based_on_time_of_day(4)    
```

```python
file_name = "lucas_pancake_recipe.txt"

ext_txt = ".txt"

if ext_txt in file_name:
    print("Its a text file!")
else:
    print("Its not a text file!")
```

Let's build our first automation!!

Task prioritization system!

```python
def analyse_task(task_name, deadline_days, estimated_hours):
    """
    Analyze a task and determine its priority level and handling strategy 
    """
    
    is_urgent = deadline_days <= 3
    is_big_task = estimated_hours > 4

    if is_urgent and is_big_task:
        priority_level = "Critical!!!"
    elif is_urgent and not is_big_task:
        priority_level = "Mid!"
    else:
        priority_level = "low"

    return {
    "task": task_name,
    "priority": priority_level
    }

analyse_task("Finishing intro to Python", 1, 1)
```

# How do we work with tabular data?

```python
import pandas as pd
```

```python
df = pd.read_csv("./camping_trip_gear.csv")

# dataframe! Has nothing to do with def which is 
# a special keyword in python for creating functions!
df
```

How would I estimated the total cost of the all of the items?

```python
df["item"]
```

```python
df.head()
```

```python
df['estimated_cost']
```

```python
for number in df['estimated_cost']:
    print(number)
    print(type(number))
```

```python
# let's calculate the total of estimated cost for all items

total_item_cost = 0
for item_cost in df['estimated_cost']:
    total_item_cost = total_item_cost+item_cost

print(total_item_cost)
```

```python
def calculate_total_item_cost(df):
    total_item_cost = 0
    for item_cost in df['estimated_cost']:
        total_item_cost = total_item_cost+item_cost

    return total_item_cost
```

```python
df
```

```python
calculate_total_item_cost(df)
```

# Working with Files!

```python
Let's go to notebook number 02!!!
```

---

## notebooks/01-python-fundamentals/summary_file.md

<details>
<summary>View summary_file.md</summary>

### Summary of the Article

- **Personal Exploration**: The author has been creating various small study apps driven by a curiosity to explore better learning methods and the ease of developing such tools with AI capabilities.

- **Motivations**:
  - A desire to improve the learning experience.
  - A creative outlet that has unintentionally become a form of procrastination.

### Apps Created

1. **Video Note Taker**:
   - Allows users to take notes linked to specific timestamps in videos, including attached screenshots.
   - Initially developed to facilitate note-taking for video reviews for a context engineering event.

2. **Video Quiz AI**:
   - Generates custom quizzes from videos and their transcriptions using the open-source tool, whisper-cpp.
   - Features explanations for answers linked to video timestamps, though primarily used for entertainment (e.g., quizzing on "The Office").

3. **Simple Quiz App in Plain HTML/JS**:
   - A minimalistic app that allows users to upload JSON files with AI-generated quiz questions.
   - Offers a straightforward quiz interface and the ability to export results as Anki cards or CSV files.

4. **PDF Chat**:
   - A chat interface that uses AI to answer questions based on PDF contents and directs users to relevant pages.
   - A simpler alternative to existing chat-with-PDF applications, facing challenges with PDF integration.

### Overall Intent and Philosophy

- **Obsessed with a 'Meta-Workflow'**: The author aims to build a personal system of interconnected apps for enhanced learning experiences.
- **Vision**: Inspired by technologists, the goal is to create an engaging, efficient learning environment that merges technology with thought processes.
- **Challenges**: Acknowledges the risk of falling into procrastination through optimization rather than focusing on actual learning, emphasizing the essential need for intentional focus in the learning process.

</details>

---

## notebooks/01-python-fundamentals/summary_report.md

<details>
<summary>View summary_report.md</summary>

- Sample file content
- Includes appended information
- Contains new data entries
- Learn Python
- Learn French
- Practice piano
- Learn Python programming  
- Explore AI tools and their applications  
Sure! Please provide the content you would like summarized into bullet points.
- Debate on whether Lucas is the best or the worst.
- Opposing opinions about Lucas's qualities or actions.
### Article Summary

- **Personal Exploration**: Author creates small study apps to enhance learning and explore AI development.
  
- **Motivations**:
  - Improve learning experiences.
  - Creative outlet that may lead to procrastination.

### Apps Created

1. **Video Note Taker**:
   - Notes linked to video timestamps with screenshots.
   - Developed for note-taking during video reviews.

2. **Video Quiz AI**:
   - Generates quizzes from videos and transcriptions.
   - Provides answer explanations with timestamps; mainly for entertainment.

3. **Simple Quiz App**:
   - Minimalist app for uploading JSON quiz questions.
   - Allows exporting results as Anki cards or CSV files.

4. **PDF Chat**:
   - AI-powered chat answering questions from PDF content.
   - A simpler alternative to existing PDF chat solutions.

### Philosophy and Intent

- **Meta-Workflow Focus**: Building interconnected apps to enhance learning.
- **Vision**: Create engaging learning environments merging tech and thought.
- **Challenges**: Risk of procrastination through over-optimization; emphasizes need for focused learning.
- **App Development**: Created various study apps for topics like quizzing, note-taking, and PDF interactions.
- **Motivation**: Driven by curiosity to explore better learning methods and ease of AI coding; also a form of creative procrastination.
- **Video Note Taker**: Note-taking app that links notes to video timestamps and screenshots, built for reviewing engineering submissions.
- **Video Quiz AI**: App for generating quizzes from videos using transcriptions; provides clickable explanations linked to video timestamps; primarily used for fun.
- **Simple Quiz App**: Plain HTML app that quizzes users with AI-generated questions from a JSON file; can export results to Anki or CSV.
- **PDF Chat**: A tool to ask questions about PDFs, using AI to provide answers and references; simpler than similar platforms, but functional.
- **Learning Philosophy**: Desires to create a seamless learning environment that merges technology with thought; inspired by technologists.
- **Challenges**: Struggles with procrastination and the optimization of study tools instead of focused learning; acknowledges that real learning requires intentional effort.


</details>

---

## notebooks/02-ai-apis/01-ai-apis-overview.ipynb

# PACKAGES!!!!

```python
# import package!!!!!!
# we import the OpenAI functionality from the openai package
from openai import OpenAI
```

```python
import sys

sys.path.append("../../scripts/")

# ai_tools is the module!!!!!
# module is like a package but its a python file!!!!!
# from ai_tools import LUCAS, ask_ai
```

```python
# this command will install the openai and anthropic packages
!pip install openai anthropic
```

```python
from openai import OpenAI
import anthropic

# can you read this code easily?
def ask_ai(prompt, model_name="gpt-4o-mini"):
    """
    Send prompt to an LLM and get output text back.    
    """
    if "claude" in model_name:
        client = anthropic.Anthropic()

        # Send a message to the Claude AI
        response = client.messages.create(
            model=model_name,
            messages=[
                {
                    "role": "user", "content": prompt
                }
                ],
            max_tokens=4000,
        )
        output = response.content[0].text
        return output        
    else:
        client = OpenAI()
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
        )
        output = response.choices[0].message.content
        return output
```

```python

```

```python

```

```python

```

```python

```

```python

```

# Using AI APIs: A Comprehensive Guide

In this lesson, we'll explore how to interact with various AI models using their respective APIs. We'll cover text generation, image creation, and audio transcription across different providers.

## Setting Up Libraries

First, let's install all required libraries:

```python
# Install required libraries (uncomment to run)
# %pip install openai anthropic ollama
```

```python
import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"var: ")

# _set_env("OPENAI_API_KEY")
# _set_env("ANTHROPIC_API_KEY")
# _set_env("OLLAMA_API_KEY")
```

```python
from IPython.display import HTML, Image, Audio
import os
import openai
import anthropic
import ollama
```

## Helper Functions for Display

Let's create some helper functions to make our outputs look nice:

```python
import sys
sys.path.append("../../scripts/")
from demo_utils import display_chat_message, display_comparison
```

## Text Generation with Different Providers

Let's compare how different AI providers handle the same prompt: "Explain quantum computing in simple terms."

### OpenAI (GPT-4o)

```python
def ask_openai(prompt):
    client = openai.OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# Example usage
openai_response = ask_openai("Explain quantum computing to a 12 year old boy")
openai_response
```

```python
prompt = """
Create a Python script to automate sending notifications\
to my mac machine at 12:30 to remind me to lunch.
Your output should ONLY be the Python code.
"""
print(ask_openai(prompt))
```

### Anthropic (Claude)

```python
def ask_claude(prompt):
    client = anthropic.Anthropic()
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    return message.content[0].text

# Example usage
claude_response = ask_claude("Explain quantum computing in simple terms")
claude_response
```

```python
# _set_env("GOOGLE_API_KEY")
# Google (Gemini)
# pip install google-generativeai 
# import google.generativeai as genai
# def ask_gemini(prompt):
#     genai.configure(api_key="YOUR_API_KEY")
#     model = genai.GenerativeModel("gemini-1.5-flash")
#     response = model.generate_content(prompt)
#     return response.text

# # Example usage
# gemini_response = ask_gemini("Explain quantum computing in simple terms")
```

### Ollama (Local Models)

1. To use local models we'll need to download ollama from https://ollama.com/
2. Then we'll need to open up a terminal (or powershell for Windows users) and type in: `ollama run llama3.2`
3. Now, we can run the code below without any hassle!

```python
def ask_ollama(prompt):
    response = ollama.chat(
        model='llama3.2',
        messages=[{'role': 'user', 'content': prompt}]
    )
    return response['message']['content']

# Example usage
ollama_response = ask_ollama("Explain quantum computing in simple terms")
ollama_response
```

### Comparing Responses

```python
responses = [
    ("OpenAI GPT-4o", openai_response),
    ("Anthropic Claude", claude_response),
    ("Ollama (Local)", ollama_response)
]

display_comparison(*responses)
```

## Image Generation with DALL-E 3

Let's create an image using OpenAI's DALL-E 3:

```python
def generate_image(prompt):
    client = openai.OpenAI()
    response = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size="1024x1024",
        quality="standard",
        n=1,
    )
    return response.data[0].url

# Example usage
image_prompt = "A futuristic quantum computer in a cyberpunk setting, digital art style"
image_url = generate_image(image_prompt)
Image(url=image_url)
```

## Audio Transcription with Whisper

Let's transcribe audio using OpenAI's Whisper model:

```python
def transcribe_audio(file_path):
    client = openai.OpenAI()
    with open(file_path, "rb") as audio_file:
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="text"
        )
    return transcription

# Example usage (assuming you have an audio file)
transcription = transcribe_audio("./assets-resources/audio-sample.mp3")
display_chat_message("Transcription", transcription)
```

## Practice Exercise

Write a simple script that
1. Uses the OpenAI api with the model: `gpt-4o-mini` to suggest a text description for a creative image 
2. The suggestion should then be criticized by a different model (gpt-4o, claude, ollama etc...)
3. The feedback should then be incorporated by the first llm into a new improved suggestion
4. That suggestion should then be send to the openai api to create an image using the dalle3 api.

This notebook provides a comprehensive introduction to using different AI APIs. Some key takeaways:

1. Each provider has its own authentication method and API structure
2. Different models excel at different tasks
3. Local models (like through Ollama) can be useful for privacy and offline use
4. Multimodal capabilities (text, image, audio) are becoming increasingly accessible

Remember to:
- Keep your API keys secure
- Handle rate limits and errors appropriately
- Consider costs when making API calls
- Choose the right model for your specific use case

---

## notebooks/02-ai-apis/02-ai-tools-hands-on.ipynb

# AI Hacks

## 1. AI Tools

### General
- ChatGPT
- Claude
- Gemini
- Ollama/Llama Studio/Llamacpp
- Google AI Studio
- Deep Seek

### Search Tools
- Perplexity for search
- ChatGPT + search
- Gemini 1.5 Pro Deep Research

### Development Tools
- Claude/ChatGPT Projects
- Artifacts in Claude/ChatGPT
- ChatGPT Canvas

## 2. Whiteboard Techniques
- Prompt Templates
- Chaining
- Hybrid approach

## Tips & Tricks
- Let it see your screen - Gemini/ChatGPT desktop app
- Paste code + error ask it to debug
- When a task is simple but has many repeatable steps:
  - AI to generate quick Python script in correct proper runnable format
  - Copy - paste - run - finish
- When AI makes a mistake save it for later as your own personal benchmark
- Build app with AI that takes in data with a certain structure and outputs desirable output, format, etc.... then make a prompt template that produces the data into the format acceptable by that app (done) ‚Üí example my Quiz app.

## Advanced Techniques
- [Ask for full scripts with uv style package management](https://claude.ai/chat/cf3cb3d1-5b3e-4ea8-8584-5b6ceada7d84)
  - [Another example with very little debugging](https://claude.ai/chat/3276e7df-0dcf-4159-a6c0-df0ff9601e85)
- Show LLM how to call an API (in the prompt) then ask it to create something with that API
- Writing Python code with Claude Projects + relevant documentation
- Meta prompts
- Context Building Workflow with LLMs
- Test-Driven Development with AI (AI Writes Tests First)
- Voice driven prompting
- Treat AI models as a team of specialized workers



Claude Projects instructions for python scripts

```The user will input problems or Python script descriptions and you will output a single Python file with the inline metadata to run it standalone using uv, below are a few examples from the uv documentation:

<example 1>
example.py

# /// script
# dependencies = [
#   "requests<3",
#   "rich",
# ]
# ///

import requests
from rich.pretty import pprint

resp = requests.get("https://peps.python.org/api/peps.json")
data = resp.json()
pprint([(k, v["title"]) for k, v in data.items()][:10])
<example 1>

<example 2>
example2.py

# /// script
# requires-python = ">=3.12"
# dependencies = []
# ///

# Use some syntax added in Python 3.12
type Point = tuple[float, float]
print(Point)
<example 2>

<instructions>
Remember to only output the Python file so I can copy paste and run with a uv run command as shown below:
<instructions>
uv run <python script name>.py```

---

## notebooks/02-ai-apis/live-demo-apis-automations.ipynb

# Packages

```python
from openai import OpenAI
```

```python
def ask_ai(prompt, model_name="gpt-5-mini"):
    # access the openai API and call this model to get a response
    client = OpenAI()
    response = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                 "content": prompt
            }
        ],
        model=model_name
    )
    output = response.choices[0].message.content
    print(output)

    return output

ask_ai('hi')
```

```python
def summarize_all_files(folder):
    # initialize summaries list
    # loop over all files in folder
    # for the txt or md files 
    # summarize them using the openai api like this:
    # ask_ai(prompt,model_name) and it should contain summarize prompt 
    # with contents of file
    # when you're done add it to the list 
    # repeat until done
    pass

output = ask_ai("""
Write up this function for me:
def summarize_all_files(folder):
    # initialize summaries list
    # loop over all files in folder
    # for the txt or md files 
    # summarize them using the openai api like this:
    # ask_ai(prompt,model_name="gpt-5-mini") and it should contain summarize prompt 
    # with contents of file
    # when you're done add it to the list 
    # repeat until done
    pass

Your output should only be this function and nothing else.
"""
      )
```

```python
def summarize_all_files(folder):
    import os
    summaries = []
    for entry in os.listdir(folder):
        path = os.path.join(folder, entry)
        if not os.path.isfile(path):
            continue
        if not entry.lower().endswith(('.txt', '.md')):
            continue
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            summaries.append({'filename': entry, 'error': f'Could not read file: {e}'})
            continue
        prompt = (
            f"You are a helpful assistant. Summarize the following file named '{entry}'. "
            "Provide a concise summary that captures the main points, key details, and any "
            "actions or conclusions. Keep the summary clear and brief (a few short paragraphs "
            "or bullet points). Do not include the original text.\n\n"
            "---BEGIN FILE---\n"
            f"{content}\n"
            "---END FILE---"
        )
        try:
            summary = ask_ai(prompt, model_name="gpt-5-mini")
        except Exception as e:
            summaries.append({'filename': entry, 'error': f'API error: {e}'})
            continue
        summaries.append({'filename': entry, 'summary': summary})
    return summaries

summarize_all_files("/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course")
```

# Recipe for Creating Automations

![image.png](attachment:4222de03-7495-4fea-889b-944d00fd8bcb.png)

## Let's run an experiment: 

1. Let's imagine a situation we want to solve/automate
2. Let's describe it in steps (as well as we can)
3. Let's send that to our AI
4. Let's take the output
5. Execute and test


```python
output = ask_ai("""
Create this python script replacing the comments with the appropriate code and steps:



def organize_files_in_folder(folder_path):

    # List all files and folders inside a folder

    # Identify the different types of files

    # Use this rule to save files in specific folders:

    # .md, .txt, .pdfs -> documents folder

    # .png, .jpg, .jpeg, ...any other images into a media folder

    # create a third folder for the rest called: others

    # save the files in the right locations

    # return a message saying that succeded



if a comment needs some update you can upudated it at will

output should be just the script
""")
```

```python
import os
import shutil
from pathlib import Path
import sys

def organize_files_in_folder(folder_path):
    """
    Organize files in the given folder into three subfolders:
    - documents: .md, .txt, .pdf
    - media: .png, .jpg, .jpeg, .gif, .bmp, .tiff, .svg, .webp
    - others: any other file types or files without extension

    Only top-level files in folder_path are moved (subfolders are left intact).
    If a filename conflict occurs in the destination folder, a numeric suffix is added.
    Returns a success message with counts of moved files.
    """
    folder = Path(folder_path)

    if not folder.exists() or not folder.is_dir():
        return f"Error: '{folder_path}' does not exist or is not a directory."

    # Define extension sets
    documents_ext = {'.md', '.txt', '.pdf'}
    media_ext = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.svg', '.webp'}

    # Create destination folders
    documents_dir = folder / "documents"
    media_dir = folder / "media"
    others_dir = folder / "others"

    for d in (documents_dir, media_dir, others_dir):
        d.mkdir(exist_ok=True)

    moved_counts = {"documents": 0, "media": 0, "others": 0}

    for entry in folder.iterdir():
        # Only process files at the top level (skip directories)
        if not entry.is_file():
            continue

        # Skip files that are already inside one of the target folders
        if entry.parent in (documents_dir, media_dir, others_dir):
            continue

        ext = entry.suffix.lower()

        # Decide destination based on extension
        if ext in documents_ext:
            dest_dir = documents_dir
            key = "documents"
        elif ext in media_ext:
            dest_dir = media_dir
            key = "media"
        else:
            dest_dir = others_dir
            key = "others"

        # Resolve name conflicts by adding a suffix before the extension
        dest_path = dest_dir / entry.name
        if dest_path.exists():
            stem = entry.stem
            suffix = entry.suffix
            counter = 1
            while True:
                new_name = f"{stem}({counter}){suffix}"
                dest_path = dest_dir / new_name
                if not dest_path.exists():
                    break
                counter += 1

        # Move the file
        try:
            shutil.move(str(entry), str(dest_path))
            moved_counts[key] += 1
        except Exception as e:
            # If a file couldn't be moved, continue with others
            print(f"Warning: could not move '{entry.name}': {e}")

    return (f"Files organized successfully. Moved {moved_counts['documents']} to 'documents', "
            f"{moved_counts['media']} to 'media', and {moved_counts['others']} to 'others'.")


# # If a folder path is provided as an argument, use it; otherwise use the current directory
# if len(sys.argv) > 1:
#     target_folder = sys.argv[1]
# else:
#     target_folder = os.getcwd()

target_folder = "/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course/notebooks/assets"
result = organize_files_in_folder(target_folder)
print(result)
```

---

## notebooks/02-ai-apis/live_demo_summarize_files.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = ["openai"]
# ///

# Magic inline metadata

from openai import OpenAI

def ask_ai(prompt, model_name="gpt-5-mini"):
    # access the openai API and call this model to get a response
    client = OpenAI()
    response = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                 "content": prompt
            }
        ],
        model=model_name
    )
    output = response.choices[0].message.content
    print(output)

    return output

def summarize_all_files(folder):
    import os
    summaries = []
    for entry in os.listdir(folder):
        path = os.path.join(folder, entry)
        if not os.path.isfile(path):
            continue
        if not entry.lower().endswith(('.txt', '.md')):
            continue
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            summaries.append({'filename': entry, 'error': f'Could not read file: {e}'})
            continue
        prompt = (
            f"You are a helpful assistant. Summarize the following file named '{entry}'. "
            "Provide a concise summary that captures the main points, key details, and any "
            "actions or conclusions. Keep the summary clear and brief (a few short paragraphs "
            "or bullet points). Do not include the original text.\n\n"
            "---BEGIN FILE---\n"
            f"{content}\n"
            "---END FILE---"
        )
        try:
            summary = ask_ai(prompt, model_name="gpt-5-mini")
        except Exception as e:
            summaries.append({'filename': entry, 'error': f'API error: {e}'})
            continue
        summaries.append({'filename': entry, 'summary': summary})
    return summaries

output = summarize_all_files("/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course")

print(output)
```

---

## notebooks/03-automation-projects/0-simple-examples-python-snippets.ipynb

Report of several practical examples where people have automated downloads (or file retrieval) with Python. Many examples you see on Reddit, developer blogs, and tutorials show that‚Äîwhether it‚Äôs downloading a report, a media file, or scraping content for later use‚ÄîPython‚Äôs rich ecosystem makes it straightforward. Here are eight representative examples with concise code snippets:

---

### 1. Basic File Download with‚ÄØ`urllib`

A common starting point is to use Python‚Äôs built‚Äêin‚ÄØ`urllib.request.urlretrieve`‚ÄØto download a file from an HTTP URL. For example:

```python
import urllib.request

url = "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
filepath = "../assets/paper3.pdf"
urllib.request.urlretrieve(url, filepath)
print("Downloaded paper")
```

```python
!ls ../assets/*.pdf* | grep paper
```

This can be handy when you want a one-liner that mimics the familiar Unix tool.  


---

### 2. Conditional Download (Based on File Age)

Sometimes you want to download a file only if it‚Äôs missing or older than a given age. For example, to download a CSV file if it‚Äôs older than one day:

```python
import urllib.request
import os
import time

url = 'https://gist.githubusercontent.com/denandreychuk/b9aa812f10e4b60368cff69c6384a210/raw/100%20Sales%20Records.csv'
local_file = 'data.csv'
# Download if file doesn't exist or is older than 24 hours (86400 seconds)
if not os.path.exists(local_file) or (os.path.getmtime(local_file) < time.time() - 86400):
    urllib.request.urlretrieve(url, local_file)
    print("Updated data.csv")
else:
    print("data.csv is up-to-date")
```

```python
import os
import time
import requests

# fake csv file (doesn't exist online!)
url = 'https://gist.githubusercontent.com/denandreychuk/b9aa812f10e4b60368cff69c6384a210/raw/100%20Sales%20Records.csv'
local_file = 'data.csv'

def download_if_needed(url, local_file):
    # 24 hours in seconds
    max_age = 24 * 60 * 60  

    # Condition 1 + 2: Check if file exists AND whether it is older than 24h
    needs_download = (
        not os.path.exists(local_file) or
        (time.time() - os.path.getmtime(local_file)) > max_age
    )

    if needs_download:
        print("Downloading fresh copy...")
        resp = requests.get(url, timeout=10)
        if resp.status_code != 200:
            raise RuntimeError("Download failed.")
        with open(local_file, "wb") as f:
            f.write(resp.content)
    else:
        print("Local file is recent; no download needed.")

# Run it
download_if_needed(url, local_file)
```

```python
!ls *.csv
```

This pattern is popular in automating daily report updates.  


---

### 3. Downloading Large Files with‚ÄØ`requests`‚ÄØand Streaming

When downloading large files, it‚Äôs best to stream the response in chunks. This approach uses the‚ÄØ`requests`‚ÄØlibrary:

```python
import requests

url = "http://example.com/largefile.zip"
local_filename = "largefile.zip"
with requests.get(url, stream=True) as r:
    r.raise_for_status()
    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
print("Large file downloaded successfully.")
```

Using streaming helps avoid high memory usage with large downloads.  


---

### 4. Automating Downloads via Selenium

For sites that require a login or button clicks (for instance, downloading a report from a secure portal), Selenium can be used to simulate user actions:

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time

driver = webdriver.Chrome()  # Assumes chromedriver is installed and in PATH
driver.get("http://example.com/login")
# Fill in login form
driver.find_element(By.ID, "username").send_keys("myusername")
driver.find_element(By.ID, "password").send_keys("mypassword" + Keys.RETURN)
time.sleep(3)  # Wait for login to complete
# Navigate to download page and click the download button
driver.get("http://example.com/download")
driver.find_element(By.ID, "downloadButton").click()
time.sleep(5)  # Wait for download to start/complete
driver.quit()
```

This method is frequently mentioned in discussions where users automate downloading reports or files from platforms that don‚Äôt offer direct URL-based downloads.  


---

### 5. Downloading Videos Using‚ÄØ`youtube_dl`

For media files (especially videos from platforms like YouTube), many developers rely on the popular tool‚ÄØ`youtube_dl`‚ÄØ(which now has forks such as `yt-dlp`):

```python
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=EXAMPLE_ID'])
```

This snippet automatically downloads the video file using the robust features of the youtube_dl ecosystem.  


---

### 6. Invoking the Command-line‚ÄØ`wget`‚ÄØfrom Python

Sometimes you want to leverage the battle-tested command-line utility wget (especially for features like resuming downloads). You can invoke it via Python‚Äôs subprocess module:

```python
import subprocess

url = "http://example.com/file.zip"
# The "-c" flag allows resuming an interrupted download.
subprocess.run(["wget", "-c", url])
```

This is useful when you‚Äôre comfortable with wget‚Äôs features and want to combine them with your Python workflow.  


---

### 7. Concurrent Downloads Using‚ÄØ`aiohttp`‚ÄØand‚ÄØ`asyncio`

For scenarios where you need to download many files at once, asynchronous code with‚ÄØ`aiohttp`‚ÄØcan dramatically speed things up:

```python
import aiohttp
import asyncio

async def download_file(session, url, filename):
    async with session.get(url) as resp:
        with open(filename, 'wb') as f:
            while True:
                chunk = await resp.content.read(1024)
                if not chunk:
                    break
                f.write(chunk)
    print(f"Downloaded {filename}")

async def main():
    files = [
        ("http://example.com/file1.zip", "file1.zip"),
        ("http://example.com/file2.zip", "file2.zip"),
        # Add more (url, filename) tuples as needed.
    ]
    async with aiohttp.ClientSession() as session:
        tasks = [download_file(session, url, fname) for url, fname in files]
        await asyncio.gather(*tasks)

asyncio.run(main())
```

This approach uses asynchronous programming to start multiple downloads concurrently, which is ideal when processing many files in bulk.  

# Example for Organizing Downloads Folder

# Exercise for writing prompts to ChatGPT

## Create a prompt for ChatGPT to create a script to perform something like organizing files in a folder, but adding your own twist, perspective or usecase.

**Prompt to ChatGPT (GPT-4o)**
```
Write a Python script that organizes files in my 'Downloads' folder by moving them into subfolders based on their file type (e.g., PDFs to a 'PDFs' folder, images to an 'Images' folder, etc.). The script should create folders if they don‚Äôt exist.
```

```python
import os
import shutil
from collections import defaultdict

# Define the downloads directory
DOWNLOADS_DIR = os.path.expanduser("~/Downloads")

# Define file type categories
FILE_CATEGORIES = {
    "Images": [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg", ".tiff"],
    "Documents": [".pdf", ".doc", ".docx", ".txt", ".odt", ".rtf", ".tex"],
    "Spreadsheets": [".xls", ".xlsx", ".csv", ".ods"],
    "Presentations": [".ppt", ".pptx", ".key"],
    "Archives": [".zip", ".rar", ".tar", ".gz", ".7z"],
    "Executables": [".exe", ".dmg", ".pkg", ".app"],
    "Videos": [".mp4", ".mkv", ".mov", ".avi", ".flv", ".wmv"],
    "Music": [".mp3", ".wav", ".flac", ".aac", ".ogg", ".m4a"],
    "Code": [".py", ".js", ".html", ".css", ".java", ".c", ".cpp", ".go", ".sh", ".ipynb"]
}

# Reverse the dictionary to map file extensions to their respective categories
EXTENSION_TO_CATEGORY = {ext: category for category, exts in FILE_CATEGORIES.items() for ext in exts}

# Ensure categorized folders exist
def create_folders():
    for folder in FILE_CATEGORIES.keys():
        folder_path = os.path.join(DOWNLOADS_DIR, folder)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

# Organize files
def organize_downloads():
    create_folders()
    
    for filename in os.listdir(DOWNLOADS_DIR):
        file_path = os.path.join(DOWNLOADS_DIR, filename)
        
        # Skip directories
        if os.path.isdir(file_path):
            continue
        
        # Get the file extension
        _, ext = os.path.splitext(filename)
        ext = ext.lower()
        
        # Determine the category
        category = EXTENSION_TO_CATEGORY.get(ext, "Other")
        
        # Create 'Other' folder if needed
        target_folder = os.path.join(DOWNLOADS_DIR, category)
        if not os.path.exists(target_folder):
            os.makedirs(target_folder)
        
        # Move the file
        shutil.move(file_path, os.path.join(target_folder, filename))
        print(f"Moved: {filename} -> {category}/")

if __name__ == "__main__":
    organize_downloads()

```

```python

```

---

## notebooks/03-automation-projects/01-file-management-automation.ipynb

```python
import sys
sys.path.append("../../scripts/")
from ai_tools import ask_ai
from IPython.display import Markdown
```

```python

```

```python
def extract_invoice_data(invoice_contents):
    extraction_prompt = f"""Extract the following information from this invoice:
    - Date
    - Total Amount
    
    Invoice contents:
    {invoice_contents}
    
    Return only the date and total amount separated by dashes
    like this:
    
    2024-04-15 - $18,900.00
    2024-04-18 - $48,727.50 
    2023-12-31 - ‚Ç¨2,350.44
    
    Extracted information:
    """
    
    extracted_data = ask_ai(extraction_prompt)
    return extracted_data

example_data = """
TECH SOLUTIONS INC.
789 Innovation Drive
Seattle, WA 98101
Tax ID: 98-7654321

INVOICE

Bill To:                                    Invoice No: INV-2024-0103 
Sarah Johnson                               Date: April 18, 2024
789 Enterprise Road                         Due Date: May 18, 2024
Chicago, IL 60601

Description                     Quantity    Rate        Amount
-----------------------------------------------------------------
AI Model Development              120      $200.00    $24,000.00
Data Processing Services           80      $125.00    $10,000.00
System Integration                 40      $175.00     $7,000.00
Hardware Configuration             1     $3,500.00     $3,500.00
                                                    ------------
                                           Subtotal:  $44,500.00
                                           Tax (9.5%): $4,227.50
                                           Total:     $48,727.50

"""

extract_invoice_data(example_data)
```

```python

```

```python

```

```python

```

```python

```

```python

```

```python

```

Now, let's apply this to all the invoice data we have!

```python
folder_with_invoices = "../assets/fake-invoices/"
invoice_files = ["invoice1.txt", "invoice2.txt", "invoice3.txt"]

invoice_data_list = []
for invoice_file in invoice_files:
    file_path = folder_with_invoices + invoice_file
    with open(file_path, "r") as f:
        invoice_contents = f.read()
    
    extracted_data = extract_invoice_data(invoice_contents)
    date, amount = extracted_data.split(" - ")
    invoice_data = {
        "file": invoice_file,
        "date": date,
        "amount": amount
    }
    
    invoice_data_list.append(invoice_data)

invoice_data_list
```

```python
# Display the markdown content in the notebook
markdown_content = "# Invoice Data Summary\n\n"
for invoice in invoice_data_list:
    markdown_content += f"## {invoice['file']}\n\n"
    markdown_content += f"**Date:** {invoice['date']}\n\n"
    markdown_content += f"**Amount:** {invoice['amount']}\n\n"

Markdown(markdown_content)
```

## Practical Example: Document Analysis System

Let's create a system that analyzes text documents and extracts key information:

```python
def analyze_document(filename):
    """
    Analyzes a document and extracts key metrics
    """
    with open(filename, "r") as file:
        content = file.read()
    
    # Basic metrics
    metrics = {
        "filename": filename,
        "total_chars": len(content),
        "total_words": len(content.split()),
        "total_lines": len(content.splitlines()),
        "unique_words": len(set(content.lower().split()))
    }
    
    return metrics

def generate_report(metrics):
    """
    Creates a formatted report from document metrics
    """
    report = f"""Document Analysis Report
========================
Filename: {metrics['filename']}
Character Count: {metrics['total_chars']}
Word Count: {metrics['total_words']}
Line Count: {metrics['total_lines']}
Unique Words: {metrics['unique_words']}
"""
    return report

# Example usage
sample_text = """This is a sample document.
It contains multiple lines of text.
We will analyze this document."""

with open("sample_doc.txt", "w") as file:
    file.write(sample_text)

metrics = analyze_document("sample_doc.txt")
print(generate_report(metrics))
```

## Working with Multiple Files

Here's how to process multiple files in a directory:

```python
def batch_process_files(file_list, processor_func):
    """
    Process multiple files using a given processor function
    """
    results = []
    for filename in file_list:
        try:
            with open(filename, "r") as file:
                content = file.read()
                result = processor_func(content)
                results.append({
                    "filename": filename,
                    "result": result
                })
        except FileNotFoundError:
            print(f"Could not find file: {filename}")
    return results

# Example processor function
def count_words(content):
    return len(content.split())

# Example usage
files = ["file1.txt", "file2.txt", "file3.txt"]
word_counts = batch_process_files(files, count_words)
```

---

## notebooks/03-automation-projects/02-data-extraction.ipynb

# Data Extraction: Automating Invoice Processing

This notebook demonstrates how to extract structured data from text documents like receipts and invoices using AI. We'll explore:

1. Basic text extraction using prompts
2. Structured output formats (JSON, XML)

The techniques shown here can help automate manual data entry tasks and standardize information extraction from semi-structured documents.

```python
with open("./invoice-data-sample.txt", "r") as f:
    receipt_data = f.read()
    
receipt_data
```

```python
from IPython.display import Markdown

Markdown(receipt_data)
```

```python
from ai_tools import ask_ai

extraction_prompt = f"""

You are an extraction engine for receipt data.
Users will upload the contents of their receipts and you will extract
the following fields:
- Company name
- Date of closure
- Amount paid

Extract the data from the following receipt:
{receipt_data}
"""

structured_output = ask_ai(prompt=extraction_prompt)

structured_output
```

This output is ok but we don't want the conversational elements of the response right?

To get around that, let's improve our initial prompt:

```python
extraction_prompt_json = f"""
You are an extraction engine for receipt data.
Users will upload the contents of their receipts and you will extract
the following fields as JSON OBJECTS:
- Company name
- Date of closure
- Amount paid

Extract the data from the following receipt:
{receipt_data}

Your OUTPUT SHOULD ONLY BE A JSON OBJECT WITH THE FOLLOWING FIELDS:
- company_name
- date_of_closure
- amount_paid
"""

structured_output_json = ask_ai(prompt=extraction_prompt_json)

structured_output_json
```

```python
# We need to import the json library to parse the JSON output
import json

def parse_json_output(json_str):
    """
    This function parses the JSON output from the AI and removes the markdown code block markers if present.
    """
    # Remove markdown code block markers if present
    json_str = json_str.replace('```json', '').replace('```', '').strip()
    
    # Parse the JSON string into a Python dictionary
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        print("Error: Could not parse JSON string")
        return None

parsed_json = parse_json_output(structured_output_json)


parsed_json
```

```python
print(f"Company Name: {parsed_json['company_name']}")
print(f"Date of Closure: {parsed_json['date_of_closure']}")
print(f"Amount Paid: {parsed_json['amount_paid']}")
```

In Claude we can also do this quite easily using `xml` tags: `<output>{"company_name":....etc....} </output>`



```python
from ai_tools import ask_ai

ask_ai(prompt="Hi! Which model are you?", model_name="claude-3-5-sonnet-20240620")
```

```python
extraction_prompt_claude = f"""
You are an extraction engine for receipt data.
Users will upload the contents of their receipts and you will extract key fields.

Extract the following fields from this receipt:
{receipt_data}

Format your response using XML tags like this:
<output>
  <company_name>The company name</company_name>
  <date_of_closure>The date of closure</date_of_closure>
  <amount_paid>The amount paid</amount_paid>
</output>

Only include the XML tags and JSON object in your response, nothing else.
"""
output = ask_ai(prompt=extraction_prompt_claude, model_name="claude-3-5-sonnet-20240620")

output
```

Now, let's write a function that properly parses this output from Claude:

```python
def parse_claude_output(output):
    """
    This function parses the output from Claude and removes the XML tags.
    """
    # Remove XML tags if present
    output = output.replace('<output>', '').replace('</output>', '').strip()
    return output

output_parsed = parse_claude_output(output)

output_parsed
```

Now we can access each individual attribute easily by simply parsing the tags:


```python
import re

def extract_field(output, field_name):
    """Extract value between XML tags for a given field."""
    pattern = f"<{field_name}>(.*?)</{field_name}>"
    match = re.search(pattern, output)
    return match.group(1) if match else None

# Extract each field
company_name = extract_field(output_parsed, "company_name")
date_of_closure = extract_field(output_parsed, "date_of_closure") 
amount_paid = extract_field(output_parsed, "amount_paid")

print(f"Company Name: {company_name}")
print(f"Date of Closure: {date_of_closure}")
print(f"Amount Paid: {amount_paid}")
```

But what if you don't want to send your private data to some cloud provider?

In that case, we use local models! After a lot of advancements, we can now easily use local models to extract structured outputs similar to what we have been doing before.

```python
from ai_tools import ask_local_ai
import json

extraction_prompt_json = f"""
You are an extraction engine for receipt data.
Users will upload the contents of their receipts and you will extract
the following fields as JSON OBJECTS:
- Company name
- Date of closure
- Amount paid

Extract the data from the following receipt:
{receipt_data}

Your OUTPUT SHOULD ONLY BE A JSON OBJECT WITH THE FOLLOWING FIELDS:
- company_name
- date_of_closure
- amount_paid
"""

output_string = ask_local_ai(extraction_prompt_json, structured=True)

output_json = json.loads(output_string)

print(f"Company Name: {output_json['company_name']}")
print(f"Date of Closure: {output_json['date_of_closure']}")
print(f"Amount Paid: {output_json['amount_paid']}")
```

The fancier way of doing this for those interested in exploring more about structured extractions is using something called `pydantic` a data validation library that perfectly integrates with LLM APIs like openai's and anthropics to create these structured outputs in a more programatic and organized fashion.
See an example in: `./structured_output_with_pydantic.py`.

# Extracting Insights from Technology Trends Reports from OReilly Media

Radar Trends website:
- https://www.oreilly.com/radar/trends/

```python
%load_ext autoreload
%autoreload 2
```

```python
raw_contents_oreilly_tech_trends_january_2025 = """

Skip to main content
O'Reilly home
Sign In
Try Now
Teams
For business
For government
For higher ed
Individuals
Features
All features
Courses
Certifications
Interactive learning
Live events
Answers
Insights reporting
Plans
Blog
Content sponsorship
Search
Radar / Radar Trends
Radar Trends to Watch: January 2025
Developments in Security, Programming, AI, and More

By Mike Loukides
January 7, 2025

Learn faster. Dig deeper. See farther.
Join the O'Reilly online learning platform. Get a free trial today and find answers on the fly, or master something new and useful.

Learn more
Despite its 31 days, December is a short month. It's hard for announcements and happenings other than office parties to get attention. Fighting this trend, OpenAI made a series of announcements: their "12 Days of OpenAI." Not to be upstaged, Google responded with a flurry of announcements, including their Gemini 2.0 Flash Thinking model. Models appeared that could use streaming audio and video for both input and output. But perhaps the most important announcement was DeepSeek-V3, a very large mixture-of-experts model (671B parameters) that has performance on a par with the other top models‚Äîbut cost roughly 1/10th as much to train.

AI
DeepSeek-V3 is another LLM to watch. Its performance is on a par with Llama 3.1, GPT-4o, and Claude Sonnet. While training was not inexpensive, the cost of training was estimated to be roughly 10% of the bigger models.
Not to be outdone by Google, OpenAI previewed its next models: o3 and o3-mini. These are both "reasoning models" that have been trained to solve logical problems. They may be released in late January; OpenAI is looking for safety and security researchers for testing.
Not to be outdone by 12 Days of OpenAI, Google has released a new experimental model that has been trained to solve logical problems: Gemini 2.0 Flash Thinking. Unlike OpenAI's GPT models that support reasoning, Flash Thinking shows its chain of thought explicitly.
Jeremy Howard and his team have released ModernBERT, a major upgrade to the BERT model they released six years ago. It comes in two sizes: 139M and 395M parameters. It's ideal for retrieval, classification, and entity extraction, and other components of a data pipeline.
AWS's Bedrock service has the ability to check the output of other models for hallucinations.
To make sure they aren't outdone by 12 Days of OpenAI, Google has announced Android XR, an operating system for extended reality headsets and glasses. Google doesn't plan to build their own hardware; they're partnering with Samsung, Qualcomm, and other manufacturers.
Also not to be outdone by 12 Days of OpenAI, Anthropic has announced Clio, a privacy- preserving approach to finding out how people use their models. That information will be used to improve Anthropic's understanding of safety issues and to build more helpful models.
Not to be outdone by 12 Days of OpenAI, Google has announced Gemini 2.0 Flash, a multimodal model that supports streaming for both input and output. The announcement also showcased Astra, an AI agent for smartphones. Neither is generally available yet.
OpenAI has released canvas, a new feature that combines programming with writing. Changes to the canvas (code or text) immediately become part of the context. Python code is executed in the browser using Pyodide (Wasm), rather than in a container (as with Code Interpreter).
Stripe has announced an agent toolkit that lets you build payments into agentic workflows. Stripe recommends using the toolkit in test mode until the application has been thoroughly validated.
Simon Willison shows how to run a GPT-4 class model (Llama 3.3 70B) on a reasonably well-equipped laptop (64GB MacBook Pro M2).
As part of their 12 Days of OpenAI series, OpenAI finally released their video generation model, Sora. It's free to ChatGPT Plus subscribers, though limited to 50 five-second video clips per month; a ChatGPT Pro account relaxes many of the limitations.
Researchers have shown that advanced AI models, including Claude 3 Opus and OpenAI o1, are capable of "scheming": working against the interests of their users to achieve their goals. Scheming includes subverting oversight mechanisms, intentionally delivering subpar results, and even taking steps to prevent shutdown or replacement. Hello, HAL?
Roaming RAG is a new technique for retrieval augmented generation that finds relevant content by searching through headings to navigate documents‚Äîlike a human might. It requires well-structured documents. A surprisingly simple idea, really.
Google has announced PaliGemma 2, a new version of its Gemma models that incorporates vision.
GPT-4-o1-preview is no more; the preview is now the real thing, OpenAI o1. In addition to advanced reasoning skills, the production release claims to be faster and to deliver more consistent results.
A group of AI agents in Minecraft behaved surprisingly like humans‚Äîeven developing jobs and religions. Is this a way to model how human groups collaborate?
One thing the AI industry needs desperately (aside from more power) is better benchmarks. Current benchmarks are closed, easily gamed (that's what AI does), and unreproducible, and they may not test anything meaningful. Better Bench is a framework for assessing benchmark quality.
Palmyra Creative, a new language model from Writer, promises the ability to develop "style" so that all AI-generated output won't sound boringly the same.
During training AI picks up biases from human data. When humans interact with the AI, there's a feedback loop that amplifies those biases.
Programming
Unicon may never become one of the top 20 (or top 100) programming languages, but it's a descendant of Icon, which was always my favorite language for string processing.
What do CAPTCHAs mean when LLM-equipped bots can successfully complete tasks set for humans?
egui, together with eframe, is a GUI library and framework for Rust. It's portable and runs natively (on macOS, Windows, Linux, and Android), on the web (using Wasm), and in many game engines.
For the archivist in us: The Manx project isn't about an island in the Irish Sea or about cats. It's a catalog of manuals for old computers.
Cerbrec is a graphical Python framework for deep learning. It's aimed at Python programmers who don't have sufficient expertise to build applications with PyTorch or other AI libraries.
GitHub has announced free access to GitHub Copilot for all current and new users. Free access gives you 2,000 code completions and 50 chat messages per month. They've also added the ability to use Claude 3.5 Sonnet in addition to GPT-4o.
Devin, the AI assisted coding tool that claims to support software development from beginning to end, including design and debugging, has reached general availability.
JSON5, also known as "JSON for humans," is a variant of JSON that has been designed for human readability so that it can be written and maintained by hand‚Äîfor example, in configuration files.
AWS has announced two significant new services: Aurora DSQL, which is a distributed SQL database, and S3 Tables, which supports data lakehouses through Apache Iceberg.
AutoFlow is an open source tool for creating a knowledge graph. It's based on TiDB (a vector database), LlamaIndex, and DSPy.
Security
Portspoof is a security tool that causes all 65,535 TCP ports to appear open for valid services. It emulates a valid service on every port. It makes it difficult for an attacker to determine which ports are actually open without probing each port.
Let's Encrypt, which issues the certificates that websites (and other applications) use to prove their identities, has announced short-lived certificates that expire after six days. Short-lived certificates increase security by minimizing exposure if a private key is compromised.
Because of the continued presence of attackers within telecommunications networks, the US FBI and CISA have recommended the use of encrypted communications protocols. (Though they still want backdoors into encryption systems, which would make them vulnerable to attack.)
A new phishing attack uses corrupted Word documents to bypass security checks. While the documents are corrupt, Word is able to recover them.
LLM Flowbreaking is a new class of attack against language models that prevent guardrails from stopping objectionable output from reaching the user. These attacks take advantage of race conditions in the application's interaction with users.
Bootkitty is a UEFI bootkit that targets secure boot on Ubuntu systems. It appears to have been developed by cybersecurity students in Korea, then leaked (possibly accidentally). It hasn't yet been found in the wild, but when it is, it will be a dangerous threat.
DEF CON has started a project to improve cybersecurity for water infrastructure in the US. They're starting with six water companies serving rural communities.
Quantum Computing
Google has built a quantum computing chip in which an error-corrected logical qubit can remain stable for an hour. It passes the "below threshold": the error rate decreases as physical qubits are added for error correction. The chip was built in Google's new fabrication facility.
Web
Google is adding "store reviews" to Chrome. Reviews are AI-generated summaries of reports from well-known sources that report scams and other issues.
Here's a how-to on building streaming text user interfaces on the web. Streaming text is almost a necessity for building AI-driven chatbots.
Biology
Yes, we can have virtual taste. A research group has developed a lollipop interface so that people can experience taste in virtual worlds.
Post topics: Radar Trends
Post tags: Signals
Share:   Share
About O'Reilly
Teach/write/train
Careers
O'Reilly news
Media coverage
Community partners
Affiliate program
Submit an RFP
Diversity
O'Reilly for marketers
Support
Contact us
Newsletters
Privacy policy
 
International
Australia & New Zealand
Hong Kong & Taiwan
India
Indonesia
Japan
Download the O'Reilly App
Take O'Reilly with you and learn anywhere, anytime on your phone and tablet.

Apple app store Google play store
Watch on your big screen
View all O'Reilly videos, Superstream events, and Meet the Expert sessions on your home TV.

Roku Payers and TVs Amazon appstore
Do not sell my personal information
O'Reilly home
¬© 2025, O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.

Terms of service ‚Ä¢ Privacy policy ‚Ä¢ Editorial independence
"""
```

```python
from ai_tools import ask_ai, parse_json_output

extract_insights_prompt = f"""
Extract from these raw contents, all the insights regarding AI and programming into the following structure format:

- AI insights
- Programming insights

Here are the raw contents:

{raw_contents_oreilly_tech_trends_january_2025}.

Your OUTPUT SHOULD ONLY BE A JSON OBJECT WITH THE FOLLOWING FIELDS:

- ai-insights
- programming-insights
- date

Output:
"""

output = ask_ai(extract_insights_prompt)
parsed_output_json = parse_json_output(output)
parsed_output_json
```

```python
# Convert JSON insights to markdown format
print("# Tech Trends Report - " + parsed_output_json['date'])
print("\n## AI Insights")
for insight in parsed_output_json['ai-insights']:
    print(f"- {insight}")
    
print("\n## Programming Insights") 
for insight in parsed_output_json['programming-insights']:
    print(f"- {insight}")
```

We can now easily transform this into a table to store our own databaset of the recent tech trends!

```python
import pandas as pd

# Create lists of insights and dates
ai_insights = parsed_output_json['ai-insights']
prog_insights = parsed_output_json['programming-insights']
dates = [parsed_output_json['date']] * max(len(ai_insights), len(prog_insights))

# Create a DataFrame
df = pd.DataFrame({
    'AI Insights': ai_insights + [None] * (len(prog_insights) - len(ai_insights)) if len(prog_insights) > len(ai_insights) else ai_insights,
    'Programming Insights': prog_insights + [None] * (len(ai_insights) - len(prog_insights)) if len(ai_insights) > len(prog_insights) else prog_insights,
    'Date': dates
})

# Display the table
display(df)
```

```python
import requests
from bs4 import BeautifulSoup

def scrape_oreilly_ai_programming_news(url):
    """
    Scrape AI and programming-related content from O'Reilly Radar.
    """
    try:
        # Send request to the website
        response = requests.get(url)
        response.raise_for_status()
        
        # Parse HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find AI and Programming sections
        ai_section = soup.find('h2', string=lambda text: text and 'AI' in text)
        programming_section = soup.find('h2', string=lambda text: text and 'Programming' in text)
        
        news_items = []
        
        # Extract AI content
        if ai_section:
            ai_content = ai_section.find_next('ul')
            if ai_content:
                news_items.extend([li.text.strip() for li in ai_content.find_all('li')])
        
        # Extract Programming content
        if programming_section:
            programming_content = programming_section.find_next('ul')
            if programming_content:
                news_items.extend([li.text.strip() for li in programming_content.find_all('li')])
        
        return news_items
    
    except requests.exceptions.RequestException as e:
        print(f"Error scraping website: {e}")
        return []

# Example usage (replace with actual O'Reilly Radar URL)
oreilly_url = "https://www.oreilly.com/radar/radar-trends-to-watch-january-2025/"
ai_programming_news = scrape_oreilly_ai_programming_news(oreilly_url)

if ai_programming_news:
    print("\nLatest O'Reilly AI & Programming News:")
    for idx, news in enumerate(ai_programming_news, 1):
        print(f"{idx}. {news}")
```

```python

```

---

## notebooks/03-automation-projects/03-data-analysis.ipynb

1. Showing a sample of the data to an AI model like ChatGPT
2. Asking for the Python code for the analysis
3. Inspecting and running the code
4. Automating it locally in our machines to run forever

```python
import pandas as pd

df = pd.read_csv("../assets/stock-trading-data.csv")

df
```

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# import seaborn as sns
import os

# Load data
def load_data(file_path):
    df = pd.read_csv(file_path, parse_dates=['Date'])
    df.dropna(inplace=True)  # Handle missing values
    df.sort_values(by=['Date'], inplace=True)
    return df

# Compute daily returns and rolling volatility
def compute_returns_volatility(df):
    df['Daily Return'] = df.groupby('Stock Symbol')['Close Price'].pct_change()
    df['Rolling Volatility'] = df.groupby('Stock Symbol')['Daily Return'].rolling(window=20).std().reset_index(level=0, drop=True)
    return df

# Compute SMA and EMA
def compute_moving_averages(df):
    df['SMA_20'] = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.rolling(window=20).mean())
    df['EMA_20'] = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.ewm(span=20, adjust=False).mean())
    return df

# Compute Bollinger Bands
def compute_bollinger_bands(df):
    rolling_mean = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.rolling(window=20).mean())
    rolling_std = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.rolling(window=20).std())
    df['Upper Band'] = rolling_mean + (rolling_std * 2)
    df['Lower Band'] = rolling_mean - (rolling_std * 2)
    return df

# Compute MACD
def compute_macd(df):
    df['EMA_12'] = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.ewm(span=12, adjust=False).mean())
    df['EMA_26'] = df.groupby('Stock Symbol')['Close Price'].transform(lambda x: x.ewm(span=26, adjust=False).mean())
    df['MACD'] = df['EMA_12'] - df['EMA_26']
    df['Signal Line'] = df.groupby('Stock Symbol')['MACD'].transform(lambda x: x.ewm(span=9, adjust=False).mean())
    return df

# Generate visualizations
def plot_stock_prices(df, symbol):
    stock_df = df[df['Stock Symbol'] == symbol]
    plt.figure(figsize=(12, 6))
    plt.plot(stock_df['Date'], stock_df['Close Price'], label='Close Price')
    plt.plot(stock_df['Date'], stock_df['SMA_20'], label='20-day SMA', linestyle='dashed')
    plt.plot(stock_df['Date'], stock_df['EMA_20'], label='20-day EMA', linestyle='dotted')
    plt.fill_between(stock_df['Date'], stock_df['Upper Band'], stock_df['Lower Band'], color='gray', alpha=0.3, label='Bollinger Bands')
    plt.title(f'{symbol} Stock Price Analysis')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid()
    plt.savefig(f'{symbol}_stock_price.png')
    plt.close()

# Generate heatmap of correlations
def plot_correlation_heatmap(df):
    numeric_cols = ['Close Price', 'Daily Return', 'Rolling Volatility', 'SMA_20', 'EMA_20', 'RSI (Relative Strength Index)', 'MACD', 'Signal Line']
    correlation_matrix = df[numeric_cols].corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Stock Metric Correlation Heatmap')
    plt.savefig('correlation_heatmap.png')
    plt.close()

# Export analysis to Excel
def export_to_excel(df):
    output_file = 'financial_analysis_report.xlsx'
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='Stock Data', index=False)
        workbook = writer.book
        worksheet = writer.sheets['Stock Data']
        worksheet.insert_image('J2', 'AAPL_stock_price.png')
        worksheet.insert_image('J20', 'correlation_heatmap.png')
    print(f'Report saved as {output_file}')

# Main function



df = compute_returns_volatility(df)
df = compute_moving_averages(df)
df = compute_bollinger_bands(df)
df = compute_macd(df)

plot_stock_prices(df, 'AAPL')
# plot_correlation_heatmap(df)
# export_to_excel(df)
```

```python

```

```python

```



```python
![](./)
```

---

## notebooks/04-exercises/01-data-types-and-variables.ipynb

1. Identify the Data Type
Write a Python script that defines the following values and prints their data types using type():

x = 42
y = 3.14
z = "Hello, Python!"
w = True

```python
x = 42
y = 3.14
z = "Hello, Python!"
w = True

print(type(x))
print(type(y))
print(type(z))
print(type(w))
```

---

## notebooks/04-exercises/02-functions.ipynb

# CB question: 'can we define optional parameters?'

```python
def some_function_with_parameters(a=1,b=5,c=10):
    print(a)
    print(b)
    print(c)

some_function_with_parameters()
```

# KG question: 'how useful is random funtcion?'

```python
import random
```

```python
random.random()
```

```python
random.random()
```

`round()` built in function in Python

```python
round(1.3334567, 3)
```

```python
round(1.3334567, 2)
```

```python
round(1.3334567, 5)
```

# SP question: 'do we need to define a return type for a function?'

```python
def write_something(name) -> str:
    return f"Hello! {name}"



write_something("Lucas")    
```

```python
write_something()
```

```python
write_something([1,2,3])
```

# PP & PJ question: 'I think he was asking the same thing that I asked before: "some programming languages have the concept of passing arguments/parameters by Value or by Reference, does this also exist in Python?"'

```python
variable = 10

print(variable)
```

```python
def prints_number_sum(number):
    print(number + number)
```

```python
prints_number_sum(variable)
```

```python
prints_number_sum(number=25)
```

```python
prints_number_sum(number=variable)
```

# PP & PJ updated question: 'Not exactly, it was something related with this "When you pass an argument by reference, you pass a pointer to the value in memory. The function operates on the argument. When a function changes the value of an argument passed by reference, the original value changes. When you pass an argument by value, you pass a copy of the value in memory.'

```python
variable = 10

print(variable)
```

```python
def change_variable(variable1):
    
    variable1 = variable1 + 10
    
    return variable1

variable = 20
change_variable(variable)
print(variable)
```

```python
def modify_list(lst):
    lst.append(4)  # Modifies the original list

my_list = [1, 2, 3]
modify_list(my_list)
print(my_list)  # Output: [1, 2, 3, 4] (modified)
```

```python

```

---

## notebooks/04-exercises/03-conditionals-and-files.ipynb

# SV question: 'if and if -> use case explain it one more time.'

```python
name = "Lucas"

age = 33
#      True      AND  TRUE
if name=="Lucas" and age>30:
    # This gets executed!
    print("Lucas is older than 30!")
```

# PP question: 'how do we import the necessary modules when we are using Google Collab for the Notebooks?'

```python
from openai import OpenAI
import anthropic
import ollama
import json

def ask_ai(prompt, model_name="gpt-4o-mini"):
    """
    Send prompt to an LLM and get output text back.    
    """
    if "claude" in model_name:
        client = anthropic.Anthropic()

        # Send a message to the Claude AI
        response = client.messages.create(
            model=model_name,
            messages=[
                {
                    "role": "user", "content": prompt
                }
                ],
            max_tokens=4000,
        )
        output = response.content[0].text
        return output        
    else:
        client = OpenAI()
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
        )
        output = response.choices[0].message.content
        return output


def ask_local_ai(prompt, model_name="llama3.2", structured=False):
    """
    Send prompt to a local LLM and get output text back.
    """
    client = ollama.Client()
    if structured:
        response = client.chat(model=model_name, 
                               messages=
                               [
                                   {"role": "user", "content": prompt}
                                ],
                               format='json')
        return response.message.content
    else:
        response = client.chat(model=model_name, 
                               messages=
                               [
                                   {"role": "user", "content": prompt}
                                ])
        return response.message.content


def parse_dates_list(output_str):
    """
    Parse the string output containing a Python list of dates into an actual Python list.
    
    Args:
        output_str (str): String containing a Python list representation of dates
        
    Returns:
        list: List of date strings
    """
    # Remove markdown code block formatting if present
    output_str = output_str.replace('```python', '').replace('```', '').strip()
    
    # Safely evaluate the string as a Python expression
    dates_list = eval(output_str)
    
    return dates_list

def parse_json_output(json_str):
    """
    This function parses the JSON output from the AI and removes the markdown code block markers if present.
    """
    # Remove markdown code block markers if present
    json_str = json_str.replace('```json', '').replace('```', '').strip()
    
    # Parse the JSON string into a Python dictionary
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        print("Error: Could not parse JSON string")
        return None
```

# CA question: 'Can you explain how to try to open a file, but if it exists create the next rev of the file?  
the rev of a file meaning if the file name ends in _1 and it exists then write/create a filename ending _2 thanks'

```python
import os

file_name = "./ca-file1.txt"

if os.path.exists(file_name):
    file_name = file_name.replace("1", "2")
    with open(file_name, "w") as f:
        f.write(file_name)
```

```python

```

```python

```

---

## notebooks/04-exercises/04-day1-recap.ipynb

Recap the concepts and syntax we've learned yesterday:

1. Scripting = solving small scale tasks != programming which is about more serious software engineering
2. Python syntax
   1. Data types
   2. Operations
   3. Comparisons/conditionals

THe basics in Python

```python
print("Hi everyone!")

print(len("Hello, world!"))
```

Data types, operations, variables

```python
10 + 10
```

```python
type(10)
```

```python
type("Lucas is doing a recap")
```

```python
type(1.5)
```

```python
variable = 10

variable_new = "Lucas"

print(f"{variable_new} will teach {variable}x better than yesterday")
```

```python
"This " + "is a" + " course about automation"
```

```python
a = 10

if a<11:
    print("10 is smaller than 11!")
```

```python
def calculate_total_price_product(price, tax_rate):
    total = price + (price * tax_rate)
    return total

total_price = calculate_total_price_product(100, 0.1)
print(total_price)
```

```python
is_good = False

if is_good:
    print("Is good!")
else:
    print("Is not good!")
```

```python
names = ["Lucas", "AS", "BH", "Patrick", "FG", "SP"]

names[0]
```

```python
names[1]
```

```python
first_2_names = names[0:2]

first_2_names
```

```python
names = ["Lucas", "AS", "BH", "Patrick", "FG", "SP"]
for name in names:
    print("Name * 10!")
    print(name*10)
```

```python
travel_gear = {
    "clothing": ["shirts", "pants", "socks", "underwear", "jacket"],
    "toiletries": ["toothbrush", "toothpaste", "deodorant", "shampoo"],
    "electronics": ["phone", "charger", "laptop", "power bank"],
    "documents": ["passport", "tickets", "visa", "insurance"],
    "accessories": ["sunglasses", "hat", "wallet", "backpack"]
}

travel_gear["clothing"]
```

```python
import pandas as pd


df = pd.read_csv("../assets/stock-trading-data.csv")

df["Date"]
```

```python
10 > 2
```

```python
10 < 5
```

```python
def greetings_folks(names):
    for name in names:
        print(f"Hello! {name}")

greetings_folks(["Lucas", "BH", "SP", "AS", "FG"])
```

```python
file_path = "../assets/papers/paper1.txt"

with open(file_path, "r") as f:
    print(f.read())
```

What if Ii want to work with tabular data?



```python

```

---

## notebooks/04-exercises/tutorial_learning_python_with_pdfs.ipynb

# üìÑ Building a Simple Report-Style PDF with **ReportLab**

You‚Äôre about to walk through every line of a small script that spits out a finished PDF‚Äîtitle page, paragraph, table, bar chart, and even a little ‚Äúlogo‚Äù drawn on the page. No prior ReportLab or PDF-generation experience required. Just follow along, run the code cells, and a fresh `sample_report.pdf` will appear on your desktop.

---

## Prerequisites



```python
# Run this once in a terminal or Jupyter cell
!pip install reportlab

```


That‚Äôs it. ReportLab is pure-Python, no system libraries to compile.

---

## 1. Imports and Dependencies



```python
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.graphics.shapes import Drawing, Rect, Circle
from reportlab.graphics.charts.barcharts import VerticalBarChart
from reportlab.graphics import renderPDF
from reportlab.lib.enums import TA_CENTER
import os
```


**What‚Äôs going on?**

* **`platypus`** ‚Äî ReportLab‚Äôs ‚Äúbuild-a-document‚Äù framework. You feed it ‚Äúflowables‚Äù (Paragraphs, Tables, etc.) and it lays them out.
* **`lib.pagesizes.letter`** ‚Äî A tuple with width & height in points (1 pt = 1/72 inch).
* **`Drawing`, `Rect`, `Circle`, `VerticalBarChart`** ‚Äî Shapes and charts live in the graphics sub-package.
* **`os`** ‚Äî Only used to figure out where your Desktop folder lives.

---

## 2. Decide Where to Save the PDF



```python
desktop_path = os.path.expanduser("~/Desktop")   # Works on macOS & Linux
filename = os.path.join(desktop_path, "sample_report.pdf")
```


The `~` expands to your home directory. If you‚Äôre on Windows, swap `~/Desktop` for something like `C:/Users/<YOU>/Desktop` or just any folder you have write access to.

---

## 3. Create the **SimpleDocTemplate**



```python
doc = SimpleDocTemplate(
    filename,
    pagesize=letter,
    topMargin=0.5 * inch      # ¬Ω-inch top margin
)
story = []  # A plain Python list that will hold our ‚Äúflowables‚Äù

```


`SimpleDocTemplate` is the easiest wrapper around a PDF file. When we later call `doc.build(story)`, it writes the whole thing out.

---

## 4. Define Text Styles



```python
styles = getSampleStyleSheet()  # Built-in defaults

title_style = ParagraphStyle(
    "CustomTitle",
    parent=styles["Heading1"],
    fontSize=24,
    spaceAfter=30,          # pts of space below
    alignment=TA_CENTER,
    textColor=colors.darkblue
)

subtitle_style = ParagraphStyle(
    "CustomSubtitle",
    parent=styles["Heading2"],
    fontSize=16,
    spaceAfter=20,
    alignment=TA_CENTER,
    textColor=colors.grey
)

```


Paragraph styles decide **font, size, colour, alignment, spacing,** everything. We inherit from the default headings but tweak what we need.

---

## 5. Add Title, Subtitle, and Body Text



```python
from reportlab.platypus import Paragraph, Spacer

# Title & subtitle
story.append(Paragraph("Company Annual Report", title_style))
story.append(Paragraph("Financial Performance Overview 2024", subtitle_style))

# Blank vertical space (exactly 20 pts tall)
story.append(Spacer(1, 20))

# Body paragraph
paragraph_text = """
This comprehensive report presents our company's outstanding performance throughout 2024. 
Our strategic initiatives have resulted in significant growth across all key metrics, 
demonstrating our commitment to excellence and innovation. The following data showcases 
our achievements in revenue, customer satisfaction, and market expansion. We continue 
to build on our strong foundation while exploring new opportunities for sustainable growth.
"""
story.append(Paragraph(paragraph_text, styles["Normal"]))
story.append(Spacer(1, 20))

```


*Don‚Äôt overthink `Spacer`‚Äîit‚Äôs literally an invisible rectangle that forces vertical breathing room.*

---

## 6. Build a Quick Table



```python
table_data = [
    ["Quarter", "Revenue ($M)", "Growth (%)"],
    ["Q1 2024", "125.4", "12.3"],
    ["Q2 2024", "142.8", "18.7"],
    ["Q3 2024", "156.2", "15.2"],
    ["Q4 2024", "178.9", "22.1"],
]

table = Table(table_data, colWidths=[1.5 * inch] * 3)

table.setStyle(TableStyle([
    ("BACKGROUND", (0, 0), (-1, 0), colors.darkblue),   # Header row fill
    ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
    ("ALIGN",     (0, 0), (-1, -1), "CENTER"),
    ("FONTNAME",  (0, 0), (-1, 0), "Helvetica-Bold"),
    ("FONTSIZE",  (0, 0), (-1, 0), 12),
    ("BOTTOMPADDING", (0, 0), (-1, 0), 12),
    ("BACKGROUND", (0, 1), (-1, -1), colors.lightgrey),
    ("GRID", (0, 0), (-1, -1), 1, colors.black),
]))
story.append(table)
story.append(Spacer(1, 30))

```


Coordinate scheme: `(col, row)`, zero-based. `(0,0)` is top-left. `(-1,0)` means ‚Äúlast column in row 0‚Äù. Styling is chainable; set as many rules as you want in a single shot.

---

## 7. Draw a Bar Chart and a Simple ‚ÄúLogo‚Äù



```python
drawing = Drawing(400, 200)  # Canvas for vector graphics

# Faux logo: blue rectangle + orange circle
drawing.add(Rect(10, 150, 60, 30, fillColor=colors.darkblue, strokeColor=colors.black))
drawing.add(Circle(90, 165, 15, fillColor=colors.orange, strokeColor=colors.black))

# Vertical bar chart
chart = VerticalBarChart()
chart.x, chart.y = 120, 50
chart.width, chart.height = 250, 125
chart.data = [[125.4, 142.8, 156.2, 178.9]]
chart.categoryAxis.categoryNames = ["Q1", "Q2", "Q3", "Q4"]

chart.valueAxis.valueMin = 0
chart.valueAxis.valueMax = 200
chart.bars[0].fillColor = colors.darkblue
chart.bars[0].strokeColor = colors.black

drawing.add(chart)
story.append(drawing)

```


### What‚Äôs a `Drawing`?

Think of it as an SVG-ish container. Anything you add lives in its own coordinate system (origin at bottom-left). When we tuck the entire `Drawing` into `story`, ReportLab converts it to PDF vector commands.

---

## 8. Compile Everything into the PDF



```python
doc.build(story)
print(f"PDF generated successfully ‚áí {filename}")

```


`build()` chews through the flowables top-to-bottom, lays them out, writes pages, and closes the file. If there‚Äôs any error in your flowables, you‚Äôll see it now.

---

## 9. Wrap It in a Function (Optional but Clean)



```python
def create_pdf():
    # all the code above goes here
    pass

if __name__ == "__main__":
    create_pdf()

```


This ‚Äúmain guard‚Äù means:

* Running the script normally (`python myscript.py`) ‚Üí it executes.
* Importing the script from **another** Python file ‚Üí it does **not** auto-run and make random PDFs.

In a Jupyter notebook you can **skip** the guard and just call `create_pdf()` directly.

---

## üèÅ Test It



```python
create_pdf()
```


Head to your desktop‚Äî`sample_report.pdf` should be waiting. Open it. You‚Äôll see:

1. **Dark-blue title** centered, size 24 pt
2. **Grey subtitle** just below, size 16 pt
3. A paragraph of body text
4. A light-grey table of quarterly numbers
5. A mini ‚Äúlogo‚Äù plus a bar chart showing revenue growth

If something‚Äôs off (table columns mis-aligned, fonts ugly, chart too small), tweak numbers and rerun. PDF generation is instant, so experiment fearlessly.

---

---

## notebooks/05-how-to-learn-python/file.md

<details>
<summary>View file.md</summary>



</details>

---

## notebooks/05-how-to-learn-python/how-to-learn-python.ipynb

```python
print("hi")
```

# How to Learn Python Skills Fast

We're gonna start with this problem from 'LA' (shout out)

i have quite a number of files on my computer. I would like to make an app to go over those files and:

1. clasify them into folders

2. create an index to find the file(s) treating a subject

3. create a resume of specific file

Here‚Äôs a clearer and more structured version of your problem, expressed as a task someone could solve with Python:

---

### Task: File Organization and Indexing App

You want to build a Python app that can manage and organize a collection of files on your computer. The app should be able to:

1. **Classify files into folders**

   * Automatically sort files into categories (e.g., by type, topic, or metadata) and move them into the appropriate folders.

2. **Create a searchable index**

   * Build an index or database that makes it easy to search for files based on their content or subject matter.

3. **Summarize file contents**

   * For specific files (like documents or PDFs), generate a short summary of their content so you can quickly understand what they are about without opening them.

---

Do you want me to also **break this down into smaller, beginner-friendly Python subtasks** (e.g., "Step 1: scan all files in a directory," "Step 2: classify by extension") so you can gradually build towards the full app?


# Task Breakdown 

Got it üëç Here‚Äôs a clear step-by-step path with **10 tiny Python challenges** that build up toward your file organization app. No code, just the challenges:

1. **Work with paths**

   * Write a Python script that prints the current working directory and lists all files in it.

2. **Filter files by type**

   * From a folder, list only files with a specific extension (e.g., `.txt` or `.pdf`).
   

3. **Move files**

   * Write a script that moves a file from one folder to another.

4. **Batch move by type**

   * Automatically move all `.txt` files into a "texts" folder and all `.jpg` files into an "images" folder.

5. **Read file content**

   * Open a `.txt` file and print the first 5 lines.

6. **Index file names**

   * Create a Python dictionary where keys are file names and values are their file paths.

7. **Search the index**

   * Ask the user for a keyword and return all file names that contain it.

8. **Summarize text files**

   * For each `.txt` file, generate a "summary" by printing the first 2 sentences.

9. **Index with summaries**

   * Extend your dictionary so that each file also stores a short summary.

10. **Bring it together**

* Make a script that:

  1. Classifies files into folders,
  2. Creates an index of all files with paths and summaries,
  3. Lets the user search the index for keywords.

---

Do you want me to also suggest which **Python topics/syntax** each challenge will teach you along the way (so you know what to focus on before trying each one)?


1. **Work with paths**

   * Write a Python script that prints the current working directory and lists all files in it.


```python
import os

current_dir = os.getcwd()

print(current_dir)

files_in_current_dir = os.listdir(current_dir)

print(files_in_current_dir)
```

# Task 2

2. **Filter files by type**

   * From a folder, list only files with a specific extension (e.g., `.txt` or `.pdf`).

```python
import os

current_folder_path = os.getcwd()

lists_extensions = [".txt", ".pdf"]

files_in_current_dir = os.listdir(current_folder_path)

files_in_current_dir

# how to check if a string contains another?
txt_pdf_files = []
for file in files_in_current_dir:
    for ext in lists_extensions:
        if file.endswith(ext):
            txt_pdf_files.append(file)
            print(file)

txt_pdf_files
```

```python

```

```python

```

---

## notebooks/05-how-to-learn-python/learn-python-talking-to-ai.ipynb

# Exercise 1: Contact Book (Dictionaries & Functions)

Build a contact book program that stores names, emails, and phone numbers.
Your task:

Create an empty dictionary called contacts
1. Write a function add_contact(name, email, phone) that adds a contact
2. Write a function find_contact(name) that returns the contact info or "Not found"
3. Write a function list_contacts() that prints all contacts

Test it: Add 3 contacts, list them all, then search for one by name.

```python
contacts = {}

def add_contact(name, email, phone):
    contacts[name] = [email, phone]

    return contacts

def find_contact(name):
    if name in contacts.keys():
        return contacts[name]
    else:
        return "Not found"

def list_contacts():
    for ct in contacts.keys():
        print(ct)

    return "done"

    

contacts = add_contact("lucas", "something@nothing.com", "111-222")
contacts
```

```python
contacts = add_contact("john", "nothing@something.com", "222-333")
contacts               
```

```python
find_contact("lucas")
```

```python
def list_contacts():
    for ct in contacts.keys():
        print(ct)

    return "done"

    

list_contacts()
```

```python
import sys
sys.path.append("../../scripts/")
from ai_tools import ask_ai
```

```python
from IPython.display import Markdown

prompt = "I want to practice Python basics give me one exercise output should be just the exercise as markdown"

output = ask_ai(prompt)

Markdown(output)
```

```python

```

---

## notebooks/assets/automation_recipes_prompts/financial-analysis-prompt.md

<details>
<summary>View financial-analysis-prompt.md</summary>

2Ô∏è‚É£ Financial & Data Analysis Automation
‚ÄúGenerate a Python script that performs financial analysis on a CSV file containing stock trading data. The dataset includes columns: Date, Stock Symbol, Open Price, Close Price, Volume, Moving Average (50-day), and RSI (Relative Strength Index). The script should:‚Äù

Detailed Steps in the Prompt:
	1.	Load & Clean Data: Use pandas to load the CSV, handle missing values, and ensure data types are correct.
	2.	Key Financial Metrics Calculation:
	‚Ä¢	Compute daily returns and rolling volatility.
	‚Ä¢	Calculate a simple moving average (SMA) and exponential moving average (EMA) for each stock.
	‚Ä¢	Generate Bollinger Bands for price movement analysis.
	‚Ä¢	Compute Relative Strength Index (RSI) and MACD for trend analysis.
	3.	Visualization & Reporting:
	‚Ä¢	Use matplotlib and seaborn to generate:
	‚Ä¢	Line charts for stock prices over time.
	‚Ä¢	A candlestick chart using mplfinance.
	‚Ä¢	A heatmap of correlations between different stock metrics.
	‚Ä¢	Save these charts as PNG images.
	4.	Export Analysis Report:
	‚Ä¢	Generate an Excel report with the analyzed data and charts.
	‚Ä¢	Use xlsxwriter to format and insert images into the Excel file.

</details>

---

## notebooks/assets/automation_recipes_prompts/outloook-send-emails-prompt.md

<details>
<summary>View outloook-send-emails-prompt.md</summary>

1Ô∏è‚É£ Automating Outlook Table Processing & Sending Emails

Example Prompt:

‚ÄúGenerate a Python script using win32com.client to read the latest email in an Outlook inbox, extract a table from the email body, process the data to extract relevant information (e.g., rows where ‚ÄòStatus‚Äô is ‚ÄòPending‚Äô and amounts greater than $1,000), and send a summary email to multiple recipients. The script should format the extracted data into an HTML table and send an email with a custom message.‚Äù

Additional Requirements for the Script:
	‚Ä¢	The script should use win32com.client to access Outlook emails.
	‚Ä¢	It should identify and extract the latest email containing a table.
	‚Ä¢	Use pandas to parse and filter the table based on business logic (e.g., only extract rows where a certain condition is met).
	‚Ä¢	Format the extracted data as an HTML email.
	‚Ä¢	Send an email to multiple recipients using win32com.client with a customized message.

</details>

---

## notebooks/assets/automation_recipes_prompts/pdf-tasks-prompts.md

<details>
<summary>View pdf-tasks-prompts.md</summary>



# Prompt Examples for PDF Tasks

1Ô∏è‚É£ Extracting Text or Data from PDFs

Prompt:
‚ÄúGenerate a Python script that extracts text and tables from a PDF file. The script should use PyMuPDF (fitz) to extract text and pdfplumber to extract structured tables. It should save the extracted data into a .txt file for text and a .csv file for tables. Handle multi-page PDFs and ensure proper formatting.‚Äù

2Ô∏è‚É£ Merging, Splitting, and Organizing PDFs

Prompt:
‚ÄúGenerate a Python script using PyPDF2 to merge multiple PDFs into a single file and split a given PDF into separate pages. The script should prompt the user to specify which pages to extract and save them as individual files. Ensure the output filenames are automatically numbered.‚Äù

3Ô∏è‚É£ Filling Out and Signing PDFs

Prompt:
‚ÄúGenerate a Python script that fills out a PDF form using pypdf. The script should take input data from a JSON file, populate the corresponding fields in the PDF, and save the completed form. Additionally, the script should overlay a digital signature image on a specified page using reportlab.‚Äù

4Ô∏è‚É£ Annotating, Reviewing, and Commenting on PDFs

Prompt:
‚ÄúGenerate a Python script that adds annotations to a PDF using PyMuPDF (fitz). The script should allow users to highlight text, add comments, and insert sticky notes at specific locations. The annotations should be saved in a new PDF file.‚Äù

5Ô∏è‚É£ Converting PDFs to Other Formats

Prompt:
‚ÄúGenerate a Python script that converts a PDF into different formats using pdf2docx (to Word), camelot (to extract tables as CSV), and pdf2image (to convert pages into PNG images). The script should allow users to specify the desired output format and save the converted files accordingly.‚Äù



</details>

---

## notebooks/assets/automation_recipes_prompts/ppt-automation-prompt.md

<details>
<summary>View ppt-automation-prompt.md</summary>

# Prompt 1
Please write a Python script that performs the following tasks:

- Accesses the "input" folder and reads all Excel files within it.
- For each file, it retrieves the financial data located in the first worksheet, from column A to P, and excludes any rows with missing values.
- Group the entire data by the "Product" column (C) and sum up the sales column (J) for each group.
- Using the grouped data, the script should create a chart using the seaborn library for each file, and save it in the "charts" folder. If the folder does not exist, the script should create it. . The chart should include a title, appropriate axis labels, and a legend.
- The script should then create a new PowerPoint presentation, insert a slide for each chart, and above the chart, include a title. The title should be the respective excel file name without file extension. And make sure that the chart and title do not overlap
- The PowerPoint presentation should be saved in the same directory as the input files and named "financial_data.pptx".
- The script should be robust and handle any potential errors gracefully, providing appropriate error messages and notifications, and also by including proper error handling mechanisms.

Input data example in here: 

- https://github.com/Sven-Bo/automate-office-tasks-using-chatgpt-python/tree/master/02_Automate_PowerPoint/input

# Prompt 2

Please save the final PowerPoint presentation in the same directory as the Python script

```python
import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pptx import Presentation
from pptx.util import Inches

# Determine the directory of the script
script_dir = os.path.dirname(os.path.abspath(__file__))

input_folder = os.path.join(script_dir, 'input')
charts_folder = os.path.join(script_dir, 'charts')
ppt_file = 'financial_data.pptx'

# Create the charts folder if it doesn't exist
if not os.path.exists(charts_folder):
    os.mkdir(charts_folder)

# Create a new PowerPoint presentation
prs = Presentation()

# Iterate through all Excel files in the input folder
for excel_file in os.listdir(input_folder):
    if not excel_file.endswith('.xlsx'):
        continue

    # Read the financial data from the first worksheet of the Excel file
    file_path = os.path.join(input_folder, excel_file)
    df = pd.read_excel(file_path, sheet_name=0, usecols="A:P")
    df = df.dropna()

    # Group the data by the "Product" column and sum up the "Sales" column
    grouped = df.groupby('Product').sum()['Sales']

    # Create a chart using the seaborn library
    sns.barplot(x=grouped.index, y=grouped.values)
    plt.title(excel_file)
    plt.xlabel('Product')
    plt.ylabel('Sales')
    plt.tight_layout()

    # Save the chart to the charts folder
    chart_file = excel_file.replace('.xlsx', '.png')
    chart_path = os.path.join(charts_folder, chart_file)
    plt.savefig(chart_path)

    # Add a slide to the PowerPoint presentation and insert the chart and title
    slide = prs.slides.add_slide(prs.slide_layouts[5])
    title = slide.shapes.title
    title.text = excel_file.replace('.xlsx','')

    chart_file = chart_path
    left = Inches(0.5)
    top = Inches(1)
    width = Inches(9)
    height = Inches(6)
    slide.shapes.add_picture(chart_file, left, top, width=width, height=height)

# Save the PowerPoint presentation in the same directory as the script
ppt_path = os.path.join(script_dir, ppt_file)
prs.save(ppt_path)
```


</details>

---

## notebooks/assets/automation_recipes_prompts/prompt_learn_ai_python.md

<details>
<summary>View prompt_learn_ai_python.md</summary>

I'm learning Python and I just covered these topics:
"""
Python syntax, different data types, variables, comparators, conditionals, writing reading files in files, working with tabular, lists, loops, dictionaries, 
"""
here is the task I want to do:
I want to automate data analysis of .csv files and create automatic reports into a .pdf.

Can you break my task down into very smaller python script/task that I can practice?

</details>

---

## notebooks/assets/automation_recipes_prompts/uv-one-shot-prompt-to-script.md

<details>
<summary>View uv-one-shot-prompt-to-script.md</summary>

You are a Python automation engineer. Generate a single-file Python script that:

1. Can be executed directly using `uv run`.
2. Uses inline metadata at the top of the file (PEP 723 style) to declare:
   - Python version requirement (`requires-python = ">=3.11"` or higher).
   - A list of dependencies under `dependencies = []`.

3. Uses this format for metadata:
   ```python
   #!/usr/bin/env -S uv run --script
   # /// script
   # requires-python = ">=3.11"
   # dependencies = ["<your dependencies>"]
   # ///

</details>

---

## notebooks/assets/documents/ai-tools.md

<details>
<summary>View ai-tools.md</summary>

AI Hacks
	1. AI Tools
		1. General
			1. ChatGPT/Claude/Gemini/Ollama/llama studio/deep seek
		2. Search
			1. Perplexity for search
			2. ChatGPT + search
			3. Gemini 1.5 Pro Deep Research
		3. Claude/ChatGPT Projects
		4. Artifacts in Claude/ChatGPT
		5. ChatGPT Canvas
	2. Whiteboard
		1. Prompt Templates
		2. Chaining
		3. Hybrid approach
- Let it see your screen - Gemini/ChatGPT desktop app
- [ ] Paste code + error ask it to debug
- [ ] When a task is simple but has many repeatable steps
		- [ ] AI to generate quick Python script in correct proper runnable format
		- [ ] copy - paste - run - finish
- [ ] When AI makes a mistake save it for later as your own personal benchmark
- [ ] Build app with AI that takes in data with a certain structure and outputs desirable output, format, etc.... then make a prompt template that produces the data into the format acceptable by that app (done) :LiArrowBigRight: example my Quiz app.
- [ ] [Ask for full scripts with uv style package management](https://claude.ai/chat/cf3cb3d1-5b3e-4ea8-8584-5b6ceada7d84)  [[executing standalone scripts in Python with uv]]
	- [another example with very little debugging](https://claude.ai/chat/3276e7df-0dcf-4159-a6c0-df0ff9601e85)
- [ ] Show LLM how to call an API (in the prompt) then ask it to create something with that api.
- [ ] Writing Python code with Claude Projects + relevant documentation
- [ ] [[meta prompts]]
- [ ] [[Context Building Workflow with LLMs]]
- [ ] [[Test-Driven Development with AI (AI Writes Tests First)]]
- [ ] [[voice driven prompting]]
- [ ] [[Treat AI models as a team of specialized workers]]

</details>

---

## notebooks/assets/documents/trouble_shooting_setups.md

<details>
<summary>View trouble_shooting_setups.md</summary>

# Complete Beginner Setup Guide

## üçé Mac Setup (Complete from scratch)

**Step 1: Install Homebrew (package manager)**
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

**Step 2: Install Git**
```bash
brew install git
```

**Step 3: Fix permissions and install UV**
```bash
chmod +w ~/.bash_profile ~/.zshrc
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.zshrc
```

**Step 4: Setup the course**
```bash
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course
uv venv
source .venv/bin/activate
uv pip install jupyterlab ipykernel openai pandas anthropic ollama requests beautifulsoup4 matplotlib ipywidgets playwright
python -m ipykernel install --user --name=oreilly-python-ai --display-name "O'Reilly Python AI"
playwright install
echo "‚úÖ Setup complete! Run: jupyter lab"
```

---

## ü™ü Windows Setup (Complete from scratch)

**Step 1: Install Git (Download and run installer)**
- Go to: https://git-scm.com/download/win
- Download and install with default settings

**Step 2: Open PowerShell as Administrator and install UV**
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**Step 3: Close and reopen PowerShell (normal user), then setup course**
```powershell
git clone https://github.com/EnkrateiaLucca/oreilly-python-course
cd oreilly-python-course
uv venv
.venv\Scripts\activate
uv pip install jupyterlab ipykernel openai pandas anthropic ollama requests beautifulsoup4 matplotlib ipywidgets playwright
python -m ipykernel install --user --name=oreilly-python-ai --display-name "O'Reilly Python AI"
playwright install
Write-Output "‚úÖ Setup complete! Run: jupyter lab"
```

---

## üöÄ Start Working
After setup, always run:

**Mac/Linux:**
```bash
cd oreilly-python-course
source .venv/bin/activate
jupyter lab
```

**Windows:**
```powershell
cd oreilly-python-course
.venv\Scripts\activate
jupyter lab
```

---

## üîß Troubleshooting

### Mac Issues:
- If Homebrew install fails: Try running `xcode-select --install` first
- If permission denied: Run `chmod +w ~/.bash_profile ~/.zshrc`
- If uv not found: Restart terminal or run `source ~/.zshrc`

### Windows Issues:
- If PowerShell blocked: Run as Administrator and set execution policy
- If Git not found: Restart PowerShell after Git installation
- If uv not found: Close and reopen PowerShell

### Common Issues:
- If packages fail to install: Make sure virtual environment is activated
- If Jupyter won't start: Run `jupyter lab --generate-config` then try again
- If kernel not found in Jupyter: Re-run the `python -m ipykernel install` command

---

## üìö What You'll Have After Setup

‚úÖ **Python Environment**: Isolated virtual environment with UV  
‚úÖ **Jupyter Lab**: Interactive notebook environment  
‚úÖ **AI Libraries**: OpenAI, Anthropic, Ollama for AI integration  
‚úÖ **Data Science**: Pandas, Matplotlib for data analysis  
‚úÖ **Web Automation**: Playwright, BeautifulSoup for web scraping  
‚úÖ **Course Materials**: All notebooks organized and ready to use

---

## üéØ Next Steps

1. **Open Jupyter Lab**: Run `jupyter lab` in your activated environment
2. **Navigate to**: `notebooks/01-python-fundamentals/` to start learning
3. **Follow the progression**: Fundamentals ‚Üí AI APIs ‚Üí Automation Projects ‚Üí Exercises
4. **Practice**: Work through exercises to solidify your learning

Happy coding! üöÄ

</details>

---

## presentation/presentation_update.md

<details>
<summary>View presentation_update.md</summary>

Slide - # Automation as a General Skill

- I need a visual suggestion for each of the 4 examples of categories for which automation is useful as either an image prompt or go fetch it from
some free API and download it here and place it.




</details>

---

## scripts/06-browser-automation.md

<details>
<summary>View 06-browser-automation.md</summary>

# Introduction to Browser Automation with Playwright in Python

This lesson covers automating web browser interactions using the Playwright library in Python, drawing from Python programming fundamentals.

## Basic Setup and Imports

**<span style="color: red">DISCLAIMER: this notebook will only run locally (not on Google Colab)</span>**

%pip install playwright
%playwright install

## Understanding Browser Automation Flow


Let's break down the script: `./add_movie_to_watchlist.py`:

```python
import re
from playwright.sync_api import Playwright, sync_playwright, expect
import os
import sys

def run(playwright: Playwright) -> None:
    movie_to_search = sys.argv[1] 
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://letterboxd.com/")
    page.get_by_label("Do not consent").click()
    page.get_by_role("link", name="Sign in").click()
    page.get_by_label("Username").fill(os.environ["LETTERBOXD_USER"])
    page.get_by_label("Username").press("Tab")
    page.get_by_label("Password").fill(os.environ["LETTERBOXD_PWD"])
    page.get_by_role("button", name="Sign in").click()
    page.locator(".navitem > .replace").click()
    page.get_by_label("Search:").fill(movie_to_search)
    page.get_by_role("button", name="Search").click()
    page.get_by_role("link", name=movie_to_search, exact=True).first.click()
    page.get_by_role("link", name="Add this film to your").click()

    # ---------------------
    context.close()
    browser.close()


with sync_playwright() as playwright:
    run(playwright)
```

# Browser Automation with Playwright - A Tutorial

## Overview
This script uses Playwright to automate the process of adding movies to a Letterboxd watchlist.

## Script Structure

### 1. Imports and Setup
```python
import re
from playwright.sync_api import Playwright, sync_playwright, expect
import os
import sys
```
Key imports for browser automation, environment variables, and system arguments.

### 2. Main Function
```python
def run(playwright: Playwright) -> None:
```
The main function that handles browser automation, taking a Playwright instance as parameter.

### 3. Browser Setup
```python
browser = playwright.chromium.launch(headless=False)
context = browser.new_context()
page = context.new_page()
```
- Launches Chromium browser in visible mode
- Creates a new browser context
- Opens a new page

### 4. Authentication Flow
```python
page.get_by_label("Username").fill(os.environ["LETTERBOXD_USER"])
page.get_by_label("Password").fill(os.environ["LETTERBOXD_PWD"])
page.get_by_role("button", name="Sign in").click()
```
Handles login using environment variables for credentials.

### 5. Movie Search and Addition
```python
page.get_by_label("Search:").fill(movie_to_search)
page.get_by_role("button", name="Search").click()
page.get_by_role("link", name=movie_to_search, exact=True).first.click()
page.get_by_role("link", name="Add this film to your").click()
```
Searches for and adds the specified movie to watchlist.

### 6. Cleanup
```python
context.close()
browser.close()
```
Properly closes browser context and instance.

### 7. Script Execution
```python
with sync_playwright() as playwright:
    run(playwright)
```
Runs the automation script using Playwright's context manager.

## Usage
Run the script with a movie title as argument:
```bash
python add_movie_to_watchlist.py "Movie Title"
```

## Environment Variables
Required environment variables:
- `LETTERBOXD_USER`: Letterboxd username
- `LETTERBOXD_PWD`: Letterboxd password

This code demonstrates core Playwright concepts:
- Browser and context management
- Page navigation and interaction
- Form filling and button clicking
- Error handling and retries
- Environment variable usage
- Command line arguments
- Type hints and documentation

The example uses IMDb instead of Letterboxd but follows similar patterns while introducing additional programming concepts.

![](2025-02-04-14-30-46.png)

But what is cool about the script is that I didn't really write it! I obtained through a mix of prompting ChatGPT and playwright `codegen` tool that allows us to interact with a browser and get the necessary code to reproduce such action! Example of a terminal command you can run:

`playwright codegen https://letterboxd.com/`

To save credentials to avoid having to redo login steps see example scripts at:
- `./letterboxd_saving_auth_browser.py`
- `./letterboxd_using_auth.py`


</details>

---

## scripts/ai_stuff.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = ["openai", "dotenv"]
# ///

from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

api_key_openai = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=api_key_openai)

response = client.chat.completions.create(
    model="gpt-5-mini",
    messages=[
        {"role": "user", "content": "Explain best practices for beginners on running python scripts safely with uv."}
    ]
)

print(response.choices[0].message.content)
```

---

## scripts/ai_tools.py

```python
# PACKAGES 
from openai import OpenAI
import anthropic
import ollama
import json


# CODE (SCRIPT!)
def ask_ai(prompt, model_name="gpt-5-mini"):
    """
    Send prompt to an LLM and get output text back.    
    """
    if "claude" in model_name:
        client = anthropic.Anthropic()

        # Send a message to the Claude AI
        response = client.messages.create(
            model=model_name,
            messages=[
                {
                    "role": "user", "content": prompt
                }
                ],
            max_tokens=4000,
        )
        output = response.content[0].text
        return output        
    else:
        client = OpenAI()
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
        )
        output = response.choices[0].message.content
        return output


def ask_local_ai(prompt, model_name="llama3.2", structured=False):
    """
    Send prompt to a local LLM and get output text back.
    """
    client = ollama.Client()
    if structured:
        response = client.chat(model=model_name, 
                               messages=
                               [
                                   {"role": "user", "content": prompt}
                                ],
                               format='json')
        return response.message.content
    else:
        response = client.chat(model=model_name, 
                               messages=
                               [
                                   {"role": "user", "content": prompt}
                                ])
        return response.message.content


def parse_dates_list(output_str):
    """
    Parse the string output containing a Python list of dates into an actual Python list.
    
    Args:
        output_str (str): String containing a Python list representation of dates
        
    Returns:
        list: List of date strings
    """
    # Remove markdown code block formatting if present
    output_str = output_str.replace('```python', '').replace('```', '').strip()
    
    # Safely evaluate the string as a Python expression
    dates_list = eval(output_str)
    
    return dates_list

def parse_json_output(json_str):
    """
    This function parses the JSON output from the AI and removes the markdown code block markers if present.
    """
    # Remove markdown code block markers if present
    json_str = json_str.replace('```json', '').replace('```', '').strip()
    
    # Parse the JSON string into a Python dictionary
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        print("Error: Could not parse JSON string")
        return None
```

---

## scripts/browser_automation_add_movie_to_watchlist_letterboxd.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "playwright",
# ]
# ///

import re
from playwright.sync_api import Playwright, sync_playwright, expect
import os
import sys

def run(playwright: Playwright) -> None:
    movie_to_search = sys.argv[1]
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://letterboxd.com/")
    page.get_by_label("Do not consent").click()
    page.get_by_role("link", name="Sign in").click()
    page.get_by_label("Username").fill(os.environ["LETTERBOXD_USER"])
    page.get_by_label("Username").press("Tab")
    page.get_by_label("Password").fill(os.environ["LETTERBOXD_PWD"])
    page.get_by_role("button", name="Sign in").click()
    page.locator(".navitem > .replace").click()
    page.get_by_label("Search:").fill(movie_to_search)
    page.get_by_role("button", name="Search").click()
    expect(page).to_have_url(re.compile(r"/search/"), timeout=10000)
    page.wait_for_selector('a[href^="/film/"]', state="visible", timeout=10000)
    page.locator('a[href^="/film/"]').first.click()
    page.get_by_role("link", name="Add this film to your").click()

    # ---------------------
    context.close()
    browser.close()


with sync_playwright() as playwright:
    run(playwright)
```

---

## scripts/chat_with_pdfs_extract.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = [
#   "openai>=1.99.2",
#   "prompt_toolkit>=3.0.52",
#   "pdfplumber>=0.11.0",
#   "pandas>=2.2.0",
#   "rich>=13.7.0",
#   "tabulate>=0.9.0",
# ]
# ///
"""
chat_with_pdfs_extract.py

A standalone terminal chat app that:

1) Loads a PDF into memory (raw text + detected tables).
2) Asks the user what to extract.
3) Confirms the extraction request.
4) Uses GPT-5 via the OpenAI Responses API to produce a CSV.
5) Previews the CSV and saves it locally on approval.

Run:
  uv run chat_with_pdfs_extract.py /path/to/file.pdf

Requires:
  export OPENAI_API_KEY="sk-..."
"""
from __future__ import annotations

import argparse
import os
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple, Any

import pandas as pd
import pdfplumber
from openai import OpenAI
from prompt_toolkit import PromptSession
from prompt_toolkit.formatted_text import HTML
from prompt_toolkit.history import InMemoryHistory
from prompt_toolkit.styles import Style
from rich.console import Console
from rich.panel import Panel

console = Console()


# ---------------------------
# PDF loading / preprocessing
# ---------------------------

@dataclass
class PDFContext:
    path: Path
    raw_text: str
    tables: List[pd.DataFrame]
    tables_markdown: str


def load_pdf(path: Path, max_chars: int = 120_000) -> PDFContext:
    """
    Load a PDF and return:
      - raw_text: concatenated text per page (possibly truncated)
      - tables: list of DataFrames extracted by pdfplumber
      - tables_markdown: markdown previews of those tables

    Table extraction is best-effort: some PDFs won't yield tables.
    """
    if not path.exists():
        raise FileNotFoundError(f"PDF not found: {path}")

    raw_parts: List[str] = []
    tables: List[pd.DataFrame] = []

    with pdfplumber.open(str(path)) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            if text.strip():
                raw_parts.append(f"\n\n--- Page {i+1} ---\n{text}")

            # Best-effort table extraction per page.
            try:
                page_tables = page.extract_tables() or []
                for t in page_tables:
                    if not t or len(t) < 2:
                        continue
                    header = t[0]
                    body = t[1:]
                    df = pd.DataFrame(body, columns=header).dropna(axis=1, how="all")
                    if df.shape[1] > 0:
                        tables.append(df)
            except Exception:
                continue

    raw_text = "".join(raw_parts).strip()

    if len(raw_text) > max_chars:
        head = raw_text[: max_chars // 2]
        tail = raw_text[-max_chars // 2 :]
        raw_text = head + "\n\n[TRUNCATED ...]\n\n" + tail

    tables_md = tables_to_markdown(tables)

    return PDFContext(path=path, raw_text=raw_text, tables=tables, tables_markdown=tables_md)


def tables_to_markdown(tables: List[pd.DataFrame], max_rows: int = 15) -> str:
    """
    Convert extracted tables to markdown previews.

    Uses DataFrame.to_markdown if tabulate is available.
    Falls back to a simple pipe-table renderer if not.
    """
    md_parts = []
    for idx, df in enumerate(tables, start=1):
        preview = df.head(max_rows)

        try:
            md_table = preview.to_markdown(index=False)
        except ImportError:
            # Fallback renderer without tabulate.
            cols = [str(c) for c in preview.columns]
            rows = preview.fillna("").astype(str).values.tolist()

            header = "| " + " | ".join(cols) + " |"
            sep = "| " + " | ".join(["---"] * len(cols)) + " |"
            body = "\n".join("| " + " | ".join(r) + " |" for r in rows)

            md_table = "\n".join([header, sep, body])

        md_parts.append(f"\n\n### Table {idx}\n{md_table}")
    return "".join(md_parts).strip()


# ---------------------------
# OpenAI helpers
# ---------------------------

def get_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise EnvironmentError("OPENAI_API_KEY is not set. Please export it before running.")
    return OpenAI(api_key=api_key)


def response_text(resp: Any) -> str:
    """Extract concatenated text from a Responses API response."""
    out: List[str] = []
    for item in getattr(resp, "output", []):
        content = getattr(item, "content", None)
        if content:
            for c in content:
                txt = getattr(c, "text", None)
                if txt:
                    out.append(txt)
    return "".join(out).strip()


def ask_model(
    client: OpenAI,
    model: str,
    messages: List[dict],
    previous_response_id: Optional[str] = None,
    reasoning_effort: str = "minimal",
    text_format: str = "text",
) -> Tuple[str, str]:
    """
    Call the Responses API with a list of message items.
    Returns (text, response_id).
    """
    kwargs: dict = {
        "model": model,
        "input": messages,
        "text": {"format": {"type": text_format}},
        "reasoning": {"effort": reasoning_effort},
    }
    if previous_response_id:
        kwargs["previous_response_id"] = previous_response_id

    resp = client.responses.create(**kwargs)
    return response_text(resp), resp.id


# ---------------------------
# Chat UI + workflow
# ---------------------------

STYLE = Style.from_dict({"user": "#00aa00 bold", "assistant": "#0088ff bold"})

YES_RE = re.compile(r"^(y|yes|ok|okay|sure|go ahead|approve)\b", re.I)
NO_RE = re.compile(r"^(n|no|nope|cancel|stop)\b", re.I)


def print_assistant(msg: str) -> None:
    console.print(Panel(msg, title="Assistant", border_style="blue"))


def print_user(msg: str) -> None:
    console.print(Panel(msg, title="You", border_style="green"))


def preview_csv(csv_text: str, max_rows: int = 20) -> pd.DataFrame:
    """
    Parse CSV text into a DataFrame and print a short preview.
    Raises if the CSV is invalid.
    """
    from io import StringIO

    df = pd.read_csv(StringIO(csv_text.strip()))
    console.print("\n[bold]Preview (first rows):[/bold]")
    console.print(df.head(max_rows))
    return df


def main() -> None:
    parser = argparse.ArgumentParser(description="Chat with GPT-5 to extract CSV from a PDF.")
    parser.add_argument("pdf_path", type=str, help="Path to the PDF file.")
    parser.add_argument("--model", default="gpt-5", help="OpenAI model to use (default: gpt-5).")
    parser.add_argument("--max-chars", type=int, default=120_000, help="Max PDF chars to send to model.")
    args = parser.parse_args()

    pdf_path = Path(args.pdf_path).expanduser().resolve()
    console.print(f"[bold]Loading PDF:[/bold] {pdf_path}")
    ctx = load_pdf(pdf_path, max_chars=args.max_chars)
    console.print(f"[bold]Loaded PDF.[/bold] Extracted {len(ctx.tables)} tables.")

    client = get_client()
    session = PromptSession(history=InMemoryHistory(), style=STYLE)

    # Step 1: Ask user what to extract.
    print_assistant(
        "PDF loaded! What would you like to extract?\n"
        "Describe the table(s) or fields you want, and any filters.\n"
        "Example: Extract all invoice line items with columns Date, Vendor, Amount."
    )
    user_request = session.prompt(HTML("<user>You > </user> "))
    print_user(user_request)

    # Step 2: Model confirms requirement.
    confirm_messages = [
        {
            "role": "developer",
            "content": (
                "You are a careful data-extraction assistant. "
                "Restate the user's extraction request clearly, "
                "ask any missing questions, and end by asking for confirmation. "
                "Keep it short."
            ),
        },
        {"role": "user", "content": user_request},
    ]
    confirm_text, _ = ask_model(client, args.model, confirm_messages)
    print_assistant(confirm_text)

    approval = session.prompt(HTML("<user>Approve? (y/n) > </user> "))
    print_user(approval)
    if NO_RE.match(approval):
        print_assistant("Okay ‚Äî cancelled. Re-run the script to try a different request.")
        return

    # Step 3: Extraction.
    extraction_messages = [
        {
            "role": "developer",
            "content": (
                "You extract structured data from PDFs. "
                "You will be given raw text and markdown previews of detected tables. "
                "Return ONLY a CSV (comma-separated) with a header row. "
                "Do not include commentary or code fences. "
                "If you are unsure, still produce your best CSV with empty cells where needed."
            ),
        },
        {
            "role": "user",
            "content": (
                f"The user wants:\n{user_request}\n\n"
                f"PDF raw text:\n{ctx.raw_text}\n\n"
                f"Detected tables (markdown previews):\n{ctx.tables_markdown or '[none detected]'}"
            ),
        },
    ]
    csv_text, _ = ask_model(client, args.model, extraction_messages, reasoning_effort="minimal")

    print_assistant("Here is the extracted CSV. I'll preview it below:")
    try:
        df = preview_csv(csv_text)
    except Exception as e:
        print_assistant(
            "I couldn't parse the model output as CSV.\n"
            f"Error: {e}\n\nRaw output:\n{csv_text}"
        )
        return

    # Step 4: Save on approval.
    save_ans = session.prompt(HTML("<user>Save this as CSV? (y/n) > </user> "))
    print_user(save_ans)
    if not YES_RE.match(save_ans):
        print_assistant("No problem ‚Äî not saved.")
        return

    out_name = f"{pdf_path.stem}_extracted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    out_path = pdf_path.parent / out_name
    df.to_csv(out_path, index=False)
    print_assistant(f"Saved CSV to: {out_path}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nBye!")
```

---

## scripts/dashboard_guidelines_example.py

```python
#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "pandas",
#   "plotly",
#   "dash",
# ]
# ///
"""
Automata Learning Lab - Minimal Dashboard (Final Fixed Version)
"""

import argparse
import pandas as pd
from dash import Dash, dcc, html
import plotly.express as px


# ------------------------------------------------------------
# SAFE DATE PARSER (no errors='ignore', no warnings)
# ------------------------------------------------------------
def safe_parse_dates(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:
        if "date" in col.lower():
            try:
                df[col] = pd.to_datetime(df[col])
            except Exception:
                pass
    return df


# ------------------------------------------------------------
# LOAD CSV
# ------------------------------------------------------------
def load_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df = safe_parse_dates(df)
    return df


# ------------------------------------------------------------
# PLOT STYLE
# ------------------------------------------------------------
def style(fig):
    fig.update_layout(
        template="simple_white",
        paper_bgcolor="#FFFFFF",
        plot_bgcolor="#FFFFFF",
        font=dict(family="Helvetica", color="#000000"),
        margin=dict(l=40, r=20, t=50, b=40),
    )
    return fig


# ------------------------------------------------------------
# BUILD DASH APP
# ------------------------------------------------------------
def build_app(df: pd.DataFrame) -> Dash:
    numeric_cols = df.select_dtypes(include="number").columns.tolist()

    cat_col = next((c for c in df.columns if df[c].dtype == "object"), None)
    date_col = next((c for c in df.columns if "date" in c.lower()), None)

    figures = []

    # 1. Time series
    if date_col and numeric_cols:
        fig_ts = px.line(
            df.sort_values(date_col),
            x=date_col,
            y=numeric_cols[0],
            title=f"{numeric_cols[0]} Over Time",
        )
        figures.append(("Time Series", style(fig_ts)))

    # 2. Distribution
    if numeric_cols:
        fig_hist = px.histogram(
            df, x=numeric_cols[0], title=f"Distribution of {numeric_cols[0]}"
        )
        figures.append(("Distribution", style(fig_hist)))

    # 3. Category Breakdown
    if cat_col and numeric_cols:
        grouped = (
            df.groupby(cat_col)[numeric_cols[0]]
            .mean()
            .sort_values(ascending=False)
            .reset_index()
        )
        fig_bar = px.bar(
            grouped, x=cat_col, y=numeric_cols[0], title=f"{numeric_cols[0]} by {cat_col}"
        )
        figures.append(("Category Breakdown", style(fig_bar)))

    # ---- Layout ----
    app = Dash(__name__)

    app.layout = html.Div(
        style={
            "backgroundColor": "#F5F5F5",
            "minHeight": "100vh",
            "fontFamily": "Helvetica",
            "padding": "20px",
        },
        children=[
            html.H1(
                "Automata Learning Lab ‚Äî Dashboard",
                style={"borderBottom": "2px solid black", "paddingBottom": "6px"},
            ),
            html.Div(
                style={
                    "display": "grid",
                    "gridTemplateColumns": "1fr 1fr",
                    "gap": "20px",
                    "alignItems": "start",  # FIX
                },
                children=[
                    html.Div(
                        style={
                            "background": "white",
                            "padding": "16px",
                            "border": "2px solid black",
                            "borderRadius": "4px",
                            "height": "420px",        # KEY FIX
                            "overflow": "hidden",      # KEY FIX
                            "display": "flex",
                            "flexDirection": "column",
                        },
                        children=[
                            html.H3(title, style={"marginBottom": "10px"}),
                            dcc.Graph(
                                figure=fig,
                                style={
                                    "flex": "1",
                                    "height": "320px",  # FIXED GRAPH HEIGHT
                                },
                            ),
                        ],
                    )
                    for title, fig in figures
                ],
            ),
        ],
    )

    return app


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("csv_path", help="Path to CSV file")
    args = parser.parse_args()

    df = load_data(args.csv_path)
    app = build_app(df)

    app.run(debug=False)


if __name__ == "__main__":
    main()
```

---

## scripts/data_analysis_stock.py

```python
#!/usr/bin/env -S uv run
# /// script
# dependencies = [
#   "pandas>=2.0.0",
#   "plotly>=5.20.0",
# ]
# ///
"""
Generate an interactive stock trading dashboard from a CSV file.

Usage:
    python stock_dashboard.py stock-trading-data.csv
    # or, with uv:
    uv run stock_dashboard.py stock-trading-data.csv

Optional:
    python stock_dashboard.py stock-trading-data.csv -o my_dashboard.html
"""

import argparse
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio


def load_data(csv_path: Path) -> pd.DataFrame:
    df = pd.read_csv(csv_path)

    # Ensure expected columns exist (simple sanity checks)
    required_cols = [
        "Date",
        "Stock Symbol",
        "Open Price",
        "Close Price",
        "Volume",
        "Moving Average (50-day)",
        "RSI (Relative Strength Index)",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing expected columns in CSV: {missing}")

    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df = df.sort_values(["Stock Symbol", "Date"])
    return df


def build_figures(df: pd.DataFrame):
    symbols = df["Stock Symbol"].unique().tolist()
    first_symbol = symbols[0]

    # 1) Close price over time
    fig_price = px.line(
        df,
        x="Date",
        y="Close Price",
        color="Stock Symbol",
        title="Close Price Over Time",
        labels={
            "Date": "Date",
            "Close Price": "Close Price",
            "Stock Symbol": "Symbol",
        },
    )
    fig_price.update_layout(
        hovermode="x unified",
        legend_title_text="Symbol",
    )

    # 2) Volume over time
    fig_volume = px.line(
    df,
    x="Date",
    y=df["Volume"].rolling(5).mean(),  
    color="Stock Symbol",
    title="Trading Volume (5-Day Rolling Avg)",
    labels={"Date": "Date", "y": "Volume (5-day avg)", "Stock Symbol": "Symbol"},
)
    fig_volume.update_traces(mode="lines")
    fig_volume.update_layout(
        hovermode="x unified",
        legend_title_text="Symbol",
    )

    # 3) Moving average vs close price for first symbol
    df_first = df[df["Stock Symbol"] == first_symbol]

    fig_ma = go.Figure()
    fig_ma.add_trace(
        go.Scatter(
            x=df_first["Date"],
            y=df_first["Close Price"],
            mode="lines",
            name=f"{first_symbol} Close Price",
        )
    )
    fig_ma.add_trace(
        go.Scatter(
            x=df_first["Date"],
            y=df_first["Moving Average (50-day)"],
            mode="lines",
            name=f"{first_symbol} 50-day Moving Avg",
        )
    )
    fig_ma.update_layout(
        title=f"{first_symbol}: Close Price vs 50-day Moving Average",
        xaxis_title="Date",
        yaxis_title="Price",
        hovermode="x unified",
        legend_title_text="Series",
    )

    # 4) RSI over time
    fig_rsi = px.line(
        df,
        x="Date",
        y="RSI (Relative Strength Index)",
        color="Stock Symbol",
        title="RSI Over Time",
        labels={
            "Date": "Date",
            "RSI (Relative Strength Index)": "RSI",
            "Stock Symbol": "Symbol",
        },
    )
    fig_rsi.update_layout(
        hovermode="x unified",
        legend_title_text="Symbol",
        yaxis=dict(range=[0, 100]),
    )

    return fig_price, fig_volume, fig_ma, fig_rsi, first_symbol


def build_dashboard_html(
    df: pd.DataFrame,
    title: str = "Stock Trading Dashboard",
) -> str:
    fig_price, fig_volume, fig_ma, fig_rsi, first_symbol = build_figures(df)

    config = {
        "displaylogo": False,
        "modeBarButtonsToRemove": [
            "select2d",
            "lasso2d",
            "autoScale2d",
        ],
    }

    price_html = pio.to_html(
        fig_price, include_plotlyjs=False, full_html=False, config=config
    )
    volume_html = pio.to_html(
        fig_volume, include_plotlyjs=False, full_html=False, config=config
    )
    ma_html = pio.to_html(
        fig_ma, include_plotlyjs=False, full_html=False, config=config
    )
    rsi_html = pio.to_html(
        fig_rsi, include_plotlyjs=False, full_html=False, config=config
    )

    # Minimal sleek layout with CSS
    html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>{title}</title>
  <script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
  <style>
    :root {{
      --bg-color: #050816;
      --card-bg: #0b1020;
      --card-border: #1f2937;
      --accent: #38bdf8;
      --accent-soft: rgba(56, 189, 248, 0.15);
      --text-main: #e5e7eb;
      --text-muted: #9ca3af;
    }}

    * {{
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }}

    body {{
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: radial-gradient(circle at top left, #0f172a 0, #020617 45%, #000 100%);
      color: var(--text-main);
      min-height: 100vh;
      padding: 32px 24px 40px;
    }}

    .container {{
      max-width: 1280px;
      margin: 0 auto;
    }}

    .header {{
      display: flex;
      flex-direction: column;
      gap: 12px;
      margin-bottom: 28px;
    }}

    .title {{
      font-size: 1.9rem;
      font-weight: 700;
      letter-spacing: 0.03em;
      display: flex;
      align-items: center;
      gap: 10px;
    }}

    .title-pill {{
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      padding: 3px 9px;
      border-radius: 9999px;
      border: 1px solid var(--accent-soft);
      background: linear-gradient(135deg, rgba(56,189,248,0.12), rgba(8,47,73,0.3));
      color: var(--accent);
    }}

    .subtitle {{
      font-size: 0.95rem;
      color: var(--text-muted);
      max-width: 620px;
    }}

    .summary-grid {{
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
      gap: 10px;
      margin-top: 10px;
    }}

    .pill {{
      padding: 6px 10px;
      font-size: 0.8rem;
      border-radius: 9999px;
      border: 1px solid rgba(148,163,184,0.3);
      color: var(--text-muted);
      display: inline-flex;
      align-items: center;
      gap: 8px;
      white-space: nowrap;
    }}

    .pill-dot {{
      width: 6px;
      height: 6px;
      border-radius: 9999px;
      background-color: var(--accent);
    }}

    .grid {{
      display: grid;
      grid-template-columns: minmax(0, 1.4fr) minmax(0, 1fr);
      gap: 18px;
    }}

    .grid-bottom {{
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
      gap: 18px;
      margin-top: 18px;
    }}

    .card {{
      background: radial-gradient(circle at top left, rgba(15,23,42,0.85), rgba(15,23,42,0.5));
      border-radius: 18px;
      border: 1px solid var(--card-border);
      padding: 14px 12px 10px;
      box-shadow:
        0 18px 40px rgba(0,0,0,0.55),
        0 0 0 1px rgba(15,23,42,0.8);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      position: relative;
      overflow: hidden;
    }}

    .card::before {{
      content: "";
      position: absolute;
      inset: -20%;
      background:
        radial-gradient(circle at top, rgba(56,189,248,0.18), transparent 55%),
        radial-gradient(circle at bottom right, rgba(59,130,246,0.12), transparent 55%);
      opacity: 0.45;
      pointer-events: none;
    }}

    .card-inner {{
      position: relative;
      z-index: 1;
    }}

    .card-header {{
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      margin-bottom: 4px;
      gap: 8px;
    }}

    .card-title {{
      font-size: 0.88rem;
      font-weight: 600;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      color: #e5e7eb;
    }}

    .card-subtitle {{
      font-size: 0.75rem;
      color: var(--text-muted);
    }}

    .badge {{
      font-size: 0.7rem;
      padding: 2px 7px;
      border-radius: 9999px;
      border: 1px solid rgba(148,163,184,0.35);
      color: var(--text-muted);
    }}

    .chart {{
      margin-top: 6px;
      border-radius: 12px;
      background: rgba(15,23,42,0.9);
      padding: 2px;
    }}

    .hint {{
      font-size: 0.76rem;
      color: var(--text-muted);
      margin-top: 12px;
    }}

    @media (max-width: 900px) {{
      body {{
        padding: 20px 14px 32px;
      }}
      .grid {{
        grid-template-columns: minmax(0, 1fr);
      }}
    }}
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <div class="title">
        <span>{title}</span>
        <span class="title-pill">Interactive</span>
      </div>
      <p class="subtitle">
        Explore price action, trading volume, moving averages and RSI for your stock universe.
        Use the legend to toggle symbols and drag on any chart to zoom into specific periods.
      </p>
      <div class="summary-grid">
        <div class="pill">
          <span class="pill-dot"></span>
          Multi-symbol price &amp; volume
        </div>
        <div class="pill">
          <span class="pill-dot"></span>
          Moving average vs. price ({first_symbol})
        </div>
        <div class="pill">
          <span class="pill-dot"></span>
          RSI for momentum insights
        </div>
        <div class="pill">
          <span class="pill-dot"></span>
          Hover for precise values
        </div>
      </div>
    </header>

    <main>
      <section class="grid">
        <article class="card">
          <div class="card-inner">
            <div class="card-header">
              <div>
                <div class="card-title">Close Price</div>
                <div class="card-subtitle">All symbols ¬∑ time-series</div>
              </div>
              <span class="badge">Legend: click to isolate</span>
            </div>
            <div class="chart">
              {price_html}
            </div>
          </div>
        </article>

        <article class="card">
          <div class="card-inner">
            <div class="card-header">
              <div>
                <div class="card-title">Trading Volume</div>
                <div class="card-subtitle">Compare activity across symbols</div>
              </div>
              <span class="badge">Bars ¬∑ grouped</span>
            </div>
            <div class="chart">
              {volume_html}
            </div>
          </div>
        </article>
      </section>

      <section class="grid-bottom">
        <article class="card">
          <div class="card-inner">
            <div class="card-header">
              <div>
                <div class="card-title">Price vs 50-day Moving Avg</div>
                <div class="card-subtitle">{first_symbol} ¬∑ trend vs. smoothing</div>
              </div>
              <span class="badge">Mean reversion view</span>
            </div>
            <div class="chart">
              {ma_html}
            </div>
          </div>
        </article>

        <article class="card">
          <div class="card-inner">
            <div class="card-header">
              <div>
                <div class="card-title">RSI (Relative Strength Index)</div>
                <div class="card-subtitle">Momentum across symbols</div>
              </div>
              <span class="badge">0‚Äì100 band</span>
            </div>
            <div class="chart">
              {rsi_html}
            </div>
            <p class="hint">
              Tip: Watch for RSI extremes (e.g. &gt;70 or &lt;30) alongside price and volume
              for potential overbought/oversold signals.
            </p>
          </div>
        </article>
      </section>
    </main>
  </div>
</body>
</html>
"""
    return html


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate an interactive stock trading dashboard from a CSV file."
    )
    parser.add_argument(
        "input_csv",
        type=str,
        help="Path to the stock trading data CSV file.",
    )
    parser.add_argument(
        "-o",
        "--output-html",
        type=str,
        default="stock_dashboard.html",
        help="Path to the output HTML file (default: stock_dashboard.html).",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    input_path = Path(args.input_csv)
    output_path = Path(args.output_html)

    if not input_path.exists():
        raise SystemExit(f"Input CSV not found: {input_path}")

    df = load_data(input_path)
    html = build_dashboard_html(df)

    output_path.write_text(html, encoding="utf-8")
    print(f"Dashboard written to: {output_path.resolve()}")


if __name__ == "__main__":
    main()
```

---

## scripts/data_extraction_invoices.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "openai>=1.30.1",
#   "pydantic>=2.7.1",
# ]
# ///

from pathlib import Path
from openai import OpenAI
from pydantic import BaseModel
import sys

# --- OpenAI setup
client = OpenAI()

# --- Pydantic model
class InvoiceData(BaseModel):
    name: str
    date: str
    tax_id: str

# --- Change this path or pass via command line
FOLDER = Path("/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course/notebooks/assets/fake-invoices")

if len(sys.argv) > 1:
    FOLDER = Path(sys.argv[1])

def extract_from_file(file_path: Path):
    content = file_path.read_text(encoding="utf-8")
    try:
        response = client.beta.chat.completions.parse(
            model="gpt-5-mini",
            messages=[
                {"role": "system", "content": "Extract the invoice name, date, and tax ID."},
                {"role": "user", "content": content},
            ],
            response_format=InvoiceData,
        )
        return response.choices[0].message.parsed
    except Exception as e:
        print(f"‚ùå {file_path.name}: {e}")
        return None

def main():
    if not FOLDER.exists():
        print(f"‚ö†Ô∏è Folder not found: {FOLDER}")
        return

    txt_files = list(FOLDER.glob("*.txt"))
    if not txt_files:
        print("‚ö†Ô∏è No .txt files found in the folder.")
        return

    print(f"üìÇ Processing {len(txt_files)} files in {FOLDER}...\n")

    for file in txt_files:
        data = extract_from_file(file)
        if data:
            print(f"‚úÖ {file.name}: {data}\n")

if __name__ == "__main__":
    main()
```

---

## scripts/data_extraction_receipt_data.py

```python
# /// script
# requires-python=">=3.12"
# dependencies=["openai","pydantic"]
# ///

from pydantic import BaseModel, Field
from openai import OpenAI
import sys

def load_receipt_data(file_path: str):
    with open(file_path, "r") as f:
        receipt_data = f.read()
    
    return receipt_data

class ReceiptData(BaseModel):
    company_name: str = Field(description="The name of the company.")
    date_of_closure: str = Field(description="The date of closure.")
    amount_paid: float = Field(description="The amount paid ('Total Pago') after taxes.")

client = OpenAI()

file_path = sys.argv[1]
receipt_data = load_receipt_data(file_path)
completion = client.beta.chat.completions.parse(
    model="gpt-5-mini",
    messages=[
        {"role": "system", "content": """
         You are an extraction engine for receipt data.
         Users will upload the contents of their receipts and you will extract
         the following fields:
         - Company name
         - Date of closure
         - Amount paid
         """},
        {"role": "user", "content": receipt_data},
    ],
    response_format=ReceiptData,
)

message = completion.choices[0].message
if message.parsed:
    print(message.parsed)
else:
    print(message.refusal)
```

---

## scripts/data_transformation_md_to_jupyter.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = ["nbformat"]
# ///

import sys
import os
import nbformat
from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell

def markdown_to_notebook(md_path: str, ipynb_path: str = None):
    if not os.path.isfile(md_path):
        raise FileNotFoundError(f"Markdown file not found: {md_path}")
    
    with open(md_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    nb_cells = []
    in_code_block = False
    code_block_lang = ""
    code_lines = []
    markdown_lines = []

    def flush_markdown():
        nonlocal markdown_lines
        if markdown_lines:
            nb_cells.append(new_markdown_cell("".join(markdown_lines)))
            markdown_lines = []

    def flush_code():
        nonlocal code_lines
        if code_lines:
            nb_cells.append(new_code_cell("".join(code_lines)))
            code_lines = []

    for line in lines:
        if line.strip().startswith("```"):
            if not in_code_block:
                in_code_block = True
                code_block_lang = line.strip()[3:]
                flush_markdown()
            else:
                in_code_block = False
                flush_code()
        elif in_code_block:
            code_lines.append(line)
        else:
            markdown_lines.append(line)

    flush_markdown()

    nb = new_notebook(cells=nb_cells)
    output_path = ipynb_path or md_path.rsplit(".", 1)[0] + ".ipynb"
    with open(output_path, "w", encoding="utf-8") as f:
        nbformat.write(nb, f)

    print(f"Notebook written to: {output_path}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: uv run script.py <input_markdown.md> [output_notebook.ipynb]")
        sys.exit(1)
    md_path = sys.argv[1]
    ipynb_path = sys.argv[2] if len(sys.argv) > 2 else None
    markdown_to_notebook(md_path, ipynb_path)

```

---

## scripts/demo_utils.py

```python
from IPython.display import HTML

def display_chat_message(role, content, background_color="#ffffff"):
    html = f"""
    <div style="background-color: {background_color}; padding: 10px; margin: 5px; border-radius: 10px; color: #000000;">
        <strong>{role}:</strong><br>
        {content}
    </div>
    """
    return HTML(html)

def display_comparison(*responses):
    html = "<div style='display: flex; flex-wrap: wrap; gap: 10px;'>"
    for provider, response in responses:
        html += f"""
        <div style='flex: 1; min-width: 300px; background-color: #f8f9fa; padding: 15px; border-radius: 8px;'>
            <h3 style='color: #2c3e50;'>{provider}</h3>
            <p style='color: #34495e;'>{response}</p>
        </div>
        """
    html += "</div>"
    return HTML(html)
```

---

## scripts/download_papers_from_arxiv.py

```python
#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "requests",
#   "pypdf"
# ]
# ///
"""
Standalone script to download abstracts or first pages of ArXiv papers.
Usage:
    uv run download_arxiv.py urls.txt --mode abstract
    uv run download_arxiv.py urls.txt --mode firstpage
"""

import argparse
import os
import re
import requests
from pypdf import PdfReader
from io import BytesIO


# ----------------------------------------
# Extracts the arXiv ID from a URL
# Input: "https://arxiv.org/abs/2310.15511"
# Output: "2310.15511"
# ----------------------------------------
def extract_arxiv_id(url: str) -> str:
    m = re.search(r"arxiv\.org\/abs\/([\w\.\/\-]+)", url.strip())
    return m.group(1) if m else None


# ----------------------------------------
# Fetches title + abstract from ArXiv API
# Input: arxiv_id ("2310.15511")
# Process: calls API, extracts <title> + <summary>
# Output: dict with id, title, abstract
# ----------------------------------------
def fetch_metadata(arxiv_id: str) -> dict:
    api_url = f"http://export.arxiv.org/api/query?id_list={arxiv_id}"
    r = requests.get(api_url, timeout=10)
    if r.status_code != 200:
        raise RuntimeError(f"Failed to fetch metadata for {arxiv_id}")

    # Extract relevant fields from XML
    abstract_match = re.search(r"<summary>(.*?)</summary>", r.text, re.S)
    title_match = re.search(r"<title>(.*?)</title>", r.text, re.S)

    abstract = abstract_match.group(1).strip() if abstract_match else "No abstract found."
    title = title_match.group(1).strip() if title_match else "No title found."

    return {
        "id": arxiv_id,
        "title": title,
        "abstract": abstract
    }


# ----------------------------------------
# Downloads PDF + extracts the first page
# Input: arxiv_id + output path
# Process: fetch PDF, load with pypdf, get page 1 text
# Output: text file containing first-page content
# ----------------------------------------
def download_pdf_first_page(arxiv_id: str, output_path: str):
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    pdf_bytes = requests.get(pdf_url, timeout=15)
    if pdf_bytes.status_code != 200:
        raise RuntimeError(f"Could not download PDF for {arxiv_id}")

    reader = PdfReader(BytesIO(pdf_bytes.content))
    first_page = reader.pages[0]
    text = first_page.extract_text() or ""

    # Save first page text
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)


# ----------------------------------------
# MAIN SCRIPT LOGIC
# Input: filename with ArXiv URLs + mode ("abstract" or "firstpage")
# Process: iterate through URLs ‚Üí extract ID ‚Üí process accordingly
# Output: one text file per paper in outputs/
# ----------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("file", help=".txt file with ArXiv URLs")
    parser.add_argument("--mode", choices=["abstract", "firstpage"], default="abstract")
    
    args = parser.parse_args()

    # Read list of URLs from text file
    with open(args.file, "r", encoding="utf-8") as f:
        urls = f.readlines()

    # Create output directory
    os.makedirs("outputs", exist_ok=True)

    for url in urls:
        arxiv_id = extract_arxiv_id(url)
        if not arxiv_id:
            print(f"Skipping invalid line: {url.strip()}")
            continue

        print(f"Processing {arxiv_id}...")

        # --------- Mode: Save Abstract ---------
        if args.mode == "abstract":
            meta = fetch_metadata(arxiv_id)
            out_path = os.path.join("outputs", f"{arxiv_id}_abstract.txt")

            # Save metadata (title + abstract)
            with open(out_path, "w", encoding="utf-8") as f:
                f.write(f"Title: {meta['title']}\n\n")
                f.write(meta["abstract"])

            print(f"Saved abstract ‚Üí {out_path}")

        # --------- Mode: Save First PDF Page ---------
        elif args.mode == "firstpage":
            out_path = os.path.join("outputs", f"{arxiv_id}_firstpage.txt")
            download_pdf_first_page(arxiv_id, out_path)
            print(f"Saved first page ‚Üí {out_path}")


if __name__ == "__main__":
    main()
```

---

## scripts/extract_pdf_data.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "pdfplumber>=0.11",
#   "pydantic>=2.7",
#   "openai>=1.51",
#   "tqdm>=4.66",
#   "python-slugify>=8.0",
#   "docling>=2.5"  # optional; if unavailable, we fall back to pdfplumber
# ]
# ///
"""
Usage
-----
uv run extract_tables_llm.py input1.pdf [input2.pdf ...] \
  --out out_dir --engine [auto|docling|pdfplumber] --model gpt-4o-mini --per-table

What it does
------------
- Extracts per-page text from each PDF (Docling if available/selected; else pdfplumber).
- Sends text chunks to OpenAI with Pydantic Structured Outputs to detect & normalize tables.
- Writes each detected table as CSV (default), plus a summary JSONL of provenance.

Notes
-----
[Unverified] LLM parsing of tables from plain text can be imperfect (merged cells, wrapped lines).
Prefer for quick prototyping / weakly-structured docs. For production-grade table extraction,
use lattice/stream parsers (Camelot/Tabula) or Docling‚Äôs native table objects directly.
"""

from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from typing import List, Optional, Dict, Any

import pdfplumber
from pydantic import BaseModel, Field, ValidationError
from slugify import slugify
from tqdm import tqdm

# OpenAI (new SDK style)
from openai import OpenAI

# -----------------------------
# Pydantic schemas (Structured Outputs)
# -----------------------------

class TableCell(BaseModel):
    value: str = Field(..., description="Raw cell text without surrounding quotes.")


class TableRow(BaseModel):
    cells: List[TableCell] = Field(..., description="Row cells, keep column order.")


class DetectedTable(BaseModel):
    title: Optional[str] = Field(None, description="Optional short title or caption if present nearby.")
    page: int = Field(..., description="1-based page number where this table appears.")
    columns: List[str] = Field(..., description="Header names; if missing, synthesize short, neutral headers.")
    rows: List[TableRow] = Field(..., description="Body rows; each row length should match columns length.")
    source_span: Optional[str] = Field(
        None, description="Short snippet of the text region used to infer this table."
    )
    confidence: float = Field(
        ..., ge=0.0, le=1.0, description="Heuristic confidence (0-1) that this is a correct table parse."
    )


class TablesBundle(BaseModel):
    tables: List[DetectedTable] = Field(default_factory=list)
    notes: Optional[str] = Field(
        None, description="Any caveats or formatting decisions made during parsing."
    )


# -----------------------------
# Prompt for the model
# -----------------------------
SYSTEM_PROMPT = """You extract tables from noisy plain text.
Rules:
- Only return actual tabular data (records with consistent columns).
- If headers are unclear, infer short neutral headers (e.g., col_1, col_2, ...).
- Keep rows aligned with headers; trim whitespace; do not invent values.
- If a number is split across lines, join it.
- If you cannot reliably reconstruct a table from the text, omit it.
- Provide a confidence score per table (0..1).
"""

USER_PROMPT_TEMPLATE = """Extract all reliable tables from the following text (page {page_num} of a PDF). 
Return nothing if no table can be confidently reconstructed.

--- BEGIN TEXT ---
{page_text}
--- END TEXT ---
"""


# -----------------------------
# Text extraction engines
# -----------------------------
def extract_text_pdfplumber(path: Path) -> List[str]:
    """Return list of page texts (1 string per page)."""
    pages: List[str] = []
    with pdfplumber.open(path) as pdf:
        for p in pdf.pages:
            txt = p.extract_text() or ""
            pages.append(txt)
    return pages


def extract_text_docling(path: Path) -> List[str]:
    """
    [Unverified] Minimal Docling text extraction.
    If docling is not installed or fails, raise ImportError to let caller fallback.
    """
    try:
        from docling.document_converter import DocumentConverter  # type: ignore
    except Exception as e:  # pragma: no cover
        raise ImportError("docling not available") from e

    converter = DocumentConverter()
    result = converter.convert(str(path))
    # Concatenate text blocks per page if available; fallback to whole text split.
    # [Unverified] API surfaces may differ across versions.
    if hasattr(result, "document") and hasattr(result.document, "pages"):
        pages = []
        for i, page in enumerate(result.document.pages, start=1):
            blocks = []
            for b in getattr(page, "blocks", []):
                t = getattr(b, "text", "")
                if t:
                    blocks.append(t)
            pages.append("\n".join(blocks))
        return pages
    # Fallback: split by form-feed if present
    text_all = getattr(result, "text", "") or ""
    return text_all.split("\f") if "\f" in text_all else [text_all]


# -----------------------------
# Chunking (simple: page-by-page)
# -----------------------------
def iter_page_chunks(pages: List[str]) -> List[Dict[str, Any]]:
    """Yield dicts: {'page_num': int, 'text': str} for non-empty pages."""
    out: List[Dict[str, Any]] = []
    for i, t in enumerate(pages, start=1):
        clean = (t or "").strip()
        if clean:
            out.append({"page_num": i, "text": clean})
    return out


# -----------------------------
# LLM call (Structured Outputs)
# -----------------------------
def extract_tables_from_text(
    client: OpenAI, model: str, page_num: int, text: str
) -> TablesBundle:
    user_prompt = USER_PROMPT_TEMPLATE.format(page_num=page_num, page_text=text[:120000])  # crude safety cap
    # Use structured outputs with Pydantic parsing
    try:
        resp = client.responses.parse(
            model=model,
            temperature=0,
            input=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            response_format=TablesBundle,
        )
        parsed: TablesBundle = resp.parsed  # type: ignore
        # Backfill page numbers in case model omitted them
        for tbl in parsed.tables:
            if not getattr(tbl, "page", None):
                tbl.page = page_num
        return parsed
    except ValidationError as ve:
        # Model returned wrong shape; return empty bundle with note
        return TablesBundle(tables=[], notes=f"ValidationError: {ve}")
    except Exception as e:
        return TablesBundle(tables=[], notes=f"LLM error: {e}")


# -----------------------------
# Write outputs
# -----------------------------
def write_table_csv(out_dir: Path, base: str, idx: int, table: DetectedTable) -> Path:
    out_dir.mkdir(parents=True, exist_ok=True)
    fname = f"{base}_p{table.page:03d}_t{idx:02d}.csv"
    fpath = out_dir / fname
    # CSV write
    with fpath.open("w", encoding="utf-8") as f:
        f.write(",".join(table.columns) + "\n")
        for row in table.rows:
            cells = [c.value.replace("\n", " ").replace(",", " ").strip() for c in row.cells]
            # pad/truncate to columns length
            if len(cells) < len(table.columns):
                cells = cells + [""] * (len(table.columns) - len(cells))
            elif len(cells) > len(table.columns):
                cells = cells[: len(table.columns)]
            f.write(",".join(cells) + "\n")
    return fpath


def write_summary_jsonl(summary_path: Path, records: List[Dict[str, Any]]) -> None:
    with summary_path.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")


# -----------------------------
# Main
# -----------------------------
def main():
    ap = argparse.ArgumentParser(description="Extract tables from PDFs via LLM-structured outputs on extracted text.")
    ap.add_argument("inputs", nargs="+", help="One or more PDF files (or directories to scan).")
    ap.add_argument("--out", default="tables_out", help="Output directory (default: tables_out).")
    ap.add_argument("--engine", choices=["auto", "docling", "pdfplumber"], default="auto",
                    help="Text extraction engine (default: auto).")
    ap.add_argument("--model", default="gpt-4o-mini", help="OpenAI model name (default: gpt-4o-mini).")
    ap.add_argument("--per-table", action="store_true", help="Write each table to its own CSV file (default).")
    ap.add_argument("--merge", action="store_true", help="Also write one merged CSV per PDF (union of all tables).")
    args = ap.parse_args()

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Collect PDF files
    pdfs: List[Path] = []
    for inp in args.inputs:
        p = Path(inp)
        if p.is_file() and p.suffix.lower() == ".pdf":
            pdfs.append(p)
        elif p.is_dir():
            pdfs.extend([q for q in p.rglob("*.pdf") if q.is_file()])
        else:
            print(f"[warn] Skipping non-PDF input: {p}")

    if not pdfs:
        print("[error] No PDF files found.")
        raise SystemExit(2)

    # OpenAI client (requires OPENAI_API_KEY in env)
    client = OpenAI()

    summary_records: List[Dict[str, Any]] = []

    for pdf_path in tqdm(pdfs, desc="Processing PDFs"):
        base = slugify(pdf_path.stem) or "document"
        print(f"\n=== {pdf_path.name} ===")

        # Choose engine
        pages: List[str] = []
        use_engine = args.engine
        if use_engine == "auto":
            # Try docling first if installed; else pdfplumber
            try:
                pages = extract_text_docling(pdf_path)
                use_engine = "docling"
            except Exception:
                pages = extract_text_pdfplumber(pdf_path)
                use_engine = "pdfplumber"
        elif use_engine == "docling":
            try:
                pages = extract_text_docling(pdf_path)
            except Exception as e:
                print(f"[warn] Docling failed ({e}); falling back to pdfplumber.")
                pages = extract_text_pdfplumber(pdf_path)
                use_engine = "pdfplumber"
        else:
            pages = extract_text_pdfplumber(pdf_path)

        # Iterate pages -> LLM
        all_tables: List[DetectedTable] = []
        for chunk in iter_page_chunks(pages):
            bundle = extract_tables_from_text(
                client=client, model=args.model, page_num=chunk["page_num"], text=chunk["text"]
            )
            if bundle.notes:
                print(f"[note] {bundle.notes}")
            if bundle.tables:
                all_tables.extend(bundle.tables)

        # Write per-table CSVs
        emitted_files: List[str] = []
        for i, tbl in enumerate(all_tables, start=1):
            path_csv = write_table_csv(out_dir, base, i, tbl)
            emitted_files.append(str(path_csv))

        # Optional merge (simple vertical concat with column union)
        if args.merge and all_tables:
            # Build unioned header
            all_cols = []
            for t in all_tables:
                for c in t.columns:
                    if c not in all_cols:
                        all_cols.append(c)
            merged_path = out_dir / f"{base}__merged.csv"
            with merged_path.open("w", encoding="utf-8") as f:
                # add provenance columns
                header = all_cols + ["__page", "__table_index", "__title", "__engine"]
                f.write(",".join(header) + "\n")
                for idx, t in enumerate(all_tables, start=1):
                    col_index = {c: k for k, c in enumerate(t.columns)}
                    for row in t.rows:
                        row_vals = [""] * len(all_cols)
                        for c_idx, c_name in enumerate(t.columns):
                            v = row.cells[c_idx].value if c_idx < len(row.cells) else ""
                            row_vals[col_index[c_name]] = v.replace("\n", " ").replace(",", " ").strip()
                        meta = [str(t.page), str(idx), (t.title or "").replace(",", " "), "LLM"]
                        f.write(",".join(row_vals + meta) + "\n")
            emitted_files.append(str(merged_path))

        summary_records.append(
            {
                "pdf": str(pdf_path),
                "engine_used": use_engine,
                "pages": len(pages),
                "tables_found": len(all_tables),
                "outputs": emitted_files,
            }
        )

    # Write summary JSONL
    write_summary_jsonl(out_dir / "_summary.jsonl", summary_records)
    print(f"\nDone. Summary written to: {out_dir / '_summary.jsonl'}")
    print("[Reminder] This LLM-based method is a prototype; validate outputs before relying on them.")
    

if __name__ == "__main__":
    main()

```

---

## scripts/file_management_backup.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# dependencies = []
# ///

import os
import shutil
from datetime import datetime

# Config (change to your own!)
SOURCE_FOLDER = "."
DEST_FOLDER = "."
TODAY = datetime.today().strftime('%Y-%m-%d')

# Create destination if it doesn't exist
os.makedirs(DEST_FOLDER, exist_ok=True)

# Walk the source folder
for root, dirs, files in os.walk(SOURCE_FOLDER):
    # Limit folder depth to 2 below source (so total 3: base + 2)
    depth = root[len(SOURCE_FOLDER):].count(os.sep)
    if depth > 2:
        continue

    # Compute relative path and new destination
    rel_path = os.path.relpath(root, SOURCE_FOLDER)
    target_dir = os.path.join(DEST_FOLDER, rel_path)
    os.makedirs(target_dir, exist_ok=True)

    # Copy and rename files
    for file in files:
        src_file = os.path.join(root, file)
        new_filename = f"{TODAY}_{file}"
        dst_file = os.path.join(target_dir, new_filename)
        shutil.copy2(src_file, dst_file)

print(f"Backup completed with date-prefix '{TODAY}'.")

```

---

## scripts/file_management_dir_org_with_visuals.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "pandas>=2.0.0",
#     "matplotlib>=3.7.0",
#     "seaborn>=0.12.0",
#     "plotly>=5.17.0",
#     "rich>=13.0.0",
#     "numpy>=1.24.0"
# ]
# ///

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import Progress
import sys
import numpy as np

def get_file_size_mb(size_bytes):
    """Convert bytes to MB"""
    return round(size_bytes / (1024 * 1024), 2)

def get_file_info(root_path):
    """Scan all files in the directory tree and collect information"""
    files_data = []
    console = Console()
    
    with Progress() as progress:
        task = progress.add_task("[cyan]Scanning files...", total=None)
        
        for root, dirs, files in os.walk(root_path):
            for file in files:
                # Skip hidden files and system files
                if file.startswith('.'):
                    continue
                    
                file_path = os.path.join(root, file)
                try:
                    stat_info = os.stat(file_path)
                    
                    # Get relative folder path
                    rel_folder = os.path.relpath(root, root_path)
                    if rel_folder == '.':
                        rel_folder = 'Root'
                    
                    # Get file extension
                    ext = os.path.splitext(file)[1].lower()
                    if not ext:
                        ext = 'No Extension'
                    
                    # File size in MB
                    size_mb = get_file_size_mb(stat_info.st_size)
                    
                    # Last modified date
                    mod_time = datetime.fromtimestamp(stat_info.st_mtime)
                    
                    files_data.append({
                        'filename': file,
                        'folder': rel_folder,
                        'extension': ext,
                        'size_mb': size_mb,
                        'size_bytes': stat_info.st_size,
                        'modified_date': mod_time,
                        'full_path': file_path
                    })
                    
                    progress.advance(task)
                    
                except (OSError, IOError):
                    # Skip files that can't be accessed
                    continue
    
    return pd.DataFrame(files_data)

def create_summary_table(df):
    """Create a rich summary table"""
    console = Console()
    
    # Overall summary
    table = Table(title="üìÅ Folder Summary", style="cyan")
    table.add_column("Metric", style="bold blue")
    table.add_column("Value", style="green")
    
    total_files = len(df)
    total_size_gb = df['size_mb'].sum() / 1024
    avg_file_size = df['size_mb'].mean()
    largest_file = df.loc[df['size_mb'].idxmax()]
    
    table.add_row("Total Files", f"{total_files:,}")
    table.add_row("Total Size", f"{total_size_gb:.2f} GB")
    table.add_row("Average File Size", f"{avg_file_size:.2f} MB")
    table.add_row("Largest File", f"{largest_file['filename']} ({largest_file['size_mb']:.2f} MB)")
    table.add_row("Total Folders", f"{df['folder'].nunique()}")
    table.add_row("File Types", f"{df['extension'].nunique()}")
    
    console.print(table)
    console.print()

def create_folder_breakdown_table(df):
    """Create a breakdown by folder"""
    console = Console()
    
    folder_summary = df.groupby('folder').agg({
        'filename': 'count',
        'size_mb': ['sum', 'mean'],
        'extension': 'nunique'
    }).round(2)
    
    folder_summary.columns = ['File Count', 'Total Size (MB)', 'Avg Size (MB)', 'File Types']
    folder_summary = folder_summary.sort_values('Total Size (MB)', ascending=False)
    
    table = Table(title="üìÇ Breakdown by Folder", style="yellow")
    table.add_column("Folder", style="bold cyan")
    table.add_column("Files", justify="right", style="green")
    table.add_column("Total Size (MB)", justify="right", style="blue")
    table.add_column("Avg Size (MB)", justify="right", style="magenta")
    table.add_column("File Types", justify="right", style="yellow")
    
    for folder, row in folder_summary.head(20).iterrows():
        table.add_row(
            folder,
            f"{int(row['File Count']):,}",
            f"{row['Total Size (MB)']:,.2f}",
            f"{row['Avg Size (MB)']:,.2f}",
            f"{int(row['File Types'])}"
        )
    
    console.print(table)
    console.print()

def create_file_type_table(df):
    """Create a breakdown by file type"""
    console = Console()
    
    ext_summary = df.groupby('extension').agg({
        'filename': 'count',
        'size_mb': ['sum', 'mean']
    }).round(2)
    
    ext_summary.columns = ['File Count', 'Total Size (MB)', 'Avg Size (MB)']
    ext_summary = ext_summary.sort_values('File Count', ascending=False)
    
    table = Table(title="üìÑ Breakdown by File Type", style="green")
    table.add_column("Extension", style="bold blue")
    table.add_column("Files", justify="right", style="cyan")
    table.add_column("Total Size (MB)", justify="right", style="yellow")
    table.add_column("Avg Size (MB)", justify="right", style="magenta")
    
    for ext, row in ext_summary.head(15).iterrows():
        table.add_row(
            ext,
            f"{int(row['File Count']):,}",
            f"{row['Total Size (MB)']:,.2f}",
            f"{row['Avg Size (MB)']:,.2f}"
        )
    
    console.print(table)
    console.print()

def create_visualizations(df):
    """Create beautiful visualizations"""
    # Set up the plotting style
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(20, 15))
    fig.patch.set_facecolor('#0a0a0a')
    
    # 1. Files by folder (top 10)
    plt.subplot(2, 3, 1)
    folder_counts = df['folder'].value_counts().head(10)
    bars = plt.bar(range(len(folder_counts)), folder_counts.values, 
                   color=plt.cm.viridis(np.linspace(0, 1, len(folder_counts))))
    plt.title('üìÅ Files by Folder (Top 10)', fontsize=14, color='white', pad=20)
    plt.ylabel('Number of Files', color='white')
    plt.xticks(range(len(folder_counts)), folder_counts.index, rotation=45, ha='right', color='white')
    plt.yticks(color='white')
    
    # 2. File size distribution by folder
    plt.subplot(2, 3, 2)
    folder_sizes = df.groupby('folder')['size_mb'].sum().sort_values(ascending=False).head(10)
    bars = plt.bar(range(len(folder_sizes)), folder_sizes.values,
                   color=plt.cm.plasma(np.linspace(0, 1, len(folder_sizes))))
    plt.title('üíæ Total Size by Folder (Top 10)', fontsize=14, color='white', pad=20)
    plt.ylabel('Size (MB)', color='white')
    plt.xticks(range(len(folder_sizes)), folder_sizes.index, rotation=45, ha='right', color='white')
    plt.yticks(color='white')
    
    # 3. File types distribution
    plt.subplot(2, 3, 3)
    ext_counts = df['extension'].value_counts().head(10)
    colors = plt.cm.Set3(np.linspace(0, 1, len(ext_counts)))
    plt.pie(ext_counts.values, labels=ext_counts.index, autopct='%1.1f%%', 
            colors=colors, startangle=90)
    plt.title('üìÑ File Types Distribution', fontsize=14, color='white', pad=20)
    
    # 4. File size histogram
    plt.subplot(2, 3, 4)
    # Filter out very large files for better visualization
    size_data = df[df['size_mb'] < df['size_mb'].quantile(0.95)]['size_mb']
    plt.hist(size_data, bins=50, color='skyblue', alpha=0.7, edgecolor='white')
    plt.title('üìä File Size Distribution', fontsize=14, color='white', pad=20)
    plt.xlabel('Size (MB)', color='white')
    plt.ylabel('Frequency', color='white')
    plt.xticks(color='white')
    plt.yticks(color='white')
    
    # 5. Files over time (by modification date)
    plt.subplot(2, 3, 5)
    df['month_year'] = df['modified_date'].dt.to_period('M')
    monthly_counts = df['month_year'].value_counts().sort_index().tail(12)
    plt.plot(range(len(monthly_counts)), monthly_counts.values, 
             marker='o', linewidth=2, markersize=6, color='orange')
    plt.title('üìÖ Files Modified Over Time (Last 12 Months)', fontsize=14, color='white', pad=20)
    plt.ylabel('Number of Files', color='white')
    plt.xticks(range(len(monthly_counts)), [str(x) for x in monthly_counts.index], 
               rotation=45, ha='right', color='white')
    plt.yticks(color='white')
    plt.grid(True, alpha=0.3)
    
    # 6. Largest files
    plt.subplot(2, 3, 6)
    largest_files = df.nlargest(10, 'size_mb')
    bars = plt.barh(range(len(largest_files)), largest_files['size_mb'].values,
                    color=plt.cm.Reds(np.linspace(0.3, 1, len(largest_files))))
    plt.title('üéØ Largest Files', fontsize=14, color='white', pad=20)
    plt.xlabel('Size (MB)', color='white')
    plt.yticks(range(len(largest_files)), 
               [f[:30] + '...' if len(f) > 30 else f for f in largest_files['filename']], 
               color='white')
    plt.xticks(color='white')
    
    plt.tight_layout()
    plt.savefig('dir_analysis.png', dpi=300, bbox_inches='tight', 
                facecolor='#0a0a0a', edgecolor='none')
    plt.show()

def save_detailed_csv(df):
    """Save detailed file list to CSV"""
    # Create a clean version for CSV
    csv_df = df.copy()
    csv_df['size_gb'] = csv_df['size_mb'] / 1024
    csv_df = csv_df.sort_values(['folder', 'size_mb'], ascending=[True, False])
    
    # Select relevant columns
    csv_df = csv_df[['folder', 'filename', 'extension', 'size_mb', 'size_gb', 
                     'modified_date', 'full_path']]
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'dir_file_inventory_{timestamp}.csv'
    csv_df.to_csv(filename, index=False)
    
    console = Console()
    console.print(f"\nüíæ Detailed file inventory saved to: [bold green]{filename}[/bold green]")

def main():
    dir_path = sys.argv[1]
    
    try:
        save = sys.argv[2]
    except IndexError:
        save = False
    
    console = Console()
    console.print(f"[bold cyan]üîç Analyzing {dir_path} Folder...[/bold cyan]\n")
    
    # Check if path exists
    if not os.path.exists(dir_path):
        console.print(f"[bold red]‚ùå Path not found: {dir_path}[/bold red]")
        return
    
    # Scan files
    df = get_file_info(dir_path)
    
    if df.empty:
        console.print("[bold yellow]‚ö†Ô∏è  No files found![/bold yellow]")
        return
    
    # Display summary tables
    create_summary_table(df)
    create_folder_breakdown_table(df)
    create_file_type_table(df)
    
    # Create visualizations
    console.print("[bold green]üìä Creating visualizations...[/bold green]")
    create_visualizations(df)
    
    # Save detailed CSV
    if save:
        save_detailed_csv(df)
    else:
        console.print("[bold yellow]‚ö†Ô∏è  Detailed CSV not saved![/bold yellow]")
    
    console.print("\n[bold green]‚ú® Analysis complete! Check the generated files:[/bold green]")
    console.print("üìä dir_analysis.png - Visual charts")
    console.print("üìã dir_file_inventory_*.csv - Detailed file list")

if __name__ == "__main__":
    main()
```

---

## scripts/file_management_image_categorization_ai.py

```python
#!/usr/bin/env -S uv run -s
# /// script
# requires-python = ">=3.9"
# ///

"""
Simple image organizer that classifies images with Ollama and organizes them
into category folders.

Usage examples:
  - Run and move files (default):
      ./image_classification_usecases.py \
        "/path/to/folder" \
        --categories Screenshots Photos Documents Receipts Other

  - Run and copy files instead of moving:
      ./image_classification_usecases.py \
        "/path/to/folder" \
        --categories Screenshots Photos Documents Receipts Other \
        --copy

  - Custom output directory:
      ./image_classification_usecases.py \
        "/path/to/folder" \
        --categories Screenshots Photos \
        --output "/path/to/organized-images"

Note:
  - Requires `ollama` and the `qwen2.5vl` model available locally.
  - The model is prompted to output only the category name.
"""

from __future__ import annotations

import argparse
import hashlib
import os
import shutil
import subprocess
from pathlib import Path
from typing import Dict, Iterable, List, Tuple


SUPPORTED_EXTENSIONS = {
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp", ".tif", ".tiff", ".heic"
}


def _iter_images(folder_path: Path) -> Iterable[Path]:
    for path in folder_path.rglob("*"):
        if path.is_file() and path.suffix.lower() in SUPPORTED_EXTENSIONS:
            yield path


def _short_file_hash(path: Path, length: int = 8) -> str:
    md5 = hashlib.md5()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5.update(chunk)
    return md5.hexdigest()[:length]


def _classify_with_ollama(image_path: Path, categories: List[str]) -> str:
    # Prompt asks the model to return ONLY the category name
    prompt = (
        f"Classify this image in: '{image_path}' according to these categories: {categories}, "
        "your output should ONLY be the category name and nothing else."
    )
    try:
        result = subprocess.run(
            ["ollama", "run", "llama3.2:vision", prompt],
            capture_output=True,
            text=True,
            check=True,
        )
        print(result)
        raw = (result.stdout or "").strip()
        # Use the last non-empty line, stripped of extra punctuation/quotes
        candidate = "\n".join([line for line in raw.splitlines() if line.strip()]).splitlines()[-1].strip()
        candidate = candidate.strip("`\"' .:;!#[]{}()\t")
        # Normalize to one of the provided categories; case-insensitive exact match
        lower_to_original = {c.lower(): c for c in categories}
        if candidate.lower() in lower_to_original:
            return lower_to_original[candidate.lower()]
        # Try contains match (e.g., "Category: Photos")
        for c in categories:
            if c.lower() in candidate.lower():
                return c
        return "Uncategorized"
    except Exception:
        return "Uncategorized"


def _prepare_destination(base: Path, category: str, source_file: Path) -> Path:
    category_dir = base / category
    category_dir.mkdir(parents=True, exist_ok=True)
    destination = category_dir / source_file.name
    if destination.exists():
        suffix = source_file.suffix
        stem = source_file.stem
        digest = _short_file_hash(source_file)
        destination = category_dir / f"{stem}_{digest}{suffix}"
    return destination


def _move_or_copy(src: Path, dst: Path, copy: bool) -> None:
    if copy:
        shutil.copy2(src, dst)
    else:
        shutil.move(str(src), str(dst))


def organize_images_in_folder(
    folder_path: str,
    categories: Iterable[str],
    output_path: str = "./organized-images",
    copy_instead_of_move: bool = False,
) -> Dict[str, object]:
    folder = Path(folder_path).expanduser().resolve()
    output = Path(output_path).expanduser().resolve()
    output.mkdir(parents=True, exist_ok=True)

    category_list = list(categories)
    if not category_list:
        category_list = ["Screenshots", "Photos", "Documents", "Receipts", "Other"]

    summary = {
        "processed": 0,
        "moved": 0,
        "copied": 0,
        "errors": 0,
        "uncategorized": 0,
        "by_category": {c: 0 for c in category_list},
        "mappings": [],  # list of (src, dst, category)
    }

    if not folder.exists() or not folder.is_dir():
        raise FileNotFoundError(f"Folder not found or not a directory: {folder}")

    for image_path in _iter_images(folder):
        summary["processed"] += 1
        category = _classify_with_ollama(image_path, category_list)
        if category not in category_list:
            summary["uncategorized"] += 1
        destination = _prepare_destination(output, category, image_path)
        try:
            _move_or_copy(image_path, destination, copy_instead_of_move)
            summary["mappings"].append((str(image_path), str(destination), category))
            if copy_instead_of_move:
                summary["copied"] += 1
            else:
                summary["moved"] += 1
            if category in summary["by_category"]:
                summary["by_category"][category] += 1
            else:
                summary["by_category"][category] = 1
        except Exception:
            summary["errors"] += 1

    _print_report(summary, output, copy_instead_of_move)
    return summary


def _print_report(summary: Dict[str, object], output_dir: Path, copied: bool) -> None:
    action_emoji = "üìÑ‚û°Ô∏è" if copied else "üì¶‚û°Ô∏è"
    action_word = "copied" if copied else "moved"

    print("\n" + "‚ïê" * 60)
    print("üóÇÔ∏è  Image Organizer Report")
    print("‚ïê" * 60)
    print(f"üìç Output directory: {output_dir}")
    print(
        f"üìä Processed: {summary['processed']}  |  ‚úÖ {action_word.title()}: {summary[action_word]}  |  ‚ö†Ô∏è Errors: {summary['errors']}  |  üè∑Ô∏è Uncategorized: {summary['uncategorized']}"
    )

    if summary.get("by_category"):
        print("\nüìÅ By category:")
        for category, count in sorted(summary["by_category"].items(), key=lambda x: (-x[1], x[0])):
            if count > 0:
                print(f"  - üè∑Ô∏è {category}: {count}")

    # Show up to 10 mappings for quick reference
    mappings: List[Tuple[str, str, str]] = summary.get("mappings", [])  # type: ignore[assignment]
    if mappings:
        print("\n" + ("üó∫Ô∏è  Mappings (first 10):"))
        for src, dst, category in mappings[:10]:
            print(f"  {action_emoji} [{category}] {Path(src).name} ‚Üí {dst}")

    print("\n‚ú® Done.")


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Organize images in a folder using Ollama classification.")
    parser.add_argument("folder", help="Path to the folder containing images")
    parser.add_argument(
        "--categories",
        "-c",
        nargs="+",
        default=[],
        help="List of categories to use for classification (space-separated)",
    )
    parser.add_argument(
        "--output",
        "-o",
        default="./organized-images",
        help="Destination base folder (default: ./organized-images)",
    )
    parser.add_argument(
        "--copy",
        action="store_true",
        help="Copy files instead of moving (default is to move)",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = _parse_args()
    organize_images_in_folder(
        folder_path=args.folder,
        categories=args.categories,
        output_path=args.output,
        copy_instead_of_move=args.copy,
    )
```

---

## scripts/generate_image_with_openai.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "openai",
#   "pillow",
# ]
# ///

import base64
import io
import sys
from datetime import datetime
from pathlib import Path

from openai import OpenAI
from PIL import Image


def generate_image(prompt: str) -> bytes:
    """Generate an image using OpenAI's API and return the image bytes."""
    client = OpenAI()
    
    response = client.images.generate(
        model="gpt-image-1",
        prompt=prompt,
        n=1,
        size="1024x1024",
    )
    
    image_bytes = base64.b64decode(response.data[0].b64_json)
    return image_bytes


def display_image(image_bytes: bytes) -> Image.Image:
    """Display the image and return the PIL Image object."""
    image = Image.open(io.BytesIO(image_bytes))
    image.show()
    return image


def save_image(image_bytes: bytes, prompt: str) -> Path:
    """Save the image locally with a timestamped filename."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_prompt = "".join(c if c.isalnum() or c in " -_" else "" for c in prompt[:30]).strip()
    safe_prompt = safe_prompt.replace(" ", "_")
    filename = f"{timestamp}_{safe_prompt}.png"
    
    filepath = Path(filename)
    filepath.write_bytes(image_bytes)
    return filepath


def main():
    prompt = input("Enter your image prompt: ").strip()
    
    if not prompt:
        print("Error: Prompt cannot be empty.")
        sys.exit(1)
    
    print(f"\nGenerating image for: '{prompt}'...")
    
    try:
        image_bytes = generate_image(prompt)
    except Exception as e:
        print(f"Error generating image: {e}")
        sys.exit(1)
    
    print("Image generated. Displaying...")
    display_image(image_bytes)
    
    while True:
        response = input("\nApprove and save this image? (yes/no): ").strip().lower()
        if response in ("yes", "y"):
            filepath = save_image(image_bytes, prompt)
            print(f"Image saved to: {filepath.absolute()}")
            break
        elif response in ("no", "n"):
            print("Image discarded.")
            break
        else:
            print("Please enter 'yes' or 'no'.")


if __name__ == "__main__":
    main()
```

---

## scripts/generate_ppt_from_paper.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "python-pptx>=0.6.23",
# ]
# ///

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.text import PP_ALIGN
from pptx.enum.shapes import MSO_SHAPE
from pptx.enum.dml import MSO_THEME_COLOR

# Colors from brand guidelines
BLACK = RGBColor(0,0,0)
WHITE = RGBColor(255,255,255)
L1 = RGBColor(0xF9,0xF9,0xF9)
L2 = RGBColor(0xF5,0xF5,0xF5)
L3 = RGBColor(0xF0,0xF0,0xF0)
D1 = RGBColor(0x33,0x33,0x33)
D2 = RGBColor(0x55,0x55,0x55)
D3 = RGBColor(0x66,0x66,0x66)
SUCCESS_BG = RGBColor(0xE8,0xF5,0xE8)
WARN_BG = RGBColor(0xFF,0xEE,0xEE)

prs = Presentation()
prs.slide_width = Inches(13.333)  # 16:9 default wide
prs.slide_height = Inches(7.5)

def set_slide_bg(slide, color=WHITE):
    fill = slide.background.fill
    fill.solid()
    fill.fore_color.rgb = color

def add_footer(slide):
    left = Inches(0.5)
    top = Inches(7.0)
    width = Inches(12.33)
    height = Inches(0.3)
    tx = slide.shapes.add_textbox(left, top, width, height)
    p = tx.text_frame.paragraphs[0]
    run = p.add_run()
    run.text = "¬© 2025 Automata Learning Lab | Professional educational materials crafted with care"
    run.font.name = "Helvetica"
    run.font.size = Pt(8)
    run.font.color.rgb = D3
    p.alignment = PP_ALIGN.CENTER

def add_title(slide, text, subtitle=None):
    # big uppercase title
    left = Inches(0.7); top = Inches(0.6); width = Inches(11.9); height = Inches(1.2)
    box = slide.shapes.add_textbox(left, top, width, height)
    tf = box.text_frame
    tf.clear()
    p = tf.paragraphs[0]
    p.text = text.upper()
    p.font.name = "Helvetica"
    p.font.size = Pt(34)
    p.font.bold = True
    p.font.color.rgb = BLACK
    if subtitle:
        p2 = tf.add_paragraph()
        p2.text = subtitle
        p2.font.name = "Helvetica"
        p2.font.size = Pt(18)
        p2.font.color.rgb = D2
        p2.space_before = Pt(10)

def add_bullets(slide, bullets, left=0.9, top=1.8, width=11.6, height=4.8, font_size=20):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(width), Inches(height))
    tf = box.text_frame
    tf.word_wrap = True
    tf.clear()
    for i, b in enumerate(bullets):
        p = tf.paragraphs[0] if i==0 else tf.add_paragraph()
        p.text = b
        p.level = 0
        p.font.name = "Helvetica"
        p.font.size = Pt(font_size)
        p.font.color.rgb = D1
        p.space_after = Pt(6)

def add_card(slide, title, body_lines, left, top, width, height, bg=L1, border=BLACK):
    shape = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(left), Inches(top), Inches(width), Inches(height))
    shape.fill.solid()
    shape.fill.fore_color.rgb = bg
    shape.line.color.rgb = border
    shape.line.width = Pt(1.5)
    tf = shape.text_frame
    tf.clear()
    p = tf.paragraphs[0]
    p.text = title
    p.font.name = "Helvetica"
    p.font.size = Pt(16)
    p.font.bold = True
    p.font.color.rgb = BLACK
    p.space_after = Pt(4)
    for line in body_lines:
        bp = tf.add_paragraph()
        bp.text = line
        bp.level = 0
        bp.font.name = "Helvetica"
        bp.font.size = Pt(13)
        bp.font.color.rgb = D1

def section_slide(title):
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    set_slide_bg(slide, WHITE)
    add_title(slide, title)
    add_footer(slide)
    return slide

# --- Slides content ---

# 1. Title slide
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Context Engineering 2.0", "The Context of Context Engineering (SII-GAIR, 2025)")
# author line
box = slide.shapes.add_textbox(Inches(0.7), Inches(2.2), Inches(12), Inches(1))
tf = box.text_frame
p=tf.paragraphs[0]
p.text="Key ideas & design considerations for modern AI agents"
p.font.name="Helvetica"; p.font.size=Pt(20); p.font.color.rgb=D2
add_footer(slide)

# 2. Agenda
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Agenda")
add_bullets(slide, [
    "Why context engineering matters now",
    "Entropy reduction view + formal definition",
    "Historical evolution: eras 1.0 ‚Üí 4.0",
    "Design considerations: collect, manage, use",
    "Applications, challenges, future directions"
])
add_footer(slide)

# 3. Motivation / problem
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Motivation")
add_bullets(slide, [
    "LLMs are sensitive to what‚Äôs inside the context window.",
    "Long‚Äëhorizon reasoning needs more than a bigger window.",
    "We need systematic ways to align machine behavior with human intent."
])
add_footer(slide)

# 4. What is context engineering?
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "What is Context Engineering?")
add_bullets(slide, [
    "Practice of designing, organizing, and managing context so machines act on human intentions.",
    "Includes prompting, RAG, tool use, memory, multimodal inputs, and context sharing.",
])
# callout
add_card(slide, "Core idea = Entropy Reduction", [
    "Humans fill gaps implicitly; machines can‚Äôt.",
    "So we compress high‚Äëentropy real‚Äëworld intent",
    "into low‚Äëentropy representations a model can use."
], left=0.9, top=3.3, width=11.5, height=2.2, bg=L2)
add_footer(slide)

# 5. Historical perspective
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "A Longer History")
add_bullets(slide, [
    "Context engineering predates LLMs by ~20 years.",
    "Roots in ubiquitous computing, context‚Äëaware systems, HCI.",
    "Each leap in machine intelligence changes how we build interfaces."
])
add_footer(slide)

# 6. Four-stage trajectory
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Four Eras of Context Engineering")
add_card(slide, "1.0 Primitive Computation", [
    "1990s‚Äì2020",
    "Structured, low‚Äëentropy inputs",
    "Humans translate intent for machines"
], left=0.7, top=1.9, width=3.1, height=2.2, bg=L1)
add_card(slide, "2.0 Agent‚ÄëCentric Intelligence", [
    "2020‚Äìpresent",
    "Natural language + ambiguity tolerance",
    "Prompting, RAG, tools, memory"
], left=3.95, top=1.9, width=3.3, height=2.2, bg=L1)
add_card(slide, "3.0 Human‚ÄëLevel (future)", [
    "Human‚Äëlike sensing & understanding",
    "Seamless collaboration",
    "Less explicit context mgmt"
], left=7.45, top=1.9, width=2.9, height=2.2, bg=L1)
add_card(slide, "4.0 Superhuman (speculative)", [
    "Models construct context proactively",
    "Reveal hidden needs",
    "‚ÄòGod‚Äôs‚Äëeye view‚Äô of intent"
], left=10.55, top=1.9, width=2.1, height=2.2, bg=L1)
add_bullets(slide, ["Trend: increasing intelligence ‚Üí decreasing human effort"], top=4.7, font_size=18)
add_footer(slide)

# 7. Design considerations overview
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Design Considerations (Era 2.0)")
add_bullets(slide, [
    "Three dimensions form the pipeline:",
    "1) Context Collection & Storage",
    "2) Context Management",
    "3) Context Usage"
], top=1.9)
add_footer(slide)

# 8. Collection principles
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Context Collection & Storage")
add_bullets(slide, [
    "Multimodal sources: text, images, audio, sensors, logs, APIs.",
    "Distributed storage: on‚Äëdevice, edge, cloud, layered by latency.",
])
add_card(slide, "Principle: Minimal Sufficiency", [
    "Collect/store only what's needed for the task.",
    "Value is sufficiency, not volume."
], left=0.9, top=3.2, width=5.8, height=2.2, bg=SUCCESS_BG)
add_card(slide, "Principle: Semantic Continuity", [
    "Preserve continuity of meaning, not just raw data.",
    "Maintain coherence across time and devices."
], left=6.6, top=3.2, width=5.6, height=2.2, bg=L2)
add_footer(slide)

# 9. Context management
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Context Management")
add_bullets(slide, [
    "Process raw context: clean, normalize, align modalities.",
    "Organize memory in layers: short‚Äëterm ‚Üí long‚Äëterm.",
    "Isolate context to avoid contamination across tasks.",
    "Abstract/compress using summaries or embeddings (‚Äòself‚Äëbaking‚Äô)."
])
add_footer(slide)

# 10. Context usage
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Context Usage")
add_bullets(slide, [
    "Intra‚Äësystem sharing: agents pass context via prompts, schemas, or shared memory.",
    "Cross‚Äësystem sharing: adapters or common representations between tools/agents.",
    "Context selection: pick relevant pieces for understanding and action.",
    "Proactive inference: mine preferences, infer hidden goals, offer help.",
    "Lifelong preservation: update personal context without drift."
])
add_footer(slide)

# 11. Application: Gemini CLI
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Applications: Coding CLIs")
add_bullets(slide, [
    "Example: Gemini CLI uses GEMINI.md files as project context.",
    "Static context loads at startup; dynamic context accumulates in dialog.",
    "History is periodically summarized into compact ‚Äòreasoning state‚Äô."
])
add_footer(slide)

# 12. Application: Deep Research
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Applications: Deep Research Agents")
add_bullets(slide, [
    "Open‚Äëended, long‚Äëhorizon search + reasoning.",
    "Need periodic compression to stay within window.",
    "Context snapshots preserve evidence and guide next searches."
])
add_footer(slide)

# 13. Application: BCI
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Applications: Brain‚ÄëComputer Interfaces")
add_bullets(slide, [
    "BCIs can collect internal user state directly (attention, emotion, cognitive load).",
    "Richer + more convenient context collection beyond language.",
    "Still early: noisy signals, coarse interpretation."
])
add_footer(slide)

# 14. Challenges
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Challenges & Future Directions")
add_bullets(slide, [
    "Collection is still inefficient; users can‚Äôt always articulate intent.",
    "Storage bottlenecks at lifelong scale.",
    "Processing degradation in long contexts (latency + reasoning quality).",
    "System instability and semantic drift over time.",
    "Need better architectures + adaptive selection + evaluation frameworks."
])
add_footer(slide)

# 15. Conclusion
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Conclusion")
add_bullets(slide, [
    "Context engineering is a long‚Äëevolving discipline.",
    "Core job: reduce entropy between human intent and machine action.",
    "Era 2.0 requires systematic pipelines for collection, management, and usage.",
    "As models improve, human effort shifts from explicit context handling to oversight."
])
add_footer(slide)

# 16. Q&A
slide = prs.slides.add_slide(prs.slide_layouts[6])
set_slide_bg(slide)
add_title(slide, "Q&A")
add_footer(slide)

out_path = "./context-engineering-2.0-presentation.pptx"
prs.save(out_path)
out_path
```

---

## scripts/generic_presentation_maker.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "pandas",
#     "matplotlib",
#     "python-pptx",
#     "pydantic",
#     "openai"
# ]
# ///

"""
AI-powered CSV -> PPTX report generator using:
- OpenAI Responses API with Pydantic structured outputs
- Matplotlib for charts
- python-pptx for PowerPoint generation

Usage:
    uv run ai_csv_to_ppt.py path/to/data.csv \
        --output-pptx report.pptx \
        --charts-dir charts

Environment:
    export OPENAI_API_KEY=...
"""

import argparse
import os
from dataclasses import dataclass
from typing import List, Optional, Literal

import matplotlib
matplotlib.use("Agg")  # non-interactive backend
import matplotlib.pyplot as plt
import pandas as pd
from openai import OpenAI
from pydantic import BaseModel, Field
from pptx import Presentation
from pptx.util import Inches

# ---------- OpenAI / model config ----------

MODEL = "gpt-4.1-mini"  # change if you prefer another Responses-capable model


# ---------- Pydantic models for structured outputs ----------

ChartType = Literal["line", "bar", "scatter", "histogram"]


class ChartSpec(BaseModel):
    """Specification for a single chart."""
    title: str = Field(description="Title of the chart as shown in the slide.")
    description: str = Field(
        description="Short natural-language description of what this chart shows."
    )
    chart_type: ChartType = Field(
        description='One of: "line", "bar", "scatter", "histogram".'
    )
    x_column: str = Field(
        description="Column name to use on the X-axis (time, category, or index-like)."
    )
    y_columns: List[str] = Field(
        description="One or more numeric columns to plot on the Y-axis."
    )
    # Optional: simple filter to focus the data (not required)
    filter_hint: Optional[str] = Field(
        default=None,
        description=(
            "Optional human-readable filter for the subset of rows to plot, "
            "e.g. 'only year >= 2020' or 'Category == A'. This is advisory; "
            "the Python code may ignore it if it cannot be applied safely."
        ),
    )


class SlideSpec(BaseModel):
    """A single slide with bullets and optional charts."""
    title: str = Field(description="Slide title.")
    bullets: List[str] = Field(
        description="2‚Äì6 bullet points summarizing the key insight for this slide."
    )
    charts: List[ChartSpec] = Field(
        default_factory=list,
        description="0‚Äì2 charts to include on this slide.",
    )


class PresentationPlan(BaseModel):
    """Overall presentation plan."""
    title: str = Field(description="Overall presentation title.")
    subtitle: Optional[str] = Field(
        default=None, description="Optional subtitle or context."
    )
    slides: List[SlideSpec] = Field(
        description="Ordered list of slides to include."
    )


# ---------- Helper: summarize dataframe for the model ----------

def summarize_dataframe_for_llm(df: pd.DataFrame, csv_path: str, max_cols: int = 12) -> str:
    """Create a compact textual summary of the dataframe for the LLM."""
    lines: List[str] = []
    lines.append(f"Source file: {os.path.basename(csv_path)}")
    lines.append(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
    lines.append("")

    # Basic dtype info and sample values
    cols = list(df.columns)
    if len(cols) > max_cols:
        cols = cols[:max_cols]
        lines.append(
            f"Only the first {max_cols} columns are shown below (dataset has more columns)."
        )
        lines.append("")

    for col in cols:
        series = df[col]
        dtype = str(series.dtype)
        non_null = series.dropna()
        sample_values = non_null.head(5).tolist()

        lines.append(f"Column: {col}")
        lines.append(f"  dtype: {dtype}")
        lines.append(f"  non-null count: {non_null.shape[0]}")
        if sample_values:
            lines.append("  sample values:")
            for v in sample_values:
                lines.append(f"    - {repr(v)}")
        lines.append("")

    # Basic numeric summary
    numeric_cols = df.select_dtypes(include="number").columns.tolist()
    if numeric_cols:
        lines.append("Numeric columns summary (pandas describe):")
        lines.append(df[numeric_cols].describe().to_string())
        lines.append("")

    return "\n".join(lines)


# ---------- LLM: design the presentation plan ----------

def generate_presentation_plan(client: OpenAI, df: pd.DataFrame, csv_path: str) -> PresentationPlan:
    """
    Call the OpenAI Responses API with a Pydantic structured output
    to design a presentation for the given dataframe.
    """
    dataset_summary = summarize_dataframe_for_llm(df, csv_path)

    instructions = """
You are a senior data storyteller and presentation designer.

You will receive a description of a tabular dataset (columns, dtypes, sample values, summary stats).
Your task is to design a concise, insight-focused PowerPoint-style presentation.

OUTPUT FORMAT (STRICT):
- You MUST output a JSON object that matches the provided schema:
  PresentationPlan -> slides: list[SlideSpec] -> charts: list[ChartSpec].
- Use ONLY the column names exactly as provided.
- chart_type must be one of: "line", "bar", "scatter", "histogram".
- x_column must be a single column from the dataset.
- y_columns must be a list of one or more numeric columns.

CONTENT GUIDELINES:
- Use at most 6 slides (excluding the title slide).
- Each slide:
  - 2 to 6 clear, non-redundant bullet points.
  - At most 2 charts (0, 1, or 2).
- Prefer:
  - time-like or ordered columns (e.g., dates) for x_column in line charts.
  - categorical columns for x_column in bar charts.
  - pairs of numeric columns for scatter charts.
  - single numeric column for histograms.
- Avoid:
  - using the same chart repeatedly with trivial changes.
  - referencing columns that do not exist.
- The overall title and subtitle should describe the whole dataset and its purpose (if clear).
"""

    # Structured Outputs via Responses API + Pydantic
    # See: https://platform.openai.com/docs/guides/structured-outputs
    response = client.responses.parse(
        model=MODEL,
        instructions=instructions,
        input=dataset_summary,
        temperature=0.3,
        text_format=PresentationPlan,  # Pydantic model schema
    )

    plan: PresentationPlan = response.output_parsed
    return plan


# ---------- Chart generation from ChartSpec ----------

@dataclass
class ChartContext:
    path: str
    spec: ChartSpec


def safe_apply_filter_hint(df: pd.DataFrame, filter_hint: Optional[str]) -> pd.DataFrame:
    """
    Very conservative: currently just returns the original df.

    You could extend this to parse simple hints like
    'only year >= 2020' or 'Category == A', but doing that
    robustly requires additional logic.

    For safety and generic use, we ignore filter_hint here.
    """
    _ = filter_hint  # unused for now
    return df


def create_chart_from_spec(
    df: pd.DataFrame,
    spec: ChartSpec,
    charts_dir: str,
    slide_idx: int,
    chart_idx: int,
) -> ChartContext:
    """
    Given the dataframe and a ChartSpec, generate a PNG chart
    and return its file path + context.
    """
    os.makedirs(charts_dir, exist_ok=True)

    # Apply filter (currently no-op)
    data = safe_apply_filter_hint(df, spec.filter_hint)

    # Ensure columns exist
    if spec.x_column not in data.columns:
        raise ValueError(f"x_column '{spec.x_column}' not found in dataframe.")
    for col in spec.y_columns:
        if col not in data.columns:
            raise ValueError(f"y_column '{col}' not found in dataframe.")

    x = data[spec.x_column]
    y_cols = spec.y_columns

    plt.figure(figsize=(8, 4.5))

    if spec.chart_type == "line":
        for col in y_cols:
            plt.plot(x, data[col], label=col)
    elif spec.chart_type == "bar":
        # For multi-series bar, we do grouped bars; for 1 series, simple bar
        import numpy as np

        indices = np.arange(len(x))
        width = 0.8 / max(len(y_cols), 1)
        for i, col in enumerate(y_cols):
            plt.bar(indices + i * width, data[col], width=width, label=col)
        plt.xticks(indices + (len(y_cols) - 1) * width / 2, x, rotation=45, ha="right")
    elif spec.chart_type == "scatter":
        # scatter uses first y as main; others as additional series
        for col in y_cols:
            plt.scatter(x, data[col], label=col, alpha=0.7)
    elif spec.chart_type == "histogram":
        # histogram uses first y-column only
        col = y_cols[0]
        plt.hist(data[col].dropna(), bins=30)
    else:
        raise ValueError(f"Unsupported chart_type: {spec.chart_type}")

    plt.title(spec.title)
    plt.xlabel(spec.x_column)
    if spec.chart_type != "histogram":
        plt.ylabel(", ".join(y_cols))
    else:
        plt.ylabel("Frequency")
    if len(y_cols) > 1 and spec.chart_type != "histogram":
        plt.legend()
    plt.tight_layout()

    filename = f"slide{slide_idx:02d}_chart{chart_idx:02d}.png"
    path = os.path.join(charts_dir, filename)
    plt.savefig(path)
    plt.close()

    return ChartContext(path=path, spec=spec)


# ---------- PPTX generation ----------

def build_presentation(
    plan: PresentationPlan,
    df: pd.DataFrame,
    charts_dir: str,
    csv_path: str,
    output_pptx: str,
) -> None:
    prs = Presentation()

    # Title slide
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = plan.title or "AI-Generated Data Report"

    if len(slide.placeholders) > 1:
        subtitle_shape = slide.placeholders[1]
        subtitle_text_parts = []
        if plan.subtitle:
            subtitle_text_parts.append(plan.subtitle)
        subtitle_text_parts.append(f"Source: {os.path.basename(csv_path)}")
        subtitle_shape.text = " | ".join(subtitle_text_parts)

    # Content slides
    content_layout = prs.slide_layouts[1]  # title + content

    for i, slide_spec in enumerate(plan.slides, start=1):
        slide = prs.slides.add_slide(content_layout)
        slide.shapes.title.text = slide_spec.title

        # Bullet text in main placeholder
        body = slide.placeholders[1]
        tf = body.text_frame
        tf.clear()
        if slide_spec.bullets:
            # First bullet as main paragraph
            p = tf.paragraphs[0]
            p.text = slide_spec.bullets[0]
            p.level = 0

            for bullet in slide_spec.bullets[1:]:
                bp = tf.add_paragraph()
                bp.text = bullet
                bp.level = 0

        # Generate and place charts
        for j, chart_spec in enumerate(slide_spec.charts[:2], start=1):  # max 2 charts visually
            try:
                ctx = create_chart_from_spec(df, chart_spec, charts_dir, i, j)
            except Exception as e:
                # If chart creation fails, add a note to the slide instead of breaking everything
                err_par = tf.add_paragraph()
                err_par.text = f"(Chart '{chart_spec.title}' could not be created: {e})"
                err_par.level = 1
                continue

            # Add the chart image to the slide
            # Simple layout: charts below bullets, from left to right
            left = Inches(0.5 + (j - 1) * 4.8)
            top = Inches(3.0)
            width = Inches(4.5)
            slide.shapes.add_picture(ctx.path, left, top, width=width)

    prs.save(output_pptx)


# ---------- Optional extra AI call: refine title/subtitle (example of multi-call usage) ----------

def refine_title_with_ai(client: OpenAI, plan: PresentationPlan, csv_path: str) -> PresentationPlan:
    """
    Second call to AI: rewrite title & subtitle to be a bit more polished.
    If it fails for any reason, return the original plan.
    """

    class TitleRefinement(BaseModel):
        title: str
        subtitle: Optional[str] = None

    try:
        instructions = """
You are a presentation title editor.
Given a draft title and subtitle plus a file name, return a polished title and subtitle.
Keep it concise and professional.
"""
        input_text = f"""
Draft title: {plan.title!r}
Draft subtitle: {plan.subtitle!r}
File name: {os.path.basename(csv_path)!r}
"""

        resp = client.responses.parse(
            model=MODEL,
            instructions=instructions,
            input=input_text,
            temperature=0.2,
            text_format=TitleRefinement,
        )
        refined = resp.output_parsed
        plan.title = refined.title
        plan.subtitle = refined.subtitle
    except Exception:
        # swallow errors; keep original plan
        pass

    return plan


# ---------- CLI / main ----------

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate an AI-designed PowerPoint report from a CSV file."
    )
    parser.add_argument("csv_path", help="Path to the input .csv file.")
    parser.add_argument(
        "--output-pptx",
        default="ai_data_report.pptx",
        help="Output PowerPoint path (default: ai_data_report.pptx).",
    )
    parser.add_argument(
        "--charts-dir",
        default="charts",
        help="Directory to store generated chart images (default: charts).",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    if not os.path.exists(args.csv_path):
        raise FileNotFoundError(f"CSV file not found: {args.csv_path}")

    print(f"Loading CSV: {args.csv_path}")
    df = pd.read_csv(args.csv_path)

    client = OpenAI()

    print("Calling OpenAI to design presentation plan (structured output)...")
    plan = generate_presentation_plan(client, df, args.csv_path)

    print("Refining title & subtitle with a second AI call...")
    plan = refine_title_with_ai(client, plan, args.csv_path)

    print("Building PowerPoint and generating charts...")
    build_presentation(
        plan=plan,
        df=df,
        charts_dir=args.charts_dir,
        csv_path=args.csv_path,
        output_pptx=args.output_pptx,
    )

    print(f"Done. Presentation saved to {args.output_pptx}")
    print(f"Charts saved under directory: {args.charts_dir}")


if __name__ == "__main__":
    main()
```

---

## scripts/live_demo_organize_files.py

```python
# /// script
# requires-python = ">=3.12"
# ///

import os
import shutil
from pathlib import Path
import sys

def organize_files_in_folder(folder_path):
    """
    Organize files in the given folder into three subfolders:
    - documents: .md, .txt, .pdf
    - media: .png, .jpg, .jpeg, .gif, .bmp, .tiff, .svg, .webp
    - others: any other file types or files without extension

    Only top-level files in folder_path are moved (subfolders are left intact).
    If a filename conflict occurs in the destination folder, a numeric suffix is added.
    Returns a success message with counts of moved files.
    """
    folder = Path(folder_path)

    if not folder.exists() or not folder.is_dir():
        return f"Error: '{folder_path}' does not exist or is not a directory."

    # Define extension sets
    documents_ext = {'.md', '.txt', '.pdf'}
    media_ext = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.svg', '.webp'}

    # Create destination folders
    documents_dir = folder / "documents"
    media_dir = folder / "media"
    others_dir = folder / "others"

    for d in (documents_dir, media_dir, others_dir):
        d.mkdir(exist_ok=True)

    moved_counts = {"documents": 0, "media": 0, "others": 0}

    for entry in folder.iterdir():
        # Only process files at the top level (skip directories)
        if not entry.is_file():
            continue

        # Skip files that are already inside one of the target folders
        if entry.parent in (documents_dir, media_dir, others_dir):
            continue

        ext = entry.suffix.lower()

        # Decide destination based on extension
        if ext in documents_ext:
            dest_dir = documents_dir
            key = "documents"
        elif ext in media_ext:
            dest_dir = media_dir
            key = "media"
        else:
            dest_dir = others_dir
            key = "others"

        # Resolve name conflicts by adding a suffix before the extension
        dest_path = dest_dir / entry.name
        if dest_path.exists():
            stem = entry.stem
            suffix = entry.suffix
            counter = 1
            while True:
                new_name = f"{stem}({counter}){suffix}"
                dest_path = dest_dir / new_name
                if not dest_path.exists():
                    break
                counter += 1

        # Move the file
        try:
            shutil.move(str(entry), str(dest_path))
            moved_counts[key] += 1
        except Exception as e:
            # If a file couldn't be moved, continue with others
            print(f"Warning: could not move '{entry.name}': {e}")

    return (f"Files organized successfully. Moved {moved_counts['documents']} to 'documents', "
            f"{moved_counts['media']} to 'media', and {moved_counts['others']} to 'others'.")

organized_files = organize_files_in_folder("/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course/notebooks/assets")

print(organized_files)
```

---

## scripts/paper_download_summarize.py

```python
# # Prompt used:
# I want to create an. automation that given a url for a pdf it will:

# Download the paper locally as demonstrated in this example:
# import urllib.request
# url = "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
# filepath = "../assets/paper3.pdf"
# urllib.request.urlretrieve(url, filepath)
# print("Downloaded paper")

# It will load the contents of the pdf ?

# It will use the OpenAI API to summarize that pdf similar to what you can see in this example:
# from openai import OpenAI
# def ask_ai(prompt, model_name="gpt-5-mini"):
#  # access the openai API and call this model to get a response
#  client = OpenAI()
#  response = client.chat.completions.create(
#      messages=[
#          {
#              "role": "user",
#               "content": prompt
#          }
#      ],
#      model=model_name
#  )
#  output = response.choices[0].message.content
#  print(output)
#  return output
# I also want to be able to run this script with uv using inline metadata 
# https://docs.astral.sh/uv/guides/scripts/

# just add the necessary metadata

# THe output summary should be saved as a .md file with the name of the paper separated by dashes
# AI Tools used: Claude 4.5 Opus chatbot


# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "pymupdf",
#     "openai",
# ]
# ///

import urllib.request
import os
import re
from openai import OpenAI
import fitz  # PyMuPDF


def download_pdf(url: str, filepath: str) -> str:
    """Download a PDF from a URL to a local filepath."""
    urllib.request.urlretrieve(url, filepath)
    print(f"Downloaded paper to {filepath}")
    return filepath


def extract_text_from_pdf(filepath: str) -> str:
    """Extract text content from a PDF file."""
    doc = fitz.open(filepath)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    print(f"Extracted {len(text)} characters from PDF")
    return text


def ask_ai(prompt: str, model_name: str = "gpt-4o-mini") -> str:
    """Call OpenAI API to get a response."""
    client = OpenAI()
    response = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": prompt
            }
        ],
        model=model_name
    )
    output = response.choices[0].message.content
    return output


def summarize_pdf(pdf_text: str, model_name: str = "gpt-4o-mini") -> str:
    """Generate a summary of the PDF content using OpenAI."""
    prompt = f"""Please provide a comprehensive summary of the following academic paper. 
Include:
- Main objective/research question
- Key methodology
- Main findings/results
- Conclusions and implications

Paper content:
{pdf_text[:100000]}  # Truncate to avoid token limits
"""
    summary = ask_ai(prompt, model_name)
    print("Generated summary")
    return summary


def extract_paper_name(url: str) -> str:
    """Extract and format the paper name from the URL for use as filename."""
    # Get the filename from URL
    filename = url.split("/")[-1]
    # Remove .pdf extension
    name = filename.replace(".pdf", "").replace("-Paper", "")
    # Clean up the name - replace underscores with dashes, remove special chars
    name = re.sub(r'[^a-zA-Z0-9\-]', '-', name)
    # Remove multiple consecutive dashes
    name = re.sub(r'-+', '-', name)
    # Remove leading/trailing dashes
    name = name.strip('-')
    return name


def save_summary(summary: str, paper_name: str, output_dir: str = ".") -> str:
    """Save the summary as a markdown file."""
    filepath = os.path.join(output_dir, f"{paper_name}.md")
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# Summary: {paper_name}\n\n")
        f.write(summary)
    print(f"Saved summary to {filepath}")
    return filepath


def summarize_paper_from_url(url: str, output_dir: str = ".", model_name: str = "gpt-4o-mini") -> str:
    """Main function: Download PDF, extract text, summarize, and save."""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract paper name for filenames
    paper_name = extract_paper_name(url)
    
    # Download PDF
    pdf_path = os.path.join(output_dir, f"{paper_name}.pdf")
    download_pdf(url, pdf_path)
    
    # Extract text
    pdf_text = extract_text_from_pdf(pdf_path)
    
    # Summarize
    summary = summarize_pdf(pdf_text, model_name)
    
    # Save summary
    summary_path = save_summary(summary, paper_name, output_dir)
    
    return summary_path


# Example usage
if __name__ == "__main__":
    url = "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
    output_dir = "./summaries"
    
    summary_path = summarize_paper_from_url(url, output_dir)
    print(f"\nDone! Summary saved to: {summary_path}")
```

---

## scripts/repo_to_markdown.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = []
# ///
"""
Fetch contents of .md, .py, and .ipynb files from a repository
and combine them into a single markdown file.

Usage:
    uv run repo_to_markdown.py /path/to/repo
    uv run repo_to_markdown.py /path/to/repo -o output.md
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime


EXTENSIONS = {'.md', '.py', '.ipynb'}

EXCLUDED_DIRS = {
    '.git', '__pycache__', 'node_modules', '.venv', 'venv', 
    'env', '.env', '.idea', '.vscode', 'dist', 'build', 
    'egg-info', '.eggs', '.tox', '.pytest_cache'
}


def should_include_file(filepath: Path) -> bool:
    """Check if file should be included based on extension."""
    return filepath.suffix.lower() in EXTENSIONS


def should_skip_directory(dirname: str) -> bool:
    """Check if directory should be skipped."""
    return dirname in EXCLUDED_DIRS or dirname.startswith('.')


def read_file_contents(filepath: Path) -> str:
    """Read and return file contents, handling different file types."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # For Jupyter notebooks, extract a readable format
        if filepath.suffix.lower() == '.ipynb':
            return format_notebook(content, filepath)
        
        return content
    except UnicodeDecodeError:
        return f"[Error: Could not decode file as UTF-8]"
    except Exception as e:
        return f"[Error reading file: {e}]"


def format_notebook(content: str, filepath: Path) -> str:
    """Format Jupyter notebook content into readable markdown."""
    try:
        notebook = json.loads(content)
        cells = notebook.get('cells', [])
        
        formatted_parts = []
        for i, cell in enumerate(cells):
            cell_type = cell.get('cell_type', 'unknown')
            source = ''.join(cell.get('source', []))
            
            if cell_type == 'markdown':
                formatted_parts.append(source)
            elif cell_type == 'code':
                formatted_parts.append(f"```python\n{source}\n```")
            elif cell_type == 'raw':
                formatted_parts.append(f"```\n{source}\n```")
        
        return '\n\n'.join(formatted_parts)
    except json.JSONDecodeError:
        return f"[Error: Invalid JSON in notebook]"
    except Exception as e:
        return f"[Error parsing notebook: {e}]"


def get_language_for_extension(ext: str) -> str:
    """Return the markdown code fence language for a file extension."""
    mapping = {
        '.py': 'python',
        '.md': 'markdown',
    }
    return mapping.get(ext.lower(), '')


def collect_files(repo_path: Path) -> list[tuple[Path, str]]:
    """Walk the repo and collect all matching files with their contents."""
    files_collected = []
    
    for root, dirs, files in os.walk(repo_path):
        # Filter out excluded directories (modifies dirs in-place)
        dirs[:] = [d for d in dirs if not should_skip_directory(d)]
        dirs.sort()  # Consistent ordering
        
        root_path = Path(root)
        
        for filename in sorted(files):
            filepath = root_path / filename
            
            if should_include_file(filepath):
                relative_path = filepath.relative_to(repo_path)
                content = read_file_contents(filepath)
                files_collected.append((relative_path, content))
    
    return files_collected


def generate_markdown(repo_path: Path, files: list[tuple[Path, str]]) -> str:
    """Generate the combined markdown output."""
    lines = [
        f"# Repository Contents: {repo_path.name}",
        f"",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"",
        f"**Files included:** `.md`, `.py`, `.ipynb`",
        f"",
        f"**Total files:** {len(files)}",
        f"",
        "---",
        "",
        "## Table of Contents",
        "",
    ]
    
    # Generate TOC
    for i, (relative_path, _) in enumerate(files, 1):
        anchor = str(relative_path).replace('/', '-').replace('.', '-').replace('_', '-').lower()
        lines.append(f"{i}. [{relative_path}](#{anchor})")
    
    lines.extend(["", "---", ""])
    
    # Generate file contents
    for relative_path, content in files:
        ext = relative_path.suffix.lower()
        
        lines.append(f"## {relative_path}")
        lines.append("")
        
        if ext == '.ipynb':
            # Notebook content is already formatted as markdown
            lines.append(content)
        elif ext == '.md':
            # For markdown files, include as-is but in a details block to avoid formatting conflicts
            lines.append("<details>")
            lines.append(f"<summary>View {relative_path.name}</summary>")
            lines.append("")
            lines.append(content)
            lines.append("")
            lines.append("</details>")
        else:
            # For code files, wrap in code fence
            lang = get_language_for_extension(ext)
            lines.append(f"```{lang}")
            lines.append(content)
            lines.append("```")
        
        lines.extend(["", "---", ""])
    
    return '\n'.join(lines)


def main():
    parser = argparse.ArgumentParser(
        description='Combine .md, .py, and .ipynb files from a repo into a single markdown file.'
    )
    parser.add_argument(
        'repo_path',
        type=str,
        help='Path to the repository root'
    )
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='Output file path (default: <repo_name>_contents.md)'
    )
    parser.add_argument(
        '--no-notebooks',
        action='store_true',
        help='Exclude .ipynb files'
    )
    parser.add_argument(
        '--no-markdown',
        action='store_true',
        help='Exclude .md files'
    )
    parser.add_argument(
        '--no-python',
        action='store_true',
        help='Exclude .py files'
    )
    
    args = parser.parse_args()
    
    # Modify extensions based on flags
    if args.no_notebooks:
        EXTENSIONS.discard('.ipynb')
    if args.no_markdown:
        EXTENSIONS.discard('.md')
    if args.no_python:
        EXTENSIONS.discard('.py')
    
    repo_path = Path(args.repo_path).resolve()
    
    if not repo_path.exists():
        print(f"Error: Path '{repo_path}' does not exist.")
        return 1
    
    if not repo_path.is_dir():
        print(f"Error: Path '{repo_path}' is not a directory.")
        return 1
    
    # Determine output path
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = Path(f"{repo_path.name}_contents.md")
    
    print(f"Scanning: {repo_path}")
    print(f"Extensions: {', '.join(sorted(EXTENSIONS))}")
    
    # Collect files
    files = collect_files(repo_path)
    print(f"Found {len(files)} files")
    
    # Generate markdown
    markdown_content = generate_markdown(repo_path, files)
    
    # Write output
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"Output written to: {output_path}")
    return 0


if __name__ == '__main__':
    exit(main())
```

---

## scripts/repo_tree.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = []
# ///
"""
Generate a tree-like structure of a directory/repository.

Usage:
    uv run repo_tree.py [path] [options]
    
Examples:
    uv run repo_tree.py .
    uv run repo_tree.py /path/to/repo --exclude __pycache__ .git
    uv run repo_tree.py . --show-hidden
"""

import argparse
import os
from pathlib import Path


DEFAULT_EXCLUDES = {
    "__pycache__",
    ".git",
    ".svn",
    ".hg",
    "node_modules",
    ".venv",
    "venv",
    ".env",
    ".idea",
    ".vscode",
    ".DS_Store",
    "*.pyc",
    "*.pyo",
}


def should_exclude(name: str, exclude_patterns: set[str]) -> bool:
    """Check if a file/folder should be excluded."""
    if name in exclude_patterns:
        return True
    for pattern in exclude_patterns:
        if pattern.startswith("*") and name.endswith(pattern[1:]):
            return True
    return False


def generate_tree(
    root_path: Path,
    prefix: str = "",
    exclude_patterns: set[str] | None = None,
    show_hidden: bool = False,
) -> list[str]:
    """
    Generate a tree structure for the given directory.
    
    Args:
        root_path: The root directory to generate tree for
        prefix: Current prefix for tree branches
        exclude_patterns: Set of patterns to exclude
        show_hidden: Whether to show hidden files/folders
        
    Returns:
        List of strings representing the tree structure
    """
    if exclude_patterns is None:
        exclude_patterns = DEFAULT_EXCLUDES
    
    lines = []
    
    try:
        entries = list(root_path.iterdir())
    except PermissionError:
        return [f"{prefix}[Permission Denied]"]
    
    # Filter entries
    filtered_entries = []
    for entry in entries:
        name = entry.name
        
        # Skip hidden files unless requested
        if not show_hidden and name.startswith("."):
            continue
            
        # Skip excluded patterns
        if should_exclude(name, exclude_patterns):
            continue
            
        filtered_entries.append(entry)
    
    # Sort: directories first, then files, both alphabetically
    filtered_entries.sort(key=lambda e: (not e.is_dir(), e.name.lower()))
    
    # Tree characters
    BRANCH = "‚îú‚îÄ‚îÄ "
    LAST_BRANCH = "‚îî‚îÄ‚îÄ "
    VERTICAL = "‚îÇ   "
    SPACE = "    "
    
    for i, entry in enumerate(filtered_entries):
        is_last = i == len(filtered_entries) - 1
        connector = LAST_BRANCH if is_last else BRANCH
        
        lines.append(f"{prefix}{connector}{entry.name}")
        
        if entry.is_dir():
            # Recursively process subdirectory
            extension = SPACE if is_last else VERTICAL
            subtree = generate_tree(
                entry,
                prefix=prefix + extension,
                exclude_patterns=exclude_patterns,
                show_hidden=show_hidden,
            )
            lines.extend(subtree)
    
    return lines


def print_tree(
    root_path: str | Path,
    exclude_patterns: set[str] | None = None,
    show_hidden: bool = False,
) -> str:
    """
    Generate and return the full tree as a string.
    
    Args:
        root_path: Path to the root directory
        exclude_patterns: Patterns to exclude (uses defaults if None)
        show_hidden: Whether to include hidden files
        
    Returns:
        Complete tree structure as a string
    """
    root = Path(root_path).resolve()
    
    if not root.exists():
        return f"Error: Path '{root}' does not exist"
    
    if not root.is_dir():
        return f"Error: Path '{root}' is not a directory"
    
    lines = [root.name]
    tree_lines = generate_tree(
        root,
        exclude_patterns=exclude_patterns,
        show_hidden=show_hidden,
    )
    lines.extend(tree_lines)
    
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="Generate a tree-like structure of a directory/repository"
    )
    parser.add_argument(
        "path",
        nargs="?",
        default=".",
        help="Root directory path (default: current directory)",
    )
    parser.add_argument(
        "--exclude",
        "-e",
        nargs="*",
        default=None,
        help="Additional patterns to exclude",
    )
    parser.add_argument(
        "--no-default-excludes",
        action="store_true",
        help="Don't use default exclusion patterns",
    )
    parser.add_argument(
        "--show-hidden",
        "-a",
        action="store_true",
        help="Show hidden files and directories",
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output file (default: print to stdout)",
    )
    
    args = parser.parse_args()
    
    # Build exclusion patterns
    if args.no_default_excludes:
        exclude_patterns = set()
    else:
        exclude_patterns = DEFAULT_EXCLUDES.copy()
    
    if args.exclude:
        exclude_patterns.update(args.exclude)
    
    # Generate tree
    tree = print_tree(
        args.path,
        exclude_patterns=exclude_patterns,
        show_hidden=args.show_hidden,
    )
    
    # Output
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(tree)
        print(f"Tree written to {args.output}")
    else:
        print(tree)


if __name__ == "__main__":
    main()
```

---

## scripts/roster_app.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = []
# ///

"""
Simple Office Rostering App (CLI)

Features:
- Add employees (name, role, usual working days, working hours)
- Add vacation/leave dates per employee
- Define minimum staffing levels per role and weekday
- Automatically generate a roster for a given date range:
    * Respects employee working days/hours
    * Skips employees on vacation
    * Tries to meet minimum staffing per role/day (warns if impossible)
- Export roster to:
    * CSV: roster.csv
    * ICS calendar: roster.ics
- View roster as a table in the terminal
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import date, datetime, time, timedelta
from typing import Dict, List, Set


# ---------- Data Models ----------

@dataclass
class Employee:
    name: str
    role: str
    work_days: Set[int]  # 0=Monday ... 6=Sunday
    start_time: time
    end_time: time


@dataclass
class RosterEntry:
    date: date
    employee_name: str
    role: str
    start_time: time
    end_time: time


# ---------- Global In-Memory "DB" ----------

employees: Dict[str, Employee] = {}
vacations: Dict[str, Set[date]] = {}
staffing_requirements: Dict[int, Dict[str, int]] = {}  # weekday -> {role: min_staff}
roster: List[RosterEntry] = []


# ---------- Helpers ----------

WEEKDAY_NAMES = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]


def parse_weekdays(s: str) -> Set[int]:
    """
    Parse a string like "Mon,Tue,Fri" or "mon fri" into a set of weekday ints.
    """
    s = s.replace(",", " ")
    tokens = [t.strip().capitalize()[:3] for t in s.split() if t.strip()]
    result: Set[int] = set()
    for tok in tokens:
        if tok in WEEKDAY_NAMES:
            result.add(WEEKDAY_NAMES.index(tok))
        else:
            print(f"Unknown weekday token: {tok} (ignored)")
    return result


def parse_time_str(s: str) -> time:
    """
    Parse HH:MM into datetime.time.
    """
    try:
        hour, minute = s.split(":")
        return time(hour=int(hour), minute=int(minute))
    except Exception:
        raise ValueError(f"Invalid time format: {s}. Use HH:MM, e.g. 09:00")


def parse_date_str(s: str) -> date:
    """
    Parse YYYY-MM-DD into datetime.date.
    """
    try:
        return datetime.strptime(s, "%Y-%m-%d").date()
    except Exception:
        raise ValueError(f"Invalid date format: {s}. Use YYYY-MM-DD")


def input_nonempty(prompt: str) -> str:
    while True:
        value = input(prompt).strip()
        if value:
            return value
        print("Value cannot be empty.")


def print_header(title: str) -> None:
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)


# ---------- Employee Management ----------

def add_employee() -> None:
    print_header("Add Employee")
    name = input_nonempty("Employee name: ")
    if name in employees:
        print("Employee with this name already exists. Overwriting info.")

    role = input_nonempty("Role (e.g. Reception, Manager, Support): ")

    print("Enter normal working days (e.g. Mon-Fri or Mon,Wed,Fri):")
    work_days_str = input_nonempty("Working days: ")
    work_days = parse_weekdays(work_days_str)
    if not work_days:
        print("No valid working days parsed. Defaulting to Mon-Fri.")
        work_days = set(range(5))

    start_time = parse_time_str(input_nonempty("Start time (HH:MM, e.g. 09:00): "))
    end_time = parse_time_str(input_nonempty("End time (HH:MM, e.g. 17:00): "))

    employees[name] = Employee(
        name=name,
        role=role,
        work_days=work_days,
        start_time=start_time,
        end_time=end_time,
    )
    print(f"Employee '{name}' added/updated.")


def list_employees() -> None:
    print_header("Employees")
    if not employees:
        print("No employees defined yet.")
        return
    print(f"{'Name':20} {'Role':15} {'Days':20} {'Hours':10}")
    print("-" * 70)
    for e in employees.values():
        days = ",".join(WEEKDAY_NAMES[d] for d in sorted(e.work_days))
        hours = f"{e.start_time.strftime('%H:%M')}-{e.end_time.strftime('%H:%M')}"
        print(f"{e.name:20} {e.role:15} {days:20} {hours:10}")


# ---------- Vacation Management ----------

def add_vacation() -> None:
    print_header("Add Vacation / Leave")
    if not employees:
        print("No employees to assign vacation to. Add employees first.")
        return

    name = input_nonempty("Employee name (must exist): ")
    if name not in employees:
        print(f"Employee '{name}' not found.")
        return

    start_str = input_nonempty("Vacation start date (YYYY-MM-DD): ")
    end_str = input_nonempty("Vacation end date (YYYY-MM-DD): ")
    start = parse_date_str(start_str)
    end = parse_date_str(end_str)
    if end < start:
        print("End date is before start date. Swapping.")
        start, end = end, start

    days = vacations.setdefault(name, set())
    cur = start
    while cur <= end:
        days.add(cur)
        cur += timedelta(days=1)

    print(
        f"Vacation recorded for {name} from {start.isoformat()} to {end.isoformat()} "
        f"({(end - start).days + 1} day(s))."
    )


def list_vacations() -> None:
    print_header("Vacations / Leave")
    if not vacations:
        print("No vacations recorded yet.")
        return

    for name, days in vacations.items():
        sorted_days = sorted(days)
        print(f"\n{name}:")
        for d in sorted_days:
            print(f"  - {d.isoformat()} ({WEEKDAY_NAMES[d.weekday()]})")


# ---------- Staffing Requirements ----------

def set_staffing_requirements() -> None:
    print_header("Set Staffing Requirements by Weekday and Role")
    print("You can configure minimum staff per role for each weekday.")
    print("Leave role blank to move to next weekday.")
    print("Example: Role=Reception, Min staff=2")

    for wd in range(7):
        print(f"\n--- {WEEKDAY_NAMES[wd]} ---")
        role_map: Dict[str, int] = staffing_requirements.get(wd, {})
        while True:
            role = input("Role (blank to finish this day): ").strip()
            if not role:
                break
            min_str = input_nonempty(f"Minimum staff for role '{role}' on {WEEKDAY_NAMES[wd]}: ")
            try:
                min_staff = int(min_str)
                if min_staff < 0:
                    raise ValueError
            except ValueError:
                print("Please enter a non-negative integer.")
                continue
            role_map[role] = min_staff
            print(f"Set {WEEKDAY_NAMES[wd]} / {role} -> {min_staff}")
        if role_map:
            staffing_requirements[wd] = role_map

    print("\nStaffing requirements updated.")


def show_staffing_requirements() -> None:
    print_header("Current Staffing Requirements")
    if not staffing_requirements:
        print("No staffing requirements defined.")
        return
    for wd in range(7):
        if wd not in staffing_requirements:
            continue
        print(f"\n{WEEKDAY_NAMES[wd]}:")
        for role, min_staff in staffing_requirements[wd].items():
            print(f"  - {role}: {min_staff} staff")


# ---------- Roster Generation ----------

def generate_roster() -> None:
    global roster
    print_header("Generate Roster")
    if not employees:
        print("No employees defined. Add employees first.")
        return
    if not staffing_requirements:
        print("No staffing requirements defined. Set them first.")
        return

    start_str = input_nonempty("Roster start date (YYYY-MM-DD): ")
    end_str = input_nonempty("Roster end date (YYYY-MM-DD): ")
    start = parse_date_str(start_str)
    end = parse_date_str(end_str)
    if end < start:
        print("End date is before start date. Swapping.")
        start, end = end, start

    roster = []
    per_role_last_index: Dict[str, int] = {}

    cur = start
    while cur <= end:
        wd = cur.weekday()
        day_requirements = staffing_requirements.get(wd, {})
        if not day_requirements:
            cur += timedelta(days=1)
            continue

        for role, min_staff in day_requirements.items():
            if min_staff <= 0:
                continue

            candidates = [
                e
                for e in employees.values()
                if e.role == role
                and wd in e.work_days
                and cur not in vacations.get(e.name, set())
            ]
            candidates.sort(key=lambda e: e.name)

            if not candidates:
                print(
                    f"WARNING: {cur.isoformat()} ({WEEKDAY_NAMES[wd]}): "
                    f"No available employees for role '{role}'."
                )
                continue

            if len(candidates) < min_staff:
                print(
                    f"WARNING: {cur.isoformat()} ({WEEKDAY_NAMES[wd]}): "
                    f"Needed {min_staff} for role '{role}', only {len(candidates)} available. "
                    f"Assigning all available."
                )

            # Round-robin assignment
            count_to_assign = min(min_staff, len(candidates))
            last_idx = per_role_last_index.get(role, -1)
            for i in range(count_to_assign):
                idx = (last_idx + 1 + i) % len(candidates)
                emp = candidates[idx]
                roster.append(
                    RosterEntry(
                        date=cur,
                        employee_name=emp.name,
                        role=emp.role,
                        start_time=emp.start_time,
                        end_time=emp.end_time,
                    )
                )
            per_role_last_index[role] = (last_idx + count_to_assign) % len(candidates)

        cur += timedelta(days=1)

    print(f"\nRoster generated with {len(roster)} entries.")


def view_roster() -> None:
    print_header("Roster (Table View)")
    if not roster:
        print("No roster generated yet.")
        return
    print(f"{'Date':12} {'Day':4} {'Employee':20} {'Role':15} {'Hours':10}")
    print("-" * 70)
    for entry in sorted(roster, key=lambda r: (r.date, r.role, r.employee_name)):
        date_str = entry.date.isoformat()
        day_str = WEEKDAY_NAMES[entry.date.weekday()]
        hours = f"{entry.start_time.strftime('%H:%M')}-{entry.end_time.strftime('%H:%M')}"
        print(f"{date_str:12} {day_str:4} {entry.employee_name:20} {entry.role:15} {hours:10}")


# ---------- Export Functions ----------

def export_roster_csv(filename: str = "roster.csv") -> None:
    if not roster:
        print("No roster to export. Generate it first.")
        return
    lines = ["date,weekday,employee,role,start_time,end_time"]
    for entry in sorted(roster, key=lambda r: (r.date, r.role, r.employee_name)):
        lines.append(
            ",".join(
                [
                    entry.date.isoformat(),
                    WEEKDAY_NAMES[entry.date.weekday()],
                    entry.employee_name.replace(",", " "),
                    entry.role.replace(",", " "),
                    entry.start_time.strftime("%H:%M"),
                    entry.end_time.strftime("%H:%M"),
                ]
            )
        )
    with open(filename, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"Roster exported to {filename}")


def export_roster_ics(filename: str = "roster.ics") -> None:
    if not roster:
        print("No roster to export. Generate it first.")
        return

    def dtstamp() -> str:
        return datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")

    def dt_local(d: date, t: time) -> str:
        # Naive local datetime in "floating time" (no timezone)
        return datetime.combine(d, t).strftime("%Y%m%dT%H%M%S")

    lines = [
        "BEGIN:VCALENDAR",
        "VERSION:2.0",
        "PRODID:-//OfficeRoster//EN",
    ]

    now_stamp = dtstamp()
    for idx, entry in enumerate(roster, start=1):
        uid = f"{idx}-{entry.date.isoformat()}-{entry.employee_name.replace(' ', '')}@office-roster"
        start_dt = dt_local(entry.date, entry.start_time)
        end_dt = dt_local(entry.date, entry.end_time)
        summary = f"{entry.role}: {entry.employee_name}"
        lines.extend(
            [
                "BEGIN:VEVENT",
                f"UID:{uid}",
                f"DTSTAMP:{now_stamp}",
                f"DTSTART:{start_dt}",
                f"DTEND:{end_dt}",
                f"SUMMARY:{summary}",
                "END:VEVENT",
            ]
        )

    lines.append("END:VCALENDAR")

    with open(filename, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Roster exported to {filename} (ICS calendar)")


# ---------- Main Menu ----------

def main_menu() -> None:
    while True:
        print_header("Office Rostering App")
        print("1) Add employee")
        print("2) List employees")
        print("3) Add vacation/leave")
        print("4) List vacations/leave")
        print("5) Set staffing requirements")
        print("6) Show staffing requirements")
        print("7) Generate roster")
        print("8) View roster (table)")
        print("9) Export roster to CSV")
        print("10) Export roster to ICS calendar")
        print("0) Exit")
        choice = input("Choose an option: ").strip()

        if choice == "1":
            add_employee()
        elif choice == "2":
            list_employees()
        elif choice == "3":
            add_vacation()
        elif choice == "4":
            list_vacations()
        elif choice == "5":
            set_staffing_requirements()
        elif choice == "6":
            show_staffing_requirements()
        elif choice == "7":
            generate_roster()
        elif choice == "8":
            view_roster()
        elif choice == "9":
            export_roster_csv()
        elif choice == "10":
            export_roster_ics()
        elif choice == "0":
            print("Goodbye!")
            break
        else:
            print("Invalid choice, please try again.")


if __name__ == "__main__":
    main_menu()

```

---

## scripts/roster_app_html.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = ["flask"]
# ///

from __future__ import annotations
from flask import Flask, request, render_template_string, send_file, redirect, url_for
from dataclasses import dataclass
from datetime import date, datetime, time, timedelta
from typing import Dict, List, Set
import io

app = Flask(__name__)

# -----------------------------------------------------
# Data Models
# -----------------------------------------------------

@dataclass
class Employee:
    name: str
    role: str
    work_days: Set[int]
    start_time: time
    end_time: time

@dataclass
class RosterEntry:
    date: date
    employee_name: str
    role: str
    start_time: time
    end_time: time

employees: Dict[str, Employee] = {}
vacations: Dict[str, Set[date]] = {}
staffing: Dict[int, Dict[str, int]] = {}
roster: List[RosterEntry] = []

WEEK = ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]


# -----------------------------------------------------
# Helpers
# -----------------------------------------------------

def parse_date(s: str) -> date:
    return datetime.strptime(s, "%Y-%m-%d").date()

def parse_time(s: str) -> time:
    return datetime.strptime(s, "%H:%M").time()

def parse_days(s: str) -> Set[int]:
    toks = [x.strip().capitalize()[:3] for x in s.replace(",", " ").split()]
    return {WEEK.index(t) for t in toks if t in WEEK}


# -----------------------------------------------------
# HTML Template
# -----------------------------------------------------

BASE = """
<!DOCTYPE html>
<html>
<head>
    <title>Rostering App</title>
    <style>
        body { font-family: sans-serif; margin: 40px; background:#fafafa; }
        h2 { margin-top: 40px; }
        input, select { padding: 6px; margin: 4px; }
        .card { background:white; padding:20px; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.1); margin-bottom:20px; }
        .btn { background:#007bff; padding:8px 14px; color:white; border-radius:6px; text-decoration:none; }
        table { border-collapse:collapse; width:100%; margin-top:10px; }
        th, td { border:1px solid #ddd; padding:8px; text-align:left; }
    </style>
</head>
<body>
<h1>Office Rostering App</h1>

<div class="card">
    <h2>Add Employee</h2>
    <form method="post" action="/add_employee">
        <input name="name" placeholder="Name" required>
        <input name="role" placeholder="Role" required>
        <input name="days" placeholder="Work days (Mon-Fri)">
        <input name="start" placeholder="Start (09:00)">
        <input name="end" placeholder="End (17:00)">
        <button class="btn">Add</button>
    </form>
</div>

<div class="card">
    <h2>Add Vacation</h2>
    <form method="post" action="/add_vacation">
        <select name="name">
            {% for e in employees %}
            <option>{{e}}</option>
            {% endfor %}
        </select>
        <input name="start" type="date" required>
        <input name="end" type="date" required>
        <button class="btn">Add</button>
    </form>
</div>

<div class="card">
    <h2>Staffing Requirements</h2>
    <form method="post" action="/set_staffing">
        <select name="weekday">
            {% for i, w in week_list %}
            <option value="{{i}}">{{w}}</option>
            {% endfor %}
        </select>
        <input name="role" placeholder="Role">
        <input name="min_staff" type="number" min="0" placeholder="Min staff">
        <button class="btn">Set</button>
    </form>

    <h3>Current Requirements</h3>
    <ul>
    {% for d, roles in staffing.items() %}
        <li><b>{{week[d]}}</b>:
            {% for r, c in roles.items() %}
            {{r}}={{c}}
            {% endfor %}
        </li>
    {% endfor %}
    </ul>
</div>

<div class="card">
    <h2>Generate Roster</h2>
    <form method="post" action="/generate">
        <input name="start" type="date" required>
        <input name="end" type="date" required>
        <button class="btn">Generate</button>
    </form>
</div>

{% if roster %}
<div class="card">
    <h2>Roster</h2>
    <table>
        <tr><th>Date</th><th>Day</th><th>Employee</th><th>Role</th><th>Hours</th></tr>
        {% for r in roster %}
        <tr>
            <td>{{r.date}}</td>
            <td>{{week[r.date.weekday()]}}</td>
            <td>{{r.employee_name}}</td>
            <td>{{r.role}}</td>
            <td>{{r.start_time}}-{{r.end_time}}</td>
        </tr>
        {% endfor %}
    </table>

    <p>
        <a class="btn" href="/export_csv">Export CSV</a>
        <a class="btn" href="/export_ics">Export ICS</a>
    </p>
</div>
{% endif %}

</body>
</html>
"""


# -----------------------------------------------------
# Routes
# -----------------------------------------------------

@app.route("/")
def index():
    week_list = list(enumerate(WEEK))  # [(0,"Mon"), (1,"Tue"), ...]
    return render_template_string(
        BASE, 
        employees=list(employees.keys()), 
        staffing=staffing,
        week=WEEK,
        week_list=week_list,
        roster=roster
    )

@app.route("/add_employee", methods=["POST"])
def add_employee():
    name = request.form["name"]
    role = request.form["role"]
    days = parse_days(request.form["days"]) if request.form["days"] else set(range(5))
    start = parse_time(request.form["start"] or "09:00")
    end = parse_time(request.form["end"] or "17:00")
    employees[name] = Employee(name, role, days, start, end)
    return redirect(url_for("index"))

@app.route("/add_vacation", methods=["POST"])
def add_vacation():
    name = request.form["name"]
    start = parse_date(request.form["start"])
    end = parse_date(request.form["end"])
    vac = vacations.setdefault(name, set())
    cur = start
    while cur <= end:
        vac.add(cur)
        cur += timedelta(days=1)
    return redirect(url_for("index"))

@app.route("/set_staffing", methods=["POST"])
def set_staffing():
    wd = int(request.form["weekday"])
    role = request.form["role"]
    if not role:
        return redirect(url_for("index"))
    c = int(request.form["min_staff"])
    staffing.setdefault(wd, {})[role] = c
    return redirect(url_for("index"))

@app.route("/generate", methods=["POST"])
def generate():
    global roster
    roster = []

    start = parse_date(request.form["start"])
    end = parse_date(request.form["end"])

    last_index: Dict[str, int] = {}

    cur = start
    while cur <= end:
        wd = cur.weekday()
        day_require = staffing.get(wd, {})

        for role, needed in day_require.items():
            cands = [
                e for e in employees.values()
                if e.role == role
                and wd in e.work_days
                and cur not in vacations.get(e.name, set())
            ]
            cands.sort(key=lambda e: e.name)

            if not cands:
                continue

            assign = min(len(cands), needed)
            li = last_index.get(role, -1)

            for i in range(assign):
                idx = (li + 1 + i) % len(cands)
                emp = cands[idx]
                roster.append(RosterEntry(cur, emp.name, emp.role, emp.start_time, emp.end_time))

            last_index[role] = (li + assign) % len(cands)

        cur += timedelta(days=1)

    return redirect(url_for("index"))


# -----------------------------------------------------
# Export
# -----------------------------------------------------

@app.route("/export_csv")
def export_csv():
    if not roster:
        return redirect(url_for("index"))
    buf = io.StringIO()
    buf.write("date,weekday,employee,role,start,end\n")
    for r in roster:
        buf.write(f"{r.date},{WEEK[r.date.weekday()]},{r.employee_name},{r.role},{r.start_time},{r.end_time}\n")
    buf.seek(0)
    return send_file(
        io.BytesIO(buf.getvalue().encode()),
        as_attachment=True,
        download_name="roster.csv",
        mimetype="text/csv"
    )

@app.route("/export_ics")
def export_ics():
    if not roster:
        return redirect(url_for("index"))

    def dtfmt(d, t): return datetime.combine(d, t).strftime("%Y%m%dT%H%M%S")

    lines = ["BEGIN:VCALENDAR","VERSION:2.0"]
    for i, r in enumerate(roster):
        lines += [
            "BEGIN:VEVENT",
            f"UID:{i}@rostering",
            f"DTSTART:{dtfmt(r.date, r.start_time)}",
            f"DTEND:{dtfmt(r.date, r.end_time)}",
            f"SUMMARY:{r.role}: {r.employee_name}",
            "END:VEVENT"
        ]
    lines.append("END:VCALENDAR")

    data = "\n".join(lines).encode()
    return send_file(
        io.BytesIO(data),
        as_attachment=True,
        download_name="roster.ics",
        mimetype="text/calendar"
    )


# -----------------------------------------------------
# Run
# -----------------------------------------------------

if __name__ == "__main__":
    print("Rostering app running at http://127.0.0.1:5000")
    app.run(debug=False)

```

---

## scripts/summarize_pdf_live_demo.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "pymupdf",
#     "openai",
# ]
# ///

import urllib.request
import sys
from openai import OpenAI
import fitz


def download_pdf(url: str, filepath: str) -> str:
    """Download a PDF from a URL to a local filepath."""
    urllib.request.urlretrieve(url, filepath)
    print(f"Downloaded paper to {filepath}")
    return filepath


def extract_text_from_pdf(filepath: str) -> str:
    """Extract text content from a PDF file."""
    doc = fitz.open(filepath)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    print(f"Extracted {len(text)} characters from PDF")
    return text

def summarize_pdf(text: str) -> str:
    """Summarize the pdf text"""
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {
                "role": "user",
                "content": f"Summarize the following text: {text} into bullet points."
            }
        ],
        )
    
    return response.choices[0].message.content
    
    
    

# downloads the pdf and saves it to the current directory
# input_url = input("Enter the URL of the pdf to summarize: ") # takes input from the user when the user runs the script but manually

input_url = sys.argv[1]

pdf_path = download_pdf(input_url, "./paper.pdf")

# extract the text
text = extract_text_from_pdf(pdf_path)

# summarize the text
summary = summarize_pdf(text)

print(summary)
```

---

## scripts/additional-scripts/AH_news_summarizer.py

```python
#!/usr/bin/env python3
# /// script
# dependencies = ["requests", "beautifulsoup4", "anthropic", "python-dotenv", "feedparser", "newspaper3k"]
# ///

"""
AH_news_summarizer.py
Student: AH - "Collect news from internet and create summary for that"

This script demonstrates automated news collection from various sources,
content extraction, and AI-powered summarization for daily news briefings.

Key Learning Objectives:
- Web scraping and RSS feed parsing
- Content extraction and cleaning
- AI-powered text summarization
- Multi-source news aggregation
- Automated reporting and alerting
"""

import os
import re
import json
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
import anthropic

# News processing libraries
import feedparser
from bs4 import BeautifulSoup
from newspaper import Article

# Load environment variables
load_dotenv()

class NewsCollector:
    """
    A class to collect news from various sources including RSS feeds,
    news websites, and APIs, then summarize using AI.
    """

    def __init__(self):
        """Initialize the news collector with AI client and news sources."""
        # Initialize Anthropic client for AI-powered summarization
        self.ai_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # Define news sources (RSS feeds and websites)
        self.news_sources = {
            "technology": {
                "rss_feeds": [
                    "https://feeds.feedburner.com/oreilly/radar",
                    "https://techcrunch.com/feed/",
                    "https://www.wired.com/feed/rss",
                    "https://www.theverge.com/rss/index.xml"
                ],
                "websites": [
                    "https://news.ycombinator.com/",
                    "https://www.reddit.com/r/technology.json"
                ]
            },
            "business": {
                "rss_feeds": [
                    "https://feeds.bloomberg.com/markets/news.rss",
                    "https://www.reuters.com/technology/feed/",
                    "https://www.forbes.com/innovation/feed/"
                ]
            },
            "science": {
                "rss_feeds": [
                    "https://www.sciencedaily.com/rss/all.xml",
                    "https://www.nature.com/subjects/biological-sciences.rss"
                ]
            },
            "general": {
                "rss_feeds": [
                    "https://rss.cnn.com/rss/edition.rss",
                    "https://feeds.nbcnews.com/nbcnews/public/news"
                ]
            }
        }

        # News categories for classification
        self.categories = [
            "technology", "business", "science", "politics", "health",
            "entertainment", "sports", "world_news", "local_news"
        ]

        # Initialize session for HTTP requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def collect_rss_news(self, category: str, max_articles: int = 10) -> List[Dict[str, Any]]:
        """
        Collect news articles from RSS feeds for a specific category.

        Args:
            category: News category to collect
            max_articles: Maximum number of articles to collect per feed

        Returns:
            List of news article dictionaries
        """

        print(f"üì° Collecting RSS news for category: {category}")

        articles = []
        rss_feeds = self.news_sources.get(category, {}).get("rss_feeds", [])

        for feed_url in rss_feeds:
            try:
                print(f"  üì∞ Fetching from: {urlparse(feed_url).netloc}")

                # Parse RSS feed
                feed = feedparser.parse(feed_url)

                if feed.bozo:
                    print(f"    ‚ö†Ô∏è Warning: Feed may have issues")

                # Extract articles from feed
                for entry in feed.entries[:max_articles]:
                    article = {
                        "title": entry.get("title", "No title"),
                        "url": entry.get("link", ""),
                        "description": entry.get("description", ""),
                        "published": entry.get("published", ""),
                        "source": feed.feed.get("title", urlparse(feed_url).netloc),
                        "category": category,
                        "collection_method": "rss",
                        "content": "",  # Will be filled by content extraction
                        "summary": ""   # Will be filled by AI summarization
                    }

                    # Clean description from HTML tags
                    if article["description"]:
                        soup = BeautifulSoup(article["description"], 'html.parser')
                        article["description"] = soup.get_text().strip()

                    articles.append(article)

            except Exception as e:
                print(f"    ‚ùå Error fetching from {feed_url}: {str(e)}")

        print(f"  ‚úÖ Collected {len(articles)} articles from RSS feeds")
        return articles

    def extract_article_content(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract full content from a news article URL.

        Args:
            article: Article dictionary with URL

        Returns:
            Article dictionary with extracted content
        """

        try:
            # Use newspaper3k for content extraction
            news_article = Article(article["url"])
            news_article.download()
            news_article.parse()

            # Update article with extracted content
            article["content"] = news_article.text
            article["authors"] = news_article.authors
            article["publish_date"] = news_article.publish_date.isoformat() if news_article.publish_date else ""
            article["top_image"] = news_article.top_image
            article["keywords"] = news_article.keywords

            # Fallback to description if content is too short
            if len(article["content"]) < 100:
                article["content"] = article["description"]

        except Exception as e:
            print(f"    ‚ö†Ô∏è Content extraction failed for {article['title'][:50]}...: {str(e)}")
            # Use description as fallback content
            article["content"] = article["description"]

        return article

    def collect_from_website(self, website_url: str, category: str) -> List[Dict[str, Any]]:
        """
        Collect news from a specific website by scraping.

        Args:
            website_url: URL of the website to scrape
            category: Category to assign to collected articles

        Returns:
            List of news article dictionaries
        """

        articles = []

        try:
            print(f"  üåê Scraping website: {urlparse(website_url).netloc}")

            response = self.session.get(website_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Special handling for different websites
            if "news.ycombinator.com" in website_url:
                articles = self._scrape_hackernews(soup, category)
            elif "reddit.com" in website_url:
                articles = self._scrape_reddit_json(website_url, category)
            else:
                # Generic article extraction
                articles = self._scrape_generic_news(soup, website_url, category)

        except Exception as e:
            print(f"    ‚ùå Error scraping {website_url}: {str(e)}")

        return articles

    def _scrape_hackernews(self, soup: BeautifulSoup, category: str) -> List[Dict[str, Any]]:
        """Scrape articles from Hacker News."""
        articles = []

        # Find article titles and links
        title_links = soup.find_all('a', class_='storylink')[:10]  # Top 10 articles

        for link in title_links:
            article = {
                "title": link.get_text().strip(),
                "url": link.get('href', ''),
                "description": "",
                "published": datetime.now().isoformat(),
                "source": "Hacker News",
                "category": category,
                "collection_method": "web_scraping",
                "content": "",
                "summary": ""
            }

            # Make URL absolute if relative
            if article["url"].startswith('item?'):
                article["url"] = f"https://news.ycombinator.com/{article['url']}"
            elif not article["url"].startswith('http'):
                article["url"] = f"https://news.ycombinator.com/{article['url']}"

            articles.append(article)

        return articles

    def _scrape_reddit_json(self, url: str, category: str) -> List[Dict[str, Any]]:
        """Scrape articles from Reddit JSON API."""
        articles = []

        try:
            response = self.session.get(url, timeout=10)
            data = response.json()

            for post in data['data']['children'][:10]:  # Top 10 posts
                post_data = post['data']

                article = {
                    "title": post_data.get('title', ''),
                    "url": post_data.get('url', ''),
                    "description": post_data.get('selftext', '')[:500],  # Limit description
                    "published": datetime.fromtimestamp(post_data.get('created_utc', 0)).isoformat(),
                    "source": f"Reddit r/{post_data.get('subreddit', 'unknown')}",
                    "category": category,
                    "collection_method": "api",
                    "content": post_data.get('selftext', ''),
                    "summary": "",
                    "score": post_data.get('score', 0),
                    "comments": post_data.get('num_comments', 0)
                }

                articles.append(article)

        except Exception as e:
            print(f"    ‚ùå Error parsing Reddit JSON: {str(e)}")

        return articles

    def _scrape_generic_news(self, soup: BeautifulSoup, base_url: str, category: str) -> List[Dict[str, Any]]:
        """Generic news website scraping."""
        articles = []

        # Common selectors for news articles
        article_selectors = [
            'article h2 a', 'h2 a', 'h3 a', '.article-title a',
            '.headline a', '.story-headline a', '.entry-title a'
        ]

        for selector in article_selectors:
            links = soup.select(selector)[:10]  # Limit to 10 articles

            for link in links:
                href = link.get('href', '')
                if href:
                    # Make URL absolute
                    full_url = urljoin(base_url, href)

                    article = {
                        "title": link.get_text().strip(),
                        "url": full_url,
                        "description": "",
                        "published": datetime.now().isoformat(),
                        "source": urlparse(base_url).netloc,
                        "category": category,
                        "collection_method": "web_scraping",
                        "content": "",
                        "summary": ""
                    }

                    articles.append(article)

            if articles:  # If we found articles, no need to try other selectors
                break

        return articles

    def summarize_articles_with_ai(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Use AI to summarize individual articles and create topic summaries.

        Args:
            articles: List of articles to summarize

        Returns:
            List of articles with AI-generated summaries
        """

        print("ü§ñ Generating AI summaries for articles...")

        summarized_articles = []

        for i, article in enumerate(articles):
            try:
                print(f"  üìù Summarizing article {i+1}/{len(articles)}: {article['title'][:50]}...")

                # Prepare content for summarization
                content_to_summarize = article["content"] or article["description"]

                if len(content_to_summarize) < 50:
                    # Skip very short articles
                    article["summary"] = "Article content too short for meaningful summary."
                    article["key_points"] = []
                    article["sentiment"] = "neutral"
                    summarized_articles.append(article)
                    continue

                # Create AI prompt for article summarization
                prompt = f"""
                Summarize this news article and provide key insights:

                Title: {article['title']}
                Source: {article['source']}
                Content: {content_to_summarize[:2000]}  # Limit content length

                Provide:
                1. A concise 2-3 sentence summary
                2. 3-5 key points or takeaways
                3. Overall sentiment (positive, negative, neutral)
                4. Relevance category from: {', '.join(self.categories)}

                Respond in JSON format:
                {{
                    "summary": "concise summary",
                    "key_points": ["point1", "point2", "point3"],
                    "sentiment": "positive/negative/neutral",
                    "category": "category",
                    "importance_score": 1-10
                }}
                """

                # Make API call to Anthropic
                response = self.ai_client.messages.create(
                    model="claude-3-sonnet-20240229",
                    max_tokens=500,
                    messages=[{"role": "user", "content": prompt}]
                )

                # Parse AI response
                ai_summary = json.loads(response.content[0].text)

                # Update article with AI-generated content
                article["summary"] = ai_summary.get("summary", "Summary not available")
                article["key_points"] = ai_summary.get("key_points", [])
                article["sentiment"] = ai_summary.get("sentiment", "neutral")
                article["ai_category"] = ai_summary.get("category", article["category"])
                article["importance_score"] = ai_summary.get("importance_score", 5)

            except Exception as e:
                print(f"    ‚ùå Summarization failed for {article['title'][:50]}...: {str(e)}")
                # Fallback summary
                article["summary"] = "AI summarization unavailable. " + article["description"][:200]
                article["key_points"] = ["Content analysis pending"]
                article["sentiment"] = "neutral"
                article["importance_score"] = 5

            summarized_articles.append(article)

        print(f"  ‚úÖ Completed summarization for {len(summarized_articles)} articles")
        return summarized_articles

    def create_daily_briefing(self, articles: List[Dict[str, Any]]) -> str:
        """
        Create a comprehensive daily news briefing from collected articles.

        Args:
            articles: List of summarized articles

        Returns:
            Formatted daily briefing text
        """

        print("üìä Creating daily news briefing...")

        # Sort articles by importance score
        sorted_articles = sorted(articles, key=lambda x: x.get("importance_score", 5), reverse=True)

        # Group articles by category
        articles_by_category = {}
        for article in sorted_articles:
            category = article.get("ai_category", article["category"])
            if category not in articles_by_category:
                articles_by_category[category] = []
            articles_by_category[category].append(article)

        # Generate briefing
        briefing = []
        briefing.append("=" * 80)
        briefing.append("üì∞ DAILY NEWS BRIEFING")
        briefing.append("=" * 80)
        briefing.append(f"Generated: {datetime.now().strftime('%A, %B %d, %Y at %I:%M %p')}")
        briefing.append(f"Total articles analyzed: {len(articles)}")
        briefing.append("")

        # Executive summary
        briefing.append("üéØ EXECUTIVE SUMMARY:")
        briefing.append("-" * 30)

        # Get top 3 most important articles
        top_articles = sorted_articles[:3]
        for i, article in enumerate(top_articles, 1):
            briefing.append(f"{i}. {article['title']}")
            briefing.append(f"   {article['summary']}")
            briefing.append(f"   Source: {article['source']} | Importance: {article.get('importance_score', 5)}/10")
            briefing.append("")

        # Category breakdown
        briefing.append("üìÇ NEWS BY CATEGORY:")
        briefing.append("-" * 25)

        for category, cat_articles in articles_by_category.items():
            if not cat_articles:
                continue

            briefing.append(f"\nüî∏ {category.upper().replace('_', ' ')} ({len(cat_articles)} articles)")
            briefing.append("-" * 40)

            # Show top 3 articles per category
            for article in cat_articles[:3]:
                briefing.append(f"‚Ä¢ {article['title']}")
                briefing.append(f"  {article['summary']}")
                briefing.append(f"  Source: {article['source']} | Sentiment: {article.get('sentiment', 'neutral')}")

                # Show key points
                key_points = article.get("key_points", [])
                if key_points:
                    briefing.append("  Key Points:")
                    for point in key_points[:2]:  # Show top 2 points
                        briefing.append(f"    - {point}")
                briefing.append("")

        # Sentiment analysis
        sentiments = [article.get("sentiment", "neutral") for article in articles]
        sentiment_counts = {
            "positive": sentiments.count("positive"),
            "negative": sentiments.count("negative"),
            "neutral": sentiments.count("neutral")
        }

        briefing.append("üìà SENTIMENT ANALYSIS:")
        briefing.append("-" * 25)
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(articles)) * 100 if articles else 0
            briefing.append(f"{sentiment.capitalize()}: {count} articles ({percentage:.1f}%)")
        briefing.append("")

        # Sources summary
        sources = [article["source"] for article in articles]
        source_counts = {}
        for source in sources:
            source_counts[source] = source_counts.get(source, 0) + 1

        briefing.append("üì° TOP NEWS SOURCES:")
        briefing.append("-" * 25)
        sorted_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)
        for source, count in sorted_sources[:5]:
            briefing.append(f"{source}: {count} articles")
        briefing.append("")

        # Trending topics (most common keywords)
        all_keywords = []
        for article in articles:
            keywords = article.get("keywords", [])
            if isinstance(keywords, list):
                all_keywords.extend(keywords)

        if all_keywords:
            keyword_counts = {}
            for keyword in all_keywords:
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

            briefing.append("üî• TRENDING TOPICS:")
            briefing.append("-" * 20)
            sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
            for keyword, count in sorted_keywords[:10]:
                briefing.append(f"{keyword}: {count} mentions")
            briefing.append("")

        # Action items and recommendations
        briefing.append("üìã RECOMMENDED ACTIONS:")
        briefing.append("-" * 30)
        briefing.append("‚Ä¢ Monitor high-importance articles for follow-up")
        briefing.append("‚Ä¢ Track sentiment trends in your industry")
        briefing.append("‚Ä¢ Review negative sentiment articles for potential impact")
        briefing.append("‚Ä¢ Consider amplifying positive news related to your sector")
        briefing.append("")

        briefing.append("üîó This briefing was generated automatically using AI-powered news analysis.")

        return "\n".join(briefing)

    def collect_all_news(self, categories: List[str] = None) -> List[Dict[str, Any]]:
        """
        Collect news from all configured sources across specified categories.

        Args:
            categories: List of categories to collect (default: all)

        Returns:
            List of all collected articles
        """

        if categories is None:
            categories = list(self.news_sources.keys())

        all_articles = []

        print("üöÄ Starting comprehensive news collection...")

        for category in categories:
            print(f"\nüìÇ Processing category: {category}")

            # Collect from RSS feeds
            rss_articles = self.collect_rss_news(category, max_articles=5)
            all_articles.extend(rss_articles)

            # Collect from websites
            websites = self.news_sources.get(category, {}).get("websites", [])
            for website in websites:
                try:
                    website_articles = self.collect_from_website(website, category)
                    all_articles.extend(website_articles)
                except Exception as e:
                    print(f"    ‚ùå Website collection failed: {str(e)}")

        print(f"\nüìä Total articles collected: {len(all_articles)}")

        # Extract content for articles that don't have it
        print("üìÑ Extracting article content...")
        for i, article in enumerate(all_articles):
            if not article.get("content") and article.get("url"):
                print(f"  üìù Extracting content {i+1}/{len(all_articles)}")
                article = self.extract_article_content(article)

        return all_articles

def main():
    """
    Main function to demonstrate news collection and summarization.
    """

    print("üöÄ News Summarizer Demo")
    print("=" * 40)

    # Initialize the news collector
    collector = NewsCollector()

    # Collect news from multiple sources
    print("üì° Collecting news from various sources...")
    all_articles = collector.collect_all_news(categories=["technology", "business"])

    if not all_articles:
        print("‚ùå No articles collected. Check your internet connection and news sources.")
        return

    print(f"‚úÖ Collected {len(all_articles)} articles")

    # Generate AI summaries
    print("\nü§ñ Generating AI-powered summaries...")
    summarized_articles = collector.summarize_articles_with_ai(all_articles[:10])  # Limit for demo

    # Create daily briefing
    print("\nüìä Creating daily news briefing...")
    briefing = collector.create_daily_briefing(summarized_articles)

    # Display briefing
    print("\n" + briefing)

    # Save results
    output_file = f"news_briefing_{datetime.now().strftime('%Y%m%d')}.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(briefing)

    # Save raw data
    json_file = f"news_data_{datetime.now().strftime('%Y%m%d')}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(summarized_articles, f, indent=2, default=str, ensure_ascii=False)

    print(f"\nüíæ Briefing saved to: {output_file}")
    print(f"üíæ Raw data saved to: {json_file}")

    # Demonstrate integration possibilities
    print("\nüîó INTEGRATION POSSIBILITIES:")
    print("- Scheduled daily/hourly news collection")
    print("- Email/Slack notifications for breaking news")
    print("- RSS feed generation for custom topics")
    print("- Integration with content management systems")
    print("- Social media monitoring and analysis")
    print("- Competitor news tracking")
    print("- Industry-specific news filtering")
    print("- Multi-language news collection and translation")
    print("- Real-time news alerts based on keywords")
    print("- News trend analysis and reporting")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/BD_timesheet_automation.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "playwright",
#     "pandas",
#     "python-dotenv",
# ]
# ///

"""
BD_timesheet_automation.py - Appointment Booking Timesheet Filling

Student: BD (Initials)
Request: "appointment booking timesheet filling"

This script demonstrates how to automate timesheet filling using Playwright for web automation.
It creates a realistic example that fills out a timesheet form with appointment data.

Educational Focus:
- Web automation with Playwright
- Form filling automation
- Data handling for timesheet entries
- Error handling and waiting strategies
- Creating reusable automation functions

Prerequisites:
- Run: playwright install
- Ensure you have a timesheet web form to test with
"""

import asyncio
import pandas as pd
from datetime import datetime, timedelta
from playwright.async_api import async_playwright, Page
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class TimesheetAutomator:
    """
    A class to automate timesheet filling using Playwright.

    This demonstrates how to structure web automation code for maintainability
    and reusability across different timesheet systems.
    """

    def __init__(self, headless: bool = False):
        """
        Initialize the timesheet automator.

        Args:
            headless (bool): Whether to run browser in headless mode
        """
        self.headless = headless
        self.browser = None
        self.page = None

    async def start_browser(self):
        """
        Start the browser and create a new page.

        This method initializes Playwright and opens a browser instance.
        Using async/await allows for better performance with multiple operations.
        """
        print("üöÄ Starting browser for timesheet automation...")

        # Initialize Playwright
        self.playwright = await async_playwright().start()

        # Launch browser (Chromium by default)
        # You can also use firefox or webkit
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            slow_mo=1000  # Add 1 second delay between actions for demo purposes
        )

        # Create a new browser context (isolated session)
        self.context = await self.browser.new_context()

        # Create a new page
        self.page = await self.context.new_page()

        print("‚úÖ Browser started successfully")

    async def close_browser(self):
        """
        Clean up browser resources.

        Always important to close browser instances to free up system resources.
        """
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
        print("üîí Browser closed")

    def generate_sample_timesheet_data(self, num_days: int = 5) -> pd.DataFrame:
        """
        Generate sample timesheet data for demonstration.

        In a real scenario, this data might come from:
        - Calendar API (Google Calendar, Outlook)
        - CRM system
        - Database of appointments
        - CSV file exports

        Args:
            num_days (int): Number of days to generate data for

        Returns:
            pd.DataFrame: Sample timesheet data
        """
        print(f"üìä Generating sample timesheet data for {num_days} days...")

        # Sample appointment types and clients
        appointment_types = [
            "Client Meeting",
            "Project Planning",
            "Code Review",
            "Training Session",
            "Documentation",
            "System Maintenance"
        ]

        clients = [
            "TechCorp Inc.",
            "StartupXYZ",
            "Enterprise Solutions",
            "Digital Innovations",
            "Cloud Systems Ltd."
        ]

        # Generate data for the past week
        data = []
        start_date = datetime.now() - timedelta(days=num_days-1)

        for day in range(num_days):
            current_date = start_date + timedelta(days=day)

            # Skip weekends for this example
            if current_date.weekday() >= 5:  # Saturday = 5, Sunday = 6
                continue

            # Generate 2-4 appointments per day
            import random
            num_appointments = random.randint(2, 4)

            for _ in range(num_appointments):
                # Random start time between 9 AM and 5 PM
                start_hour = random.randint(9, 16)
                start_minute = random.choice([0, 15, 30, 45])

                # Duration between 1-3 hours
                duration_hours = random.uniform(1.0, 3.0)

                appointment = {
                    'date': current_date.strftime('%Y-%m-%d'),
                    'start_time': f"{start_hour:02d}:{start_minute:02d}",
                    'duration_hours': round(duration_hours, 2),
                    'appointment_type': random.choice(appointment_types),
                    'client': random.choice(clients),
                    'description': f"Working on {random.choice(['analysis', 'development', 'consultation', 'planning'])} tasks"
                }
                data.append(appointment)

        df = pd.DataFrame(data)
        print(f"‚úÖ Generated {len(df)} timesheet entries")
        return df

    async def create_demo_timesheet_page(self):
        """
        Create a demo HTML timesheet form for testing purposes.

        In a real scenario, you would navigate to your actual timesheet system.
        This creates a realistic form to demonstrate the automation concepts.
        """
        print("üé≠ Creating demo timesheet page...")

        # HTML for a realistic timesheet form
        html_content = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Employee Timesheet System</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }
                .container { max-width: 800px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                .header { text-align: center; color: #333; margin-bottom: 30px; }
                .form-group { margin-bottom: 15px; }
                label { display: block; font-weight: bold; margin-bottom: 5px; color: #555; }
                input, select, textarea { width: 100%; padding: 8px; border: 1px solid #ddd; border-radius: 4px; font-size: 14px; }
                button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; font-size: 16px; }
                button:hover { background: #0056b3; }
                .entry { border: 1px solid #eee; padding: 15px; margin-bottom: 10px; border-radius: 4px; background: #fafafa; }
                .success { color: green; font-weight: bold; margin-top: 20px; text-align: center; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1 class="header">üìÖ Employee Timesheet Entry System</h1>

                <div class="form-group">
                    <label for="employee-name">Employee Name:</label>
                    <input type="text" id="employee-name" name="employee-name" placeholder="Enter your full name">
                </div>

                <div class="form-group">
                    <label for="week-ending">Week Ending Date:</label>
                    <input type="date" id="week-ending" name="week-ending">
                </div>

                <h3>‚è∞ Time Entries</h3>
                <div id="time-entries">
                    <!-- Time entries will be added here -->
                </div>

                <button type="button" onclick="addTimeEntry()">+ Add Time Entry</button>
                <button type="button" onclick="submitTimesheet()" style="margin-left: 10px; background: #28a745;">Submit Timesheet</button>

                <div id="success-message" class="success" style="display: none;">
                    ‚úÖ Timesheet submitted successfully!
                </div>
            </div>

            <script>
                let entryCount = 0;

                function addTimeEntry() {
                    entryCount++;
                    const container = document.getElementById('time-entries');
                    const entry = document.createElement('div');
                    entry.className = 'entry';
                    entry.innerHTML = `
                        <h4>Entry ${entryCount}</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;">
                            <div>
                                <label>Date:</label>
                                <input type="date" name="entry-date-${entryCount}" required>
                            </div>
                            <div>
                                <label>Start Time:</label>
                                <input type="time" name="entry-start-${entryCount}" required>
                            </div>
                            <div>
                                <label>Hours:</label>
                                <input type="number" name="entry-hours-${entryCount}" step="0.25" min="0" max="24" required>
                            </div>
                        </div>
                        <div style="margin-top: 10px;">
                            <label>Project/Client:</label>
                            <input type="text" name="entry-client-${entryCount}" placeholder="Client or project name" required>
                        </div>
                        <div style="margin-top: 10px;">
                            <label>Description:</label>
                            <textarea name="entry-description-${entryCount}" rows="2" placeholder="Brief description of work performed"></textarea>
                        </div>
                    `;
                    container.appendChild(entry);
                }

                function submitTimesheet() {
                    // Simple validation
                    const employeeName = document.getElementById('employee-name').value;
                    const weekEnding = document.getElementById('week-ending').value;

                    if (!employeeName || !weekEnding) {
                        alert('Please fill in employee name and week ending date');
                        return;
                    }

                    if (entryCount === 0) {
                        alert('Please add at least one time entry');
                        return;
                    }

                    // Show success message
                    document.getElementById('success-message').style.display = 'block';

                    // In a real application, this would submit to a server
                    console.log('Timesheet submitted successfully');
                }

                // Initialize with one empty entry
                addTimeEntry();
            </script>
        </body>
        </html>
        '''

        # Set the HTML content directly to the page
        await self.page.set_content(html_content)
        print("‚úÖ Demo timesheet page created")

    async def fill_employee_info(self, employee_name: str, week_ending: str):
        """
        Fill in the employee information section.

        Args:
            employee_name (str): Name of the employee
            week_ending (str): Week ending date in YYYY-MM-DD format
        """
        print(f"üë§ Filling employee info: {employee_name}, Week ending: {week_ending}")

        # Fill employee name
        await self.page.fill('#employee-name', employee_name)

        # Fill week ending date
        await self.page.fill('#week-ending', week_ending)

        # Wait a moment for the form to update
        await self.page.wait_for_timeout(500)

    async def add_time_entry(self, entry_data: dict, entry_number: int):
        """
        Add a single time entry to the timesheet.

        Args:
            entry_data (dict): Dictionary containing entry information
            entry_number (int): The entry number (for form field identification)
        """
        print(f"‚è∞ Adding time entry {entry_number}: {entry_data['appointment_type']} for {entry_data['client']}")

        # If this is not the first entry, click "Add Time Entry" button
        if entry_number > 1:
            await self.page.click('button:has-text("+ Add Time Entry")')
            await self.page.wait_for_timeout(500)  # Wait for new entry to be added

        # Fill in the time entry fields
        # Note: The entry number in the form starts from 1
        form_entry_num = entry_number

        # Fill date
        await self.page.fill(f'input[name="entry-date-{form_entry_num}"]', entry_data['date'])

        # Fill start time
        await self.page.fill(f'input[name="entry-start-{form_entry_num}"]', entry_data['start_time'])

        # Fill hours
        await self.page.fill(f'input[name="entry-hours-{form_entry_num}"]', str(entry_data['duration_hours']))

        # Fill client/project
        await self.page.fill(f'input[name="entry-client-{form_entry_num}"]', entry_data['client'])

        # Fill description
        description = f"{entry_data['appointment_type']}: {entry_data['description']}"
        await self.page.fill(f'textarea[name="entry-description-{form_entry_num}"]', description)

        # Wait a moment for the form to update
        await self.page.wait_for_timeout(300)

    async def submit_timesheet(self):
        """
        Submit the completed timesheet.

        This method handles the final submission and verifies success.
        """
        print("üì§ Submitting timesheet...")

        # Click the submit button
        await self.page.click('button:has-text("Submit Timesheet")')

        # Wait for success message to appear
        await self.page.wait_for_selector('#success-message', state='visible', timeout=5000)

        # Verify submission was successful
        success_message = await self.page.text_content('#success-message')
        if "successfully" in success_message.lower():
            print("‚úÖ Timesheet submitted successfully!")
            return True
        else:
            print("‚ùå Timesheet submission may have failed")
            return False

    async def automate_timesheet_filling(self, timesheet_data: pd.DataFrame, employee_name: str = "John Doe"):
        """
        Main automation method that orchestrates the entire timesheet filling process.

        Args:
            timesheet_data (pd.DataFrame): DataFrame containing timesheet entries
            employee_name (str): Name of the employee
        """
        print("ü§ñ Starting timesheet automation process...")

        try:
            # Start the browser
            await self.start_browser()

            # Create demo page (in real scenario, navigate to actual timesheet URL)
            await self.create_demo_timesheet_page()

            # Calculate week ending date (assuming current week)
            from datetime import datetime, timedelta
            today = datetime.now()
            days_until_friday = (4 - today.weekday()) % 7  # Friday is day 4
            if days_until_friday == 0 and today.weekday() != 4:  # If today is not Friday
                days_until_friday = 7  # Next Friday
            week_ending = (today + timedelta(days=days_until_friday)).strftime('%Y-%m-%d')

            # Fill employee information
            await self.fill_employee_info(employee_name, week_ending)

            # Process each time entry
            for index, entry in timesheet_data.iterrows():
                await self.add_time_entry(entry.to_dict(), index + 1)

            # Wait a moment before submitting
            await self.page.wait_for_timeout(1000)

            # Submit the timesheet
            success = await self.submit_timesheet()

            if success:
                print(f"üéâ Successfully automated timesheet with {len(timesheet_data)} entries!")

            # Wait a moment to see the result
            await self.page.wait_for_timeout(2000)

        except Exception as e:
            print(f"‚ùå Error during automation: {str(e)}")
            raise
        finally:
            # Always close the browser
            await self.close_browser()

async def main():
    """
    Main function demonstrating the timesheet automation workflow.

    This function shows how to:
    1. Create sample data
    2. Initialize the automator
    3. Run the automation
    4. Handle errors appropriately
    """
    print("üöÄ Welcome to BD's Timesheet Automation Demo!")
    print("=" * 50)

    try:
        # Create the timesheet automator
        # Set headless=False to watch the automation in action
        automator = TimesheetAutomator(headless=False)

        # Generate sample timesheet data
        # In a real scenario, this might come from your calendar API or database
        timesheet_data = automator.generate_sample_timesheet_data(num_days=5)

        print("\nüìä Sample timesheet data:")
        print(timesheet_data.to_string(index=False))
        print()

        # Run the automation
        await automator.automate_timesheet_filling(
            timesheet_data=timesheet_data,
            employee_name="BD (Student)"  # Using student's initials
        )

        print("\n‚úÖ Timesheet automation completed successfully!")

    except Exception as e:
        print(f"\n‚ùå Automation failed: {str(e)}")
        print("üí° Tips for troubleshooting:")
        print("   - Ensure Playwright is installed: playwright install")
        print("   - Check if the timesheet website is accessible")
        print("   - Verify selectors match the actual form elements")
        print("   - Consider adding more wait times for slow-loading pages")

def create_real_world_integration_examples():
    """
    Function demonstrating how to integrate with real-world systems.

    This shows concepts for connecting to actual data sources and timesheet systems.
    """
    print("\nüîó Real-world Integration Examples:")
    print("=" * 40)

    print("""
    1. Calendar Integration:
       - Google Calendar API: Extract appointments
       - Outlook Calendar API: Sync meeting data
       - CalDAV: Connect to various calendar systems

    2. Data Sources:
       - CRM Systems: Salesforce, HubSpot APIs
       - Project Management: Jira, Asana, Trello APIs
       - CSV/Excel exports from existing systems

    3. Timesheet Systems:
       - Modify selectors for your specific timesheet platform
       - Add authentication handling (login flows)
       - Handle different form layouts and validation rules

    4. Error Handling:
       - Implement retry logic for network issues
       - Add screenshot capture on failures
       - Log all actions for debugging

    5. Scheduling:
       - Use cron jobs or Task Scheduler for regular automation
       - Set up monitoring and alerting for failures
       - Implement backup and recovery procedures
    """)

if __name__ == "__main__":
    # Run the main automation demo
    asyncio.run(main())

    # Show additional integration examples
    create_real_world_integration_examples()

    print("\nüéì Learning Summary:")
    print("- Web automation with Playwright")
    print("- Asynchronous programming for better performance")
    print("- Data handling with Pandas")
    print("- Error handling and resource cleanup")
    print("- Structuring automation code for maintainability")
    print("\nüí° Next Steps:")
    print("- Adapt selectors for your actual timesheet system")
    print("- Add authentication and login handling")
    print("- Integrate with your real data sources")
    print("- Set up scheduling for regular automation")
```

---

## scripts/additional-scripts/JG_api_data_storage.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "sqlite3",
#     "pandas",
#     "python-dotenv",
#     "beautifulsoup4",
# ]
# ///

"""
JG_api_data_storage.py - Access API for Website and Store Data in Database

Student: JG (Initials)
Request: "access API for website and store data in database"

This script demonstrates how to fetch data from public APIs and store it in a SQLite database.
It includes comprehensive error handling, data validation, and database management.

Educational Focus:
- Making HTTP requests to APIs
- JSON data processing
- SQLite database operations
- Data validation and cleaning
- Error handling and retry logic
- Database schema design
- Working with different API types

Prerequisites:
- No API keys required (uses public APIs)
- SQLite is included with Python
"""

import requests
import sqlite3
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
from dataclasses import dataclass
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Set up logging for better debugging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('api_data_storage.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class APIEndpoint:
    """
    Data class to represent an API endpoint configuration.

    This makes it easy to manage multiple APIs and their settings.
    """
    name: str
    url: str
    headers: Dict[str, str]
    rate_limit_delay: float = 1.0  # Seconds between requests
    description: str = ""

class APIDataFetcher:
    """
    A comprehensive class for fetching data from various APIs and storing in database.

    This class demonstrates professional API integration patterns including:
    - Rate limiting
    - Error handling and retries
    - Response validation
    - Data transformation
    """

    def __init__(self, db_path: str = "api_data.db"):
        """
        Initialize the API data fetcher.

        Args:
            db_path (str): Path to SQLite database file
        """
        self.db_path = db_path
        self.session = requests.Session()

        # Set a user agent to be a good API citizen
        self.session.headers.update({
            'User-Agent': 'Educational-API-Demo/1.0 (Python Training)'
        })

        # Initialize database
        self.init_database()

        # Define our API endpoints for demonstration
        self.api_endpoints = {
            'weather': APIEndpoint(
                name='OpenWeatherMap Demo',
                url='https://api.openweathermap.org/data/2.5/weather',
                headers={},
                rate_limit_delay=1.0,
                description='Weather data for cities'
            ),
            'jsonplaceholder': APIEndpoint(
                name='JSONPlaceholder Posts',
                url='https://jsonplaceholder.typicode.com/posts',
                headers={},
                rate_limit_delay=0.5,
                description='Sample blog posts data'
            ),
            'httpbin': APIEndpoint(
                name='HTTPBin IP Info',
                url='https://httpbin.org/ip',
                headers={},
                rate_limit_delay=0.5,
                description='IP address information'
            ),
            'random_user': APIEndpoint(
                name='Random User Generator',
                url='https://randomuser.me/api/',
                headers={},
                rate_limit_delay=1.0,
                description='Random user profiles'
            )
        }

    def init_database(self):
        """
        Initialize the SQLite database with tables for different data types.

        This creates a flexible schema that can store various types of API data.
        """
        logger.info(f"üóÑÔ∏è Initializing database: {self.db_path}")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Table for API fetch logs
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS api_fetch_logs (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        api_name TEXT NOT NULL,
                        endpoint_url TEXT NOT NULL,
                        status_code INTEGER,
                        response_size INTEGER,
                        fetch_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        success BOOLEAN NOT NULL,
                        error_message TEXT
                    )
                ''')

                # Table for weather data
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS weather_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        city_name TEXT NOT NULL,
                        country TEXT,
                        temperature REAL,
                        feels_like REAL,
                        humidity INTEGER,
                        pressure INTEGER,
                        weather_main TEXT,
                        weather_description TEXT,
                        wind_speed REAL,
                        wind_direction INTEGER,
                        cloudiness INTEGER,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        raw_data TEXT
                    )
                ''')

                # Table for generic JSON data (flexible storage)
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS json_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        source_api TEXT NOT NULL,
                        data_type TEXT NOT NULL,
                        json_content TEXT NOT NULL,
                        extracted_fields TEXT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Table for user profiles (from random user API)
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS user_profiles (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        first_name TEXT,
                        last_name TEXT,
                        email TEXT,
                        phone TEXT,
                        country TEXT,
                        city TEXT,
                        age INTEGER,
                        gender TEXT,
                        profile_picture_url TEXT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                conn.commit()
                logger.info("‚úÖ Database tables created successfully")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Database initialization error: {e}")
            raise

    def make_api_request(self, endpoint: APIEndpoint, params: Dict = None, retries: int = 3) -> Optional[Dict]:
        """
        Make a request to an API endpoint with proper error handling and retries.

        Args:
            endpoint (APIEndpoint): The API endpoint configuration
            params (Dict): Query parameters for the request
            retries (int): Number of retry attempts

        Returns:
            Optional[Dict]: JSON response data or None if failed
        """
        logger.info(f"üåê Making request to {endpoint.name}: {endpoint.url}")

        for attempt in range(retries + 1):
            try:
                # Respect rate limiting
                if attempt > 0:
                    time.sleep(endpoint.rate_limit_delay * attempt)  # Exponential backoff

                # Make the request
                response = self.session.get(
                    endpoint.url,
                    params=params,
                    headers=endpoint.headers,
                    timeout=10
                )

                # Log the request
                self.log_api_request(
                    endpoint.name,
                    endpoint.url,
                    response.status_code,
                    len(response.content) if response.content else 0,
                    response.status_code == 200,
                    None if response.status_code == 200 else f"HTTP {response.status_code}"
                )

                # Check if request was successful
                response.raise_for_status()

                # Return JSON data
                return response.json()

            except requests.exceptions.RequestException as e:
                logger.warning(f"‚ö†Ô∏è Request attempt {attempt + 1} failed: {e}")

                # Log failed request
                self.log_api_request(
                    endpoint.name,
                    endpoint.url,
                    getattr(e.response, 'status_code', 0) if hasattr(e, 'response') and e.response else 0,
                    0,
                    False,
                    str(e)
                )

                if attempt == retries:
                    logger.error(f"‚ùå All {retries + 1} attempts failed for {endpoint.name}")
                    return None

            except json.JSONDecodeError as e:
                logger.error(f"‚ùå JSON decode error: {e}")
                return None

        # Rate limiting between requests
        time.sleep(endpoint.rate_limit_delay)
        return None

    def log_api_request(self, api_name: str, url: str, status_code: int,
                       response_size: int, success: bool, error_message: str = None):
        """
        Log API request details to the database.

        Args:
            api_name (str): Name of the API
            url (str): Request URL
            status_code (int): HTTP status code
            response_size (int): Size of response in bytes
            success (bool): Whether request was successful
            error_message (str): Error message if any
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO api_fetch_logs
                    (api_name, endpoint_url, status_code, response_size, success, error_message)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (api_name, url, status_code, response_size, success, error_message))
                conn.commit()
        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to log API request: {e}")

    def fetch_and_store_weather_data(self, cities: List[str] = None):
        """
        Fetch weather data for specified cities and store in database.

        Note: This uses a demo API that might require an API key for full functionality.
        For educational purposes, we'll use a mock response structure.

        Args:
            cities (List[str]): List of city names to fetch weather for
        """
        if cities is None:
            cities = ["London", "New York", "Tokyo", "Sydney", "Paris"]

        logger.info(f"üå§Ô∏è Fetching weather data for {len(cities)} cities")

        # For demonstration, we'll create realistic sample data
        # In a real implementation, you'd use the actual OpenWeatherMap API
        import random

        for city in cities:
            # Simulate realistic weather data
            weather_data = self.generate_sample_weather_data(city)

            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO weather_data
                        (city_name, country, temperature, feels_like, humidity, pressure,
                         weather_main, weather_description, wind_speed, wind_direction,
                         cloudiness, raw_data)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        weather_data['city_name'],
                        weather_data['country'],
                        weather_data['temperature'],
                        weather_data['feels_like'],
                        weather_data['humidity'],
                        weather_data['pressure'],
                        weather_data['weather_main'],
                        weather_data['weather_description'],
                        weather_data['wind_speed'],
                        weather_data['wind_direction'],
                        weather_data['cloudiness'],
                        json.dumps(weather_data)
                    ))
                    conn.commit()

                logger.info(f"‚úÖ Stored weather data for {city}")

            except sqlite3.Error as e:
                logger.error(f"‚ùå Failed to store weather data for {city}: {e}")

            # Rate limiting
            time.sleep(0.5)

    def generate_sample_weather_data(self, city: str) -> Dict:
        """
        Generate realistic sample weather data for demonstration.

        In a real implementation, this would be the actual API response.
        """
        import random

        weather_conditions = [
            ("Clear", "clear sky"),
            ("Clouds", "few clouds"),
            ("Clouds", "scattered clouds"),
            ("Clouds", "overcast clouds"),
            ("Rain", "light rain"),
            ("Rain", "moderate rain"),
            ("Snow", "light snow"),
            ("Mist", "mist")
        ]

        countries = {
            "London": "GB",
            "New York": "US",
            "Tokyo": "JP",
            "Sydney": "AU",
            "Paris": "FR"
        }

        main_weather, description = random.choice(weather_conditions)
        base_temp = random.uniform(-10, 35)  # Celsius

        return {
            'city_name': city,
            'country': countries.get(city, "XX"),
            'temperature': round(base_temp, 1),
            'feels_like': round(base_temp + random.uniform(-3, 3), 1),
            'humidity': random.randint(30, 90),
            'pressure': random.randint(980, 1030),
            'weather_main': main_weather,
            'weather_description': description,
            'wind_speed': round(random.uniform(0, 15), 1),
            'wind_direction': random.randint(0, 360),
            'cloudiness': random.randint(0, 100)
        }

    def fetch_and_store_posts_data(self):
        """
        Fetch sample posts from JSONPlaceholder API and store in database.

        This demonstrates working with a real, free API.
        """
        logger.info("üìù Fetching posts data from JSONPlaceholder API")

        endpoint = self.api_endpoints['jsonplaceholder']
        data = self.make_api_request(endpoint)

        if data:
            try:
                # Store each post in the json_data table
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()

                    for post in data[:10]:  # Limit to first 10 posts for demo
                        # Extract key fields for easier querying
                        extracted_fields = {
                            'title': post.get('title', ''),
                            'user_id': post.get('userId', ''),
                            'post_id': post.get('id', '')
                        }

                        cursor.execute('''
                            INSERT INTO json_data
                            (source_api, data_type, json_content, extracted_fields)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            'jsonplaceholder',
                            'blog_post',
                            json.dumps(post),
                            json.dumps(extracted_fields)
                        ))

                    conn.commit()
                    logger.info(f"‚úÖ Stored {len(data[:10])} blog posts")

            except sqlite3.Error as e:
                logger.error(f"‚ùå Failed to store posts data: {e}")

    def fetch_and_store_user_profiles(self, count: int = 5):
        """
        Fetch random user profiles and store in database.

        Args:
            count (int): Number of user profiles to fetch
        """
        logger.info(f"üë• Fetching {count} random user profiles")

        endpoint = self.api_endpoints['random_user']

        for i in range(count):
            params = {'results': 1}  # Fetch one user at a time
            data = self.make_api_request(endpoint, params)

            if data and 'results' in data and len(data['results']) > 0:
                user = data['results'][0]

                try:
                    with sqlite3.connect(self.db_path) as conn:
                        cursor = conn.cursor()
                        cursor.execute('''
                            INSERT INTO user_profiles
                            (first_name, last_name, email, phone, country, city,
                             age, gender, profile_picture_url)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            user['name']['first'],
                            user['name']['last'],
                            user['email'],
                            user['phone'],
                            user['location']['country'],
                            user['location']['city'],
                            user['dob']['age'],
                            user['gender'],
                            user['picture']['large']
                        ))
                        conn.commit()

                    logger.info(f"‚úÖ Stored user profile {i+1}: {user['name']['first']} {user['name']['last']}")

                except sqlite3.Error as e:
                    logger.error(f"‚ùå Failed to store user profile {i+1}: {e}")

            # Rate limiting
            time.sleep(1)

    def fetch_ip_info(self):
        """
        Fetch IP information from HTTPBin and store in database.

        This demonstrates working with simple API responses.
        """
        logger.info("üåê Fetching IP information")

        endpoint = self.api_endpoints['httpbin']
        data = self.make_api_request(endpoint)

        if data:
            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO json_data
                        (source_api, data_type, json_content, extracted_fields)
                        VALUES (?, ?, ?, ?)
                    ''', (
                        'httpbin',
                        'ip_info',
                        json.dumps(data),
                        json.dumps({'origin_ip': data.get('origin', '')})
                    ))
                    conn.commit()

                logger.info(f"‚úÖ Stored IP information: {data.get('origin', 'Unknown')}")

            except sqlite3.Error as e:
                logger.error(f"‚ùå Failed to store IP information: {e}")

    def analyze_stored_data(self):
        """
        Analyze the data stored in the database and generate insights.

        This demonstrates how to work with the stored API data.
        """
        logger.info("üìä Analyzing stored data")

        try:
            with sqlite3.connect(self.db_path) as conn:
                # Get summary of API requests
                df_logs = pd.read_sql_query('''
                    SELECT api_name, COUNT(*) as request_count,
                           SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as successful_requests,
                           AVG(response_size) as avg_response_size
                    FROM api_fetch_logs
                    GROUP BY api_name
                ''', conn)

                print("\nüìà API Request Summary:")
                print("=" * 50)
                print(df_logs.to_string(index=False))

                # Weather data analysis
                weather_count = pd.read_sql_query('''
                    SELECT COUNT(*) as count FROM weather_data
                ''', conn).iloc[0]['count']

                if weather_count > 0:
                    df_weather = pd.read_sql_query('''
                        SELECT city_name, temperature, humidity, weather_main
                        FROM weather_data
                        ORDER BY timestamp DESC
                    ''', conn)

                    print(f"\nüå§Ô∏è Weather Data ({weather_count} records):")
                    print("=" * 50)
                    print(df_weather.to_string(index=False))

                # User profiles analysis
                user_count = pd.read_sql_query('''
                    SELECT COUNT(*) as count FROM user_profiles
                ''', conn).iloc[0]['count']

                if user_count > 0:
                    df_users = pd.read_sql_query('''
                        SELECT first_name, last_name, country, age, gender
                        FROM user_profiles
                        ORDER BY timestamp DESC
                    ''', conn)

                    print(f"\nüë• User Profiles ({user_count} records):")
                    print("=" * 50)
                    print(df_users.to_string(index=False))

                # JSON data summary
                df_json = pd.read_sql_query('''
                    SELECT source_api, data_type, COUNT(*) as count
                    FROM json_data
                    GROUP BY source_api, data_type
                ''', conn)

                if not df_json.empty:
                    print(f"\nüìÑ JSON Data Summary:")
                    print("=" * 50)
                    print(df_json.to_string(index=False))

        except sqlite3.Error as e:
            logger.error(f"‚ùå Error analyzing data: {e}")

    def export_data_to_csv(self, output_dir: str = "exported_data"):
        """
        Export stored data to CSV files for further analysis.

        Args:
            output_dir (str): Directory to save CSV files
        """
        logger.info(f"üì§ Exporting data to CSV files in {output_dir}/")

        # Create output directory
        Path(output_dir).mkdir(exist_ok=True)

        try:
            with sqlite3.connect(self.db_path) as conn:
                # Export each table
                tables = ['api_fetch_logs', 'weather_data', 'json_data', 'user_profiles']

                for table in tables:
                    df = pd.read_sql_query(f'SELECT * FROM {table}', conn)
                    if not df.empty:
                        csv_path = Path(output_dir) / f"{table}.csv"
                        df.to_csv(csv_path, index=False)
                        logger.info(f"‚úÖ Exported {len(df)} records from {table} to {csv_path}")
                    else:
                        logger.info(f"‚ÑπÔ∏è No data to export from {table}")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Error exporting data: {e}")

    def cleanup_old_data(self, days_old: int = 30):
        """
        Clean up old data from the database.

        Args:
            days_old (int): Remove data older than this many days
        """
        logger.info(f"üßπ Cleaning up data older than {days_old} days")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Clean up old logs
                cursor.execute('''
                    DELETE FROM api_fetch_logs
                    WHERE fetch_timestamp < datetime('now', '-{} days')
                '''.format(days_old))

                cursor.execute('''
                    DELETE FROM weather_data
                    WHERE timestamp < datetime('now', '-{} days')
                '''.format(days_old))

                cursor.execute('''
                    DELETE FROM json_data
                    WHERE timestamp < datetime('now', '-{} days')
                '''.format(days_old))

                cursor.execute('''
                    DELETE FROM user_profiles
                    WHERE timestamp < datetime('now', '-{} days')
                '''.format(days_old))

                conn.commit()
                logger.info("‚úÖ Old data cleaned up successfully")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Error cleaning up data: {e}")

def demonstrate_error_handling():
    """
    Demonstrate various error handling scenarios.
    """
    print("\nüö® Error Handling Demonstrations:")
    print("=" * 50)

    fetcher = APIDataFetcher("test_error_handling.db")

    # Test with invalid URL
    invalid_endpoint = APIEndpoint(
        name="Invalid API",
        url="https://invalid-url-that-does-not-exist.com/api",
        headers={}
    )

    print("1. Testing invalid URL...")
    result = fetcher.make_api_request(invalid_endpoint)
    print(f"   Result: {result}")

    # Test with valid URL but invalid endpoint
    invalid_path_endpoint = APIEndpoint(
        name="Invalid Path API",
        url="https://httpbin.org/invalid-path",
        headers={}
    )

    print("2. Testing valid URL but invalid path...")
    result = fetcher.make_api_request(invalid_path_endpoint)
    print(f"   Result: {result}")

    print("‚úÖ Error handling demonstrations completed")

def create_integration_examples():
    """
    Show examples of how to integrate with real-world APIs.
    """
    print("\nüîó Real-World API Integration Examples:")
    print("=" * 50)

    examples = {
        "Weather APIs": {
            "OpenWeatherMap": "https://openweathermap.org/api",
            "WeatherAPI": "https://www.weatherapi.com/",
            "AccuWeather": "https://developer.accuweather.com/"
        },
        "Social Media APIs": {
            "Twitter API": "https://developer.twitter.com/",
            "Instagram Basic Display": "https://developers.facebook.com/docs/instagram-basic-display-api/",
            "LinkedIn API": "https://developer.linkedin.com/"
        },
        "Financial APIs": {
            "Alpha Vantage": "https://www.alphavantage.co/",
            "Yahoo Finance": "https://github.com/ranaroussi/yfinance",
            "CoinGecko": "https://www.coingecko.com/en/api"
        },
        "News APIs": {
            "NewsAPI": "https://newsapi.org/",
            "Guardian API": "https://open-platform.theguardian.com/",
            "Reddit API": "https://www.reddit.com/dev/api/"
        }
    }

    for category, apis in examples.items():
        print(f"\n{category}:")
        for name, url in apis.items():
            print(f"  ‚Ä¢ {name}: {url}")

    print(f"\nüí° Integration Tips:")
    print("- Always read the API documentation thoroughly")
    print("- Respect rate limits and implement proper delays")
    print("- Store API keys securely in environment variables")
    print("- Implement proper error handling and logging")
    print("- Consider caching responses to reduce API calls")
    print("- Test with small datasets first")

def main():
    """
    Main function demonstrating the complete API data storage workflow.
    """
    print("üöÄ Welcome to JG's API Data Storage Demo!")
    print("=" * 50)

    try:
        # Initialize the API data fetcher
        fetcher = APIDataFetcher("jg_api_demo.db")

        print("\n1. üå§Ô∏è Fetching and storing weather data...")
        fetcher.fetch_and_store_weather_data()

        print("\n2. üìù Fetching and storing blog posts...")
        fetcher.fetch_and_store_posts_data()

        print("\n3. üë• Fetching and storing user profiles...")
        fetcher.fetch_and_store_user_profiles(count=3)

        print("\n4. üåê Fetching IP information...")
        fetcher.fetch_ip_info()

        print("\n5. üìä Analyzing stored data...")
        fetcher.analyze_stored_data()

        print("\n6. üì§ Exporting data to CSV...")
        fetcher.export_data_to_csv()

        print("\n‚úÖ API data storage demonstration completed successfully!")

        # Show database file info
        db_path = Path("jg_api_demo.db")
        if db_path.exists():
            size_mb = db_path.stat().st_size / (1024 * 1024)
            print(f"\nüìÅ Database file: {db_path.absolute()}")
            print(f"   Size: {size_mb:.2f} MB")

    except Exception as e:
        logger.error(f"‚ùå Demo failed: {e}")
        print("üí° Troubleshooting tips:")
        print("   - Check your internet connection")
        print("   - Ensure you have write permissions in the current directory")
        print("   - Try running with different API endpoints")

    # Demonstrate error handling
    demonstrate_error_handling()

    # Show integration examples
    create_integration_examples()

    print("\nüéì Learning Summary:")
    print("- Making HTTP requests to various APIs")
    print("- JSON data processing and validation")
    print("- SQLite database design and operations")
    print("- Error handling and retry mechanisms")
    print("- Data analysis with Pandas")
    print("- Professional logging and monitoring")

    print("\nüí° Next Steps:")
    print("- Sign up for API keys from services you want to use")
    print("- Implement authentication for private APIs")
    print("- Add data transformation and cleaning logic")
    print("- Set up automated data collection schedules")
    print("- Create data visualization dashboards")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/LA_file_organizer.py

```python
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "pandas",
#     "pathlib",
#     "shutil",
#     "magic",
#     "difflib",
#     "textdistance",
#     "chardet",
#     "pillow",
#     "PyPDF2",
# ]
# ///

"""
LA File Organizer
Student: LA
Description: Go over files and 1. classify them into folders 2. create an index to find files
3. create resume of specific file 4. compare side by side 2-3 file text

This script demonstrates how to:
1. Automatically classify and organize files by type and content
2. Create searchable indexes for quick file location
3. Generate summaries/resumes of file contents
4. Compare multiple files side by side with difference highlighting
5. Handle various file formats (text, images, PDFs, etc.)

Educational Focus:
- File system operations and organization
- Content analysis and classification
- Text processing and summarization
- File comparison algorithms
- Index creation and searching
- Working with multiple file formats
"""

import os
import shutil
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
import difflib
import re
from collections import defaultdict, Counter
import chardet

# For file type detection
try:
    import magic
    HAS_MAGIC = True
except ImportError:
    HAS_MAGIC = False
    print("‚ö†Ô∏è  python-magic not available. Using basic file type detection.")

# For text similarity
try:
    import textdistance
    HAS_TEXTDISTANCE = True
except ImportError:
    HAS_TEXTDISTANCE = False

# For image handling
try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

# For PDF handling
try:
    import PyPDF2
    HAS_PDF = True
except ImportError:
    HAS_PDF = False

class FileClassifier:
    """
    Intelligently classifies files based on content, type, and naming patterns
    """

    def __init__(self):
        """Initialize the file classifier with category definitions"""

        # Define file categories with extensions and keywords
        self.categories = {
            'documents': {
                'extensions': ['.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt'],
                'keywords': ['document', 'report', 'letter', 'memo', 'contract'],
                'folder': 'Documents'
            },
            'spreadsheets': {
                'extensions': ['.xls', '.xlsx', '.csv', '.ods'],
                'keywords': ['data', 'spreadsheet', 'budget', 'inventory', 'list'],
                'folder': 'Spreadsheets'
            },
            'presentations': {
                'extensions': ['.ppt', '.pptx', '.odp'],
                'keywords': ['presentation', 'slides', 'deck'],
                'folder': 'Presentations'
            },
            'images': {
                'extensions': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.svg'],
                'keywords': ['photo', 'image', 'picture', 'screenshot'],
                'folder': 'Images'
            },
            'code': {
                'extensions': ['.py', '.js', '.html', '.css', '.java', '.cpp', '.c', '.rb', '.php'],
                'keywords': ['script', 'code', 'program', 'source'],
                'folder': 'Code'
            },
            'archives': {
                'extensions': ['.zip', '.rar', '.7z', '.tar', '.gz'],
                'keywords': ['archive', 'backup', 'compressed'],
                'folder': 'Archives'
            },
            'media': {
                'extensions': ['.mp4', '.avi', '.mkv', '.mp3', '.wav', '.flac'],
                'keywords': ['video', 'audio', 'music', 'movie'],
                'folder': 'Media'
            },
            'projects': {
                'extensions': [],
                'keywords': ['project', 'work', 'assignment'],
                'folder': 'Projects'
            }
        }

        # If magic is available, initialize it
        if HAS_MAGIC:
            try:
                self.magic = magic.Magic(mime=True)
            except:
                self.magic = None
        else:
            self.magic = None

        print("‚úÖ File Classifier initialized")

    def classify_file(self, file_path: Path) -> str:
        """
        Classify a file into one of the predefined categories

        Args:
            file_path: Path to the file to classify

        Returns:
            Category name for the file
        """
        file_name = file_path.name.lower()
        file_ext = file_path.suffix.lower()

        # First, try classification by extension
        for category, details in self.categories.items():
            if file_ext in details['extensions']:
                return category

        # Then try classification by filename keywords
        for category, details in self.categories.items():
            for keyword in details['keywords']:
                if keyword in file_name:
                    return category

        # For text files, try content-based classification
        if file_ext in ['.txt', '.md', ''] and file_path.is_file():
            content_category = self._classify_by_content(file_path)
            if content_category:
                return content_category

        # If magic is available, try MIME type classification
        if self.magic and file_path.is_file():
            try:
                mime_type = self.magic.from_file(str(file_path))
                mime_category = self._classify_by_mime(mime_type)
                if mime_category:
                    return mime_category
            except:
                pass

        # Default category
        return 'miscellaneous'

    def _classify_by_content(self, file_path: Path) -> Optional[str]:
        """Classify text files based on their content"""
        try:
            # Read a sample of the file to determine encoding
            with open(file_path, 'rb') as f:
                raw_data = f.read(1024)

            encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

            # Read the file content
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read(2000).lower()  # Read first 2000 chars

            # Check for code patterns
            code_patterns = [
                r'def\s+\w+\s*\(',  # Python functions
                r'function\s+\w+\s*\(',  # JavaScript functions
                r'class\s+\w+',  # Class definitions
                r'import\s+\w+',  # Import statements
                r'#include\s*<',  # C/C++ includes
                r'<?php',  # PHP tags
            ]

            for pattern in code_patterns:
                if re.search(pattern, content):
                    return 'code'

            # Check for document patterns
            doc_patterns = [
                r'\b(dear|sincerely|regards)\b',  # Letter patterns
                r'\b(introduction|conclusion|summary)\b',  # Document structure
                r'\b(chapter|section|appendix)\b',  # Book/report structure
            ]

            for pattern in doc_patterns:
                if re.search(pattern, content):
                    return 'documents'

            return None

        except Exception:
            return None

    def _classify_by_mime(self, mime_type: str) -> Optional[str]:
        """Classify files based on MIME type"""
        mime_mappings = {
            'text/': 'documents',
            'image/': 'images',
            'video/': 'media',
            'audio/': 'media',
            'application/pdf': 'documents',
            'application/msword': 'documents',
            'application/vnd.ms-excel': 'spreadsheets',
            'application/vnd.ms-powerpoint': 'presentations',
            'application/zip': 'archives',
        }

        for mime_prefix, category in mime_mappings.items():
            if mime_type.startswith(mime_prefix):
                return category

        return None

class FileOrganizer:
    """
    Organizes files into categorized folder structures
    """

    def __init__(self, base_directory: Path):
        """
        Initialize the file organizer

        Args:
            base_directory: Base directory to organize files in
        """
        self.base_dir = Path(base_directory)
        self.classifier = FileClassifier()
        self.organization_log = []

        print(f"‚úÖ File Organizer initialized for: {self.base_dir}")

    def organize_files(self, source_directory: Path = None, dry_run: bool = True) -> Dict[str, Any]:
        """
        Organize files in the specified directory

        Args:
            source_directory: Directory to organize (defaults to base_directory)
            dry_run: If True, only plan the organization without moving files

        Returns:
            Dictionary with organization results and statistics
        """
        if source_directory is None:
            source_directory = self.base_dir

        source_path = Path(source_directory)

        if not source_path.exists():
            print(f"‚ùå Source directory does not exist: {source_path}")
            return {}

        print(f"üìÅ {'Planning' if dry_run else 'Executing'} file organization in: {source_path}")

        # Find all files to organize
        files_to_organize = []
        for file_path in source_path.rglob('*'):
            if file_path.is_file() and not file_path.name.startswith('.'):
                files_to_organize.append(file_path)

        print(f"üîç Found {len(files_to_organize)} files to organize")

        # Classify and plan organization
        organization_plan = defaultdict(list)
        for file_path in files_to_organize:
            category = self.classifier.classify_file(file_path)
            organization_plan[category].append(file_path)

        # Execute organization plan
        results = {
            'total_files': len(files_to_organize),
            'categories': dict(organization_plan),
            'moved_files': 0,
            'errors': []
        }

        if not dry_run:
            results['moved_files'] = self._execute_organization_plan(organization_plan)
        else:
            print("\nüìã Organization Plan (Dry Run):")
            for category, files in organization_plan.items():
                folder_name = self.classifier.categories.get(category, {}).get('folder', category.title())
                print(f"  {folder_name}: {len(files)} files")

        return results

    def _execute_organization_plan(self, organization_plan: Dict[str, List[Path]]) -> int:
        """Execute the file organization plan"""
        moved_count = 0

        for category, files in organization_plan.items():
            if not files:
                continue

            # Create category folder
            folder_name = self.classifier.categories.get(category, {}).get('folder', category.title())
            category_dir = self.base_dir / folder_name
            category_dir.mkdir(exist_ok=True)

            # Move files
            for file_path in files:
                try:
                    destination = category_dir / file_path.name

                    # Handle duplicate names
                    if destination.exists():
                        destination = self._get_unique_filename(destination)

                    shutil.move(str(file_path), str(destination))
                    moved_count += 1

                    # Log the move
                    self.organization_log.append({
                        'timestamp': datetime.now().isoformat(),
                        'action': 'move',
                        'source': str(file_path),
                        'destination': str(destination),
                        'category': category
                    })

                    print(f"üì¶ Moved: {file_path.name} ‚Üí {folder_name}/")

                except Exception as e:
                    print(f"‚ùå Error moving {file_path.name}: {e}")

        return moved_count

    def _get_unique_filename(self, file_path: Path) -> Path:
        """Generate a unique filename if the original already exists"""
        base = file_path.stem
        extension = file_path.suffix
        parent = file_path.parent
        counter = 1

        while True:
            new_name = f"{base}_{counter}{extension}"
            new_path = parent / new_name
            if not new_path.exists():
                return new_path
            counter += 1

class FileIndexer:
    """
    Creates searchable indexes of files for quick location and retrieval
    """

    def __init__(self, base_directory: Path):
        """Initialize the file indexer"""
        self.base_dir = Path(base_directory)
        self.index_file = self.base_dir / ".file_index.json"
        self.index = {}

        print(f"‚úÖ File Indexer initialized for: {self.base_dir}")

    def create_index(self, include_content: bool = True) -> Dict[str, Any]:
        """
        Create a comprehensive index of all files

        Args:
            include_content: Whether to index file contents for text files

        Returns:
            Dictionary containing the file index
        """
        print(f"üóÇÔ∏è  Creating file index...")

        index = {
            'created': datetime.now().isoformat(),
            'base_directory': str(self.base_dir),
            'files': {},
            'statistics': {}
        }

        files_processed = 0
        total_size = 0

        for file_path in self.base_dir.rglob('*'):
            if file_path.is_file() and not file_path.name.startswith('.'):
                try:
                    # Get file information
                    stat = file_path.stat()
                    file_info = {
                        'path': str(file_path.relative_to(self.base_dir)),
                        'absolute_path': str(file_path),
                        'size': stat.st_size,
                        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        'extension': file_path.suffix.lower(),
                        'category': FileClassifier().classify_file(file_path)
                    }

                    # Add content hash for duplicate detection
                    file_info['hash'] = self._get_file_hash(file_path)

                    # Add content preview for text files
                    if include_content and self._is_text_file(file_path):
                        content_info = self._extract_text_content(file_path)
                        file_info.update(content_info)

                    # Use relative path as key
                    key = str(file_path.relative_to(self.base_dir))
                    index['files'][key] = file_info

                    files_processed += 1
                    total_size += stat.st_size

                except Exception as e:
                    print(f"‚ö†Ô∏è  Error indexing {file_path.name}: {e}")

        # Add statistics
        index['statistics'] = {
            'total_files': files_processed,
            'total_size': total_size,
            'categories': self._get_category_stats(index['files'])
        }

        self.index = index
        self._save_index()

        print(f"‚úÖ Index created: {files_processed} files, {total_size / (1024*1024):.1f} MB")
        return index

    def _get_file_hash(self, file_path: Path) -> str:
        """Generate MD5 hash of file for duplicate detection"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""

    def _is_text_file(self, file_path: Path) -> bool:
        """Check if a file is likely to be a text file"""
        text_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv'}

        if file_path.suffix.lower() in text_extensions:
            return True

        # Try reading a small sample to see if it's text
        try:
            with open(file_path, 'rb') as f:
                sample = f.read(1024)

            # Check if the sample is mostly printable ASCII
            try:
                sample.decode('utf-8')
                return True
            except UnicodeDecodeError:
                return False
        except Exception:
            return False

    def _extract_text_content(self, file_path: Path) -> Dict[str, Any]:
        """Extract text content and metadata from a file"""
        content_info = {
            'preview': '',
            'word_count': 0,
            'line_count': 0,
            'keywords': []
        }

        try:
            # Detect encoding
            with open(file_path, 'rb') as f:
                raw_data = f.read(1024)
            encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

            # Read content
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()

            # Extract preview (first 500 characters)
            content_info['preview'] = content[:500].replace('\n', ' ').strip()

            # Count words and lines
            content_info['word_count'] = len(content.split())
            content_info['line_count'] = len(content.splitlines())

            # Extract keywords (most common words, excluding common stop words)
            stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}

            words = re.findall(r'\b[a-zA-Z]{3,}\b', content.lower())
            word_freq = Counter(word for word in words if word not in stop_words)
            content_info['keywords'] = [word for word, freq in word_freq.most_common(10)]

        except Exception as e:
            print(f"‚ö†Ô∏è  Error extracting content from {file_path.name}: {e}")

        return content_info

    def _get_category_stats(self, files: Dict) -> Dict[str, int]:
        """Get statistics about file categories"""
        categories = defaultdict(int)
        for file_info in files.values():
            categories[file_info['category']] += 1
        return dict(categories)

    def _save_index(self):
        """Save the index to a JSON file"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.index, f, indent=2, ensure_ascii=False)
            print(f"üíæ Index saved to: {self.index_file}")
        except Exception as e:
            print(f"‚ùå Error saving index: {e}")

    def search_files(self, query: str, search_type: str = 'all') -> List[Dict[str, Any]]:
        """
        Search for files based on various criteria

        Args:
            query: Search query
            search_type: Type of search ('name', 'content', 'category', 'all')

        Returns:
            List of matching file information
        """
        if not self.index:
            self._load_index()

        if not self.index:
            print("‚ùå No index available. Please create an index first.")
            return []

        query_lower = query.lower()
        results = []

        for file_path, file_info in self.index.get('files', {}).items():
            match = False

            if search_type in ['name', 'all']:
                if query_lower in file_path.lower():
                    match = True

            if search_type in ['content', 'all'] and 'preview' in file_info:
                if query_lower in file_info['preview'].lower():
                    match = True

            if search_type in ['category', 'all']:
                if query_lower in file_info.get('category', '').lower():
                    match = True

            if search_type in ['keywords', 'all'] and 'keywords' in file_info:
                if any(query_lower in keyword for keyword in file_info['keywords']):
                    match = True

            if match:
                results.append({
                    'path': file_path,
                    'relevance': self._calculate_relevance(query_lower, file_info),
                    **file_info
                })

        # Sort by relevance
        results.sort(key=lambda x: x['relevance'], reverse=True)
        return results

    def _calculate_relevance(self, query: str, file_info: Dict) -> float:
        """Calculate relevance score for search results"""
        score = 0.0

        # Filename match
        if query in file_info.get('path', '').lower():
            score += 10.0

        # Content match
        if 'preview' in file_info and query in file_info['preview'].lower():
            score += 5.0

        # Category match
        if query in file_info.get('category', '').lower():
            score += 3.0

        # Keywords match
        if 'keywords' in file_info:
            for keyword in file_info['keywords']:
                if query in keyword:
                    score += 2.0

        return score

    def _load_index(self):
        """Load the index from the JSON file"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    self.index = json.load(f)
                print(f"üìñ Loaded index from: {self.index_file}")
            except Exception as e:
                print(f"‚ùå Error loading index: {e}")
                self.index = {}

class FileSummarizer:
    """
    Creates summaries/resumes of file contents
    """

    def __init__(self):
        """Initialize the file summarizer"""
        print("‚úÖ File Summarizer initialized")

    def summarize_file(self, file_path: Path) -> Dict[str, Any]:
        """
        Create a comprehensive summary of a file

        Args:
            file_path: Path to the file to summarize

        Returns:
            Dictionary containing file summary information
        """
        if not file_path.exists():
            return {'error': f"File not found: {file_path}"}

        print(f"üìÑ Summarizing file: {file_path.name}")

        summary = {
            'file_name': file_path.name,
            'file_path': str(file_path),
            'file_size': file_path.stat().st_size,
            'modified_date': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
            'file_type': file_path.suffix.lower(),
            'summary_generated': datetime.now().isoformat()
        }

        # Type-specific summarization
        if self._is_text_file(file_path):
            summary.update(self._summarize_text_file(file_path))
        elif file_path.suffix.lower() == '.pdf' and HAS_PDF:
            summary.update(self._summarize_pdf_file(file_path))
        elif file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif'] and HAS_PIL:
            summary.update(self._summarize_image_file(file_path))
        else:
            summary['content_type'] = 'binary'
            summary['description'] = f"Binary file of type {file_path.suffix}"

        return summary

    def _is_text_file(self, file_path: Path) -> bool:
        """Check if file is a text file"""
        text_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv', '.log'}
        return file_path.suffix.lower() in text_extensions

    def _summarize_text_file(self, file_path: Path) -> Dict[str, Any]:
        """Summarize a text file"""
        try:
            # Detect encoding
            with open(file_path, 'rb') as f:
                raw_data = f.read(1024)
            encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

            # Read content
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()

            # Basic statistics
            lines = content.splitlines()
            words = content.split()

            summary = {
                'content_type': 'text',
                'character_count': len(content),
                'word_count': len(words),
                'line_count': len(lines),
                'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
                'encoding': encoding
            }

            # Extract structure for code files
            if file_path.suffix.lower() in ['.py', '.js', '.java', '.cpp', '.c']:
                summary.update(self._analyze_code_structure(content, file_path.suffix))

            # Extract first few lines as preview
            summary['preview'] = '\n'.join(lines[:10])

            # Find important sections
            summary['sections'] = self._extract_sections(content)

            return summary

        except Exception as e:
            return {'content_type': 'text', 'error': str(e)}

    def _analyze_code_structure(self, content: str, extension: str) -> Dict[str, Any]:
        """Analyze code file structure"""
        structure = {
            'functions': [],
            'classes': [],
            'imports': [],
            'comments_ratio': 0.0
        }

        lines = content.splitlines()
        comment_lines = 0

        for line in lines:
            line_stripped = line.strip()

            # Count comments
            if extension == '.py' and (line_stripped.startswith('#') or '"""' in line_stripped):
                comment_lines += 1
            elif extension in ['.js', '.java', '.cpp', '.c'] and (line_stripped.startswith('//') or '/*' in line_stripped):
                comment_lines += 1

            # Find functions
            if extension == '.py':
                if re.match(r'^\s*def\s+(\w+)', line):
                    func_match = re.search(r'def\s+(\w+)', line)
                    if func_match:
                        structure['functions'].append(func_match.group(1))
                elif re.match(r'^\s*class\s+(\w+)', line):
                    class_match = re.search(r'class\s+(\w+)', line)
                    if class_match:
                        structure['classes'].append(class_match.group(1))
                elif re.match(r'^\s*(import|from)\s+', line):
                    structure['imports'].append(line.strip())

        structure['comments_ratio'] = comment_lines / len(lines) if lines else 0
        return structure

    def _extract_sections(self, content: str) -> List[str]:
        """Extract section headers from text content"""
        sections = []
        lines = content.splitlines()

        for line in lines:
            line_stripped = line.strip()

            # Markdown headers
            if line_stripped.startswith('#'):
                sections.append(line_stripped)

            # Lines in ALL CAPS (potential headers)
            elif len(line_stripped) > 3 and line_stripped.isupper() and line_stripped.replace(' ', '').isalpha():
                sections.append(line_stripped)

        return sections[:10]  # Return first 10 sections

    def _summarize_pdf_file(self, file_path: Path) -> Dict[str, Any]:
        """Summarize a PDF file"""
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)

                summary = {
                    'content_type': 'pdf',
                    'page_count': len(pdf_reader.pages),
                    'title': pdf_reader.metadata.get('/Title', 'Unknown') if pdf_reader.metadata else 'Unknown',
                    'author': pdf_reader.metadata.get('/Author', 'Unknown') if pdf_reader.metadata else 'Unknown',
                }

                # Extract text from first few pages
                text_preview = ""
                for i, page in enumerate(pdf_reader.pages[:3]):  # First 3 pages
                    text_preview += page.extract_text() + "\n"

                summary['preview'] = text_preview[:1000]  # First 1000 characters
                summary['word_count'] = len(text_preview.split())

                return summary

        except Exception as e:
            return {'content_type': 'pdf', 'error': str(e)}

    def _summarize_image_file(self, file_path: Path) -> Dict[str, Any]:
        """Summarize an image file"""
        try:
            with Image.open(file_path) as img:
                summary = {
                    'content_type': 'image',
                    'dimensions': f"{img.width}x{img.height}",
                    'mode': img.mode,
                    'format': img.format,
                    'has_transparency': 'transparency' in img.info or img.mode in ['RGBA', 'LA']
                }

                # Basic color analysis
                if img.mode == 'RGB':
                    colors = img.getcolors(maxcolors=256*256*256)
                    if colors:
                        dominant_color = max(colors, key=lambda x: x[0])[1]
                        summary['dominant_color'] = f"RGB{dominant_color}"

                return summary

        except Exception as e:
            return {'content_type': 'image', 'error': str(e)}

class FileComparator:
    """
    Compares multiple files side by side with difference highlighting
    """

    def __init__(self):
        """Initialize the file comparator"""
        print("‚úÖ File Comparator initialized")

    def compare_files(self, file_paths: List[Path], comparison_type: str = 'auto') -> Dict[str, Any]:
        """
        Compare multiple files and highlight differences

        Args:
            file_paths: List of file paths to compare
            comparison_type: Type of comparison ('text', 'binary', 'auto')

        Returns:
            Dictionary containing comparison results
        """
        if len(file_paths) < 2:
            return {'error': 'At least 2 files required for comparison'}

        print(f"üîç Comparing {len(file_paths)} files...")

        # Validate all files exist
        for file_path in file_paths:
            if not file_path.exists():
                return {'error': f'File not found: {file_path}'}

        comparison = {
            'files': [str(path) for path in file_paths],
            'comparison_date': datetime.now().isoformat(),
            'file_info': []
        }

        # Get basic info for each file
        for file_path in file_paths:
            stat = file_path.stat()
            comparison['file_info'].append({
                'name': file_path.name,
                'size': stat.st_size,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'hash': self._get_file_hash(file_path)
            })

        # Determine comparison type
        if comparison_type == 'auto':
            comparison_type = 'text' if all(self._is_text_file(path) for path in file_paths) else 'binary'

        # Perform comparison
        if comparison_type == 'text':
            comparison.update(self._compare_text_files(file_paths))
        else:
            comparison.update(self._compare_binary_files(file_paths))

        return comparison

    def _compare_text_files(self, file_paths: List[Path]) -> Dict[str, Any]:
        """Compare text files line by line"""
        try:
            # Read all files
            file_contents = []
            for file_path in file_paths:
                # Detect encoding
                with open(file_path, 'rb') as f:
                    raw_data = f.read(1024)
                encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

                # Read content
                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                    file_contents.append(content.splitlines())

            # Calculate similarity
            similarity_matrix = self._calculate_text_similarity_matrix(file_contents)

            # Generate unified diff for first two files
            if len(file_contents) >= 2:
                diff = list(difflib.unified_diff(
                    file_contents[0],
                    file_contents[1],
                    fromfile=file_paths[0].name,
                    tofile=file_paths[1].name,
                    lineterm=''
                ))
            else:
                diff = []

            return {
                'comparison_type': 'text',
                'similarity_matrix': similarity_matrix,
                'unified_diff': diff[:100],  # Limit diff output
                'statistics': self._get_text_comparison_stats(file_contents)
            }

        except Exception as e:
            return {'comparison_type': 'text', 'error': str(e)}

    def _compare_binary_files(self, file_paths: List[Path]) -> Dict[str, Any]:
        """Compare binary files by hash and size"""
        try:
            hashes = [self._get_file_hash(path) for path in file_paths]
            sizes = [path.stat().st_size for path in file_paths]

            # Check for identical files
            identical_groups = defaultdict(list)
            for i, hash_val in enumerate(hashes):
                identical_groups[hash_val].append(i)

            identical_files = [group for group in identical_groups.values() if len(group) > 1]

            return {
                'comparison_type': 'binary',
                'hashes': hashes,
                'sizes': sizes,
                'identical_files': identical_files,
                'all_identical': len(set(hashes)) == 1
            }

        except Exception as e:
            return {'comparison_type': 'binary', 'error': str(e)}

    def _calculate_text_similarity_matrix(self, file_contents: List[List[str]]) -> List[List[float]]:
        """Calculate similarity matrix for text files"""
        n = len(file_contents)
        matrix = [[0.0] * n for _ in range(n)]

        for i in range(n):
            for j in range(n):
                if i == j:
                    matrix[i][j] = 1.0
                else:
                    # Calculate similarity ratio
                    text1 = '\n'.join(file_contents[i])
                    text2 = '\n'.join(file_contents[j])

                    # Use textdistance if available, otherwise use difflib
                    if HAS_TEXTDISTANCE:
                        similarity = textdistance.jaro_winkler(text1, text2)
                    else:
                        similarity = difflib.SequenceMatcher(None, text1, text2).ratio()

                    matrix[i][j] = similarity

        return matrix

    def _get_text_comparison_stats(self, file_contents: List[List[str]]) -> Dict[str, Any]:
        """Get statistical comparison of text files"""
        stats = {
            'line_counts': [len(content) for content in file_contents],
            'word_counts': [],
            'character_counts': []
        }

        for content_lines in file_contents:
            content_text = '\n'.join(content_lines)
            stats['word_counts'].append(len(content_text.split()))
            stats['character_counts'].append(len(content_text))

        return stats

    def _get_file_hash(self, file_path: Path) -> str:
        """Generate MD5 hash of file"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""

    def _is_text_file(self, file_path: Path) -> bool:
        """Check if file is a text file"""
        text_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv', '.log'}

        if file_path.suffix.lower() in text_extensions:
            return True

        try:
            with open(file_path, 'rb') as f:
                sample = f.read(1024)
            sample.decode('utf-8')
            return True
        except (UnicodeDecodeError, Exception):
            return False

def create_sample_files(sample_dir: Path):
    """Create sample files for demonstration"""
    sample_dir.mkdir(exist_ok=True)

    # Create various sample files
    files_to_create = {
        'report.txt': """# Annual Report 2024

## Executive Summary
This document provides an overview of our company's performance in 2024.

## Financial Highlights
- Revenue: $10M
- Profit: $2M
- Growth: 15%

## Future Plans
We plan to expand our operations in the coming year.
""",
        'script.py': """#!/usr/bin/env python3
\"\"\"
Sample Python script for demonstration
\"\"\"

def hello_world():
    \"\"\"Print hello world message\"\"\"
    print("Hello, World!")

def calculate_sum(a, b):
    \"\"\"Calculate sum of two numbers\"\"\"
    return a + b

if __name__ == "__main__":
    hello_world()
    result = calculate_sum(5, 3)
    print(f"Sum: {result}")
""",
        'data.csv': """Name,Age,City
Alice,30,New York
Bob,25,San Francisco
Charlie,35,Chicago
Diana,28,Boston
""",
        'notes.md': """# Meeting Notes

## Date: March 15, 2024

### Attendees
- Alice Johnson
- Bob Smith
- Charlie Brown

### Action Items
1. Complete budget review
2. Update project timeline
3. Schedule follow-up meeting

### Next Steps
Review progress next week.
""",
        'similar_script.py': """#!/usr/bin/env python3
\"\"\"
Another Python script for comparison
\"\"\"

def greet():
    \"\"\"Print greeting message\"\"\"
    print("Hello, Python!")

def add_numbers(x, y):
    \"\"\"Add two numbers\"\"\"
    return x + y

if __name__ == "__main__":
    greet()
    total = add_numbers(10, 5)
    print(f"Total: {total}")
""",
    }

    for filename, content in files_to_create.items():
        file_path = sample_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

    print(f"‚úÖ Created {len(files_to_create)} sample files in {sample_dir}")

def main():
    """
    Main function demonstrating the file organizer functionality
    """
    print("üìÅ LA File Organizer - Educational Demo")
    print("=" * 50)

    # Set up demo directory
    demo_dir = Path(__file__).parent / "file_organizer_demo"
    sample_dir = demo_dir / "sample_files"

    # Create sample files
    print("\nüìö Example 1: Creating sample files...")
    create_sample_files(sample_dir)

    # Initialize components
    organizer = FileOrganizer(demo_dir)
    indexer = FileIndexer(demo_dir)
    summarizer = FileSummarizer()
    comparator = FileComparator()

    # Example 1: Organize files (dry run first)
    print("\nüìö Example 2: File organization (dry run)...")
    organization_results = organizer.organize_files(sample_dir, dry_run=True)

    print(f"Organization plan:")
    for category, files in organization_results['categories'].items():
        print(f"  {category}: {len(files)} files")

    # Example 2: Create file index
    print("\nüìö Example 3: Creating file index...")
    index_results = indexer.create_index(include_content=True)

    print(f"Index statistics:")
    for category, count in index_results['statistics']['categories'].items():
        print(f"  {category}: {count} files")

    # Example 3: Search files
    print("\nüìö Example 4: Searching files...")
    search_results = indexer.search_files("python", search_type='all')

    print(f"Search results for 'python':")
    for result in search_results[:3]:  # Show top 3 results
        print(f"  {result['path']} (relevance: {result['relevance']:.1f})")

    # Example 4: Summarize a file
    print("\nüìö Example 5: File summarization...")
    script_file = sample_dir / "script.py"
    if script_file.exists():
        summary = summarizer.summarize_file(script_file)
        print(f"Summary of {script_file.name}:")
        print(f"  Type: {summary.get('content_type', 'unknown')}")
        print(f"  Lines: {summary.get('line_count', 0)}")
        print(f"  Words: {summary.get('word_count', 0)}")
        if 'functions' in summary:
            print(f"  Functions: {', '.join(summary['functions'])}")

    # Example 5: Compare files
    print("\nüìö Example 6: File comparison...")
    script1 = sample_dir / "script.py"
    script2 = sample_dir / "similar_script.py"

    if script1.exists() and script2.exists():
        comparison = comparator.compare_files([script1, script2])

        if 'similarity_matrix' in comparison:
            similarity = comparison['similarity_matrix'][0][1]
            print(f"Similarity between scripts: {similarity:.2%}")

        if 'unified_diff' in comparison and comparison['unified_diff']:
            print("First few differences:")
            for line in comparison['unified_diff'][:10]:
                print(f"  {line}")

    # Example 6: Find duplicate files
    print("\nüìö Example 7: Finding duplicate files...")
    all_files = list(sample_dir.rglob('*'))
    file_hashes = {}

    for file_path in all_files:
        if file_path.is_file():
            file_hash = comparator._get_file_hash(file_path)
            if file_hash in file_hashes:
                print(f"Potential duplicate found:")
                print(f"  Original: {file_hashes[file_hash]}")
                print(f"  Duplicate: {file_path}")
            else:
                file_hashes[file_hash] = file_path

    print(f"\nüéì Educational Notes:")
    print("1. Always backup files before reorganizing")
    print("2. Use dry runs to test organization logic")
    print("3. Index files periodically to keep search current")
    print("4. Handle different file encodings carefully")
    print("5. Consider memory usage when processing large files")
    print("6. Use appropriate similarity algorithms for your use case")
    print("7. Regular maintenance keeps file systems organized")

    # Cleanup
    try:
        shutil.rmtree(demo_dir)
        print(f"üßπ Cleaned up demo directory: {demo_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not clean up demo directory: {e}")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/LC_email_formatter.py

```python
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "requests",
#     "beautifulsoup4",
#     "pandas",
#     "jinja2",
#     "smtplib",
# ]
# ///

"""
LC Email Formatter
Student: LC
Description: Formatting e-mail based on data from a website... looking at webpage that has
table of tasks with due date and shows if task is overdue, then create email to send to
person with list of overdue tasks

This script demonstrates how to:
1. Scrape task data from web pages (tables)
2. Parse and analyze due dates
3. Identify overdue tasks
4. Generate formatted emails with task summaries
5. Use email templates for consistent formatting
6. Send emails programmatically (with safety measures)

Educational Focus:
- Web scraping with BeautifulSoup
- Date/time manipulation
- Email composition and formatting
- Template engines (Jinja2)
- Error handling and validation
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import List, Dict, Optional, Tuple
import re
from pathlib import Path
import json
from jinja2 import Template
import time

class TaskScraper:
    """
    A web scraper specifically designed to extract task data from web pages

    This class handles:
    - Scraping HTML tables containing task information
    - Parsing various date formats
    - Extracting assignee information
    - Handling different website structures
    """

    def __init__(self):
        """Initialize the scraper with default settings"""
        self.session = requests.Session()
        # Set a user agent to avoid being blocked
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        # Common date formats we might encounter
        self.date_formats = [
            "%Y-%m-%d",
            "%m/%d/%Y",
            "%d/%m/%Y",
            "%B %d, %Y",
            "%b %d, %Y",
            "%Y-%m-%d %H:%M:%S",
            "%m-%d-%Y"
        ]

        print("‚úÖ Task Scraper initialized")

    def scrape_task_table(self, url: str, table_selector: str = None) -> pd.DataFrame:
        """
        Scrape task data from a webpage table

        Args:
            url: URL of the webpage to scrape
            table_selector: CSS selector for the specific table (optional)

        Returns:
            DataFrame containing task data
        """
        print(f"üåê Scraping task data from: {url}")

        try:
            # Fetch the webpage
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Find the task table
            if table_selector:
                table = soup.select_one(table_selector)
            else:
                # Try to find the most likely task table
                table = self._find_task_table(soup)

            if not table:
                print("‚ùå No suitable table found on the webpage")
                return pd.DataFrame()

            # Extract table data
            return self._extract_table_data(table)

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error fetching webpage: {e}")
            return pd.DataFrame()
        except Exception as e:
            print(f"‚ùå Error parsing webpage: {e}")
            return pd.DataFrame()

    def _find_task_table(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """
        Intelligently find the most likely task table on a page

        Looks for tables with headers that suggest task management
        """
        tables = soup.find_all('table')

        # Keywords that suggest a task table
        task_keywords = ['task', 'due', 'deadline', 'assigned', 'status', 'priority', 'project']

        best_table = None
        best_score = 0

        for table in tables:
            score = 0

            # Check headers for task-related keywords
            headers = table.find_all(['th', 'td'])
            for header in headers[:10]:  # Check first 10 cells
                header_text = header.get_text().lower()
                for keyword in task_keywords:
                    if keyword in header_text:
                        score += 1

            if score > best_score:
                best_score = score
                best_table = table

        return best_table

    def _extract_table_data(self, table) -> pd.DataFrame:
        """Extract data from an HTML table and return as DataFrame"""

        # Find header row
        header_row = table.find('tr')
        if not header_row:
            return pd.DataFrame()

        # Extract headers
        headers = []
        for th in header_row.find_all(['th', 'td']):
            headers.append(th.get_text().strip())

        # Extract data rows
        rows = []
        for tr in table.find_all('tr')[1:]:  # Skip header row
            row = []
            for td in tr.find_all(['td', 'th']):
                row.append(td.get_text().strip())
            if row:  # Only add non-empty rows
                rows.append(row)

        # Create DataFrame
        if rows and headers:
            # Ensure all rows have the same number of columns as headers
            max_cols = len(headers)
            normalized_rows = []
            for row in rows:
                # Pad with empty strings if row is too short
                while len(row) < max_cols:
                    row.append("")
                # Truncate if row is too long
                normalized_rows.append(row[:max_cols])

            df = pd.DataFrame(normalized_rows, columns=headers)
            print(f"‚úÖ Extracted {len(df)} rows with columns: {list(df.columns)}")
            return df

        return pd.DataFrame()

class TaskAnalyzer:
    """
    Analyzes task data to identify overdue tasks and generate insights
    """

    def __init__(self):
        """Initialize the analyzer"""
        self.today = datetime.now().date()
        print("‚úÖ Task Analyzer initialized")

    def identify_overdue_tasks(self, df: pd.DataFrame,
                             due_date_column: str = None,
                             assignee_column: str = None) -> pd.DataFrame:
        """
        Identify overdue tasks from a DataFrame of tasks

        Args:
            df: DataFrame containing task data
            due_date_column: Name of the column containing due dates
            assignee_column: Name of the column containing assignee names

        Returns:
            DataFrame containing only overdue tasks with additional analysis
        """
        if df.empty:
            print("‚ö†Ô∏è  No task data to analyze")
            return pd.DataFrame()

        print(f"üìä Analyzing {len(df)} tasks for overdue items...")

        # Auto-detect date column if not specified
        if due_date_column is None:
            due_date_column = self._detect_date_column(df)

        if due_date_column is None:
            print("‚ùå Could not identify due date column")
            return df  # Return original data if we can't find dates

        # Auto-detect assignee column if not specified
        if assignee_column is None:
            assignee_column = self._detect_assignee_column(df)

        # Parse dates
        df_copy = df.copy()
        df_copy['parsed_due_date'] = df_copy[due_date_column].apply(self._parse_date)

        # Filter for valid dates and overdue tasks
        valid_dates = df_copy['parsed_due_date'].notna()
        df_valid = df_copy[valid_dates].copy()

        if df_valid.empty:
            print("‚ö†Ô∏è  No valid dates found in task data")
            return pd.DataFrame()

        # Identify overdue tasks
        df_valid['is_overdue'] = df_valid['parsed_due_date'] < self.today
        overdue_tasks = df_valid[df_valid['is_overdue']].copy()

        # Calculate days overdue
        overdue_tasks['days_overdue'] = (self.today - overdue_tasks['parsed_due_date']).dt.days

        # Add priority based on how overdue the task is
        overdue_tasks['priority'] = overdue_tasks['days_overdue'].apply(self._calculate_priority)

        print(f"üîç Found {len(overdue_tasks)} overdue tasks")

        return overdue_tasks

    def _detect_date_column(self, df: pd.DataFrame) -> Optional[str]:
        """Auto-detect which column contains due dates"""
        date_keywords = ['due', 'deadline', 'date', 'end', 'finish', 'completion']

        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in date_keywords):
                # Test if this column contains date-like values
                sample_values = df[col].dropna().head(5)
                date_count = sum(1 for val in sample_values if self._parse_date(val) is not None)
                if date_count >= len(sample_values) * 0.5:  # At least 50% are valid dates
                    print(f"üìÖ Detected due date column: {col}")
                    return col

        return None

    def _detect_assignee_column(self, df: pd.DataFrame) -> Optional[str]:
        """Auto-detect which column contains assignee names"""
        assignee_keywords = ['assign', 'owner', 'responsible', 'user', 'person', 'name']

        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in assignee_keywords):
                print(f"üë§ Detected assignee column: {col}")
                return col

        return None

    def _parse_date(self, date_str: str) -> Optional[datetime.date]:
        """Parse a date string into a datetime.date object"""
        if pd.isna(date_str) or not date_str:
            return None

        date_str = str(date_str).strip()

        # Try different date formats
        for fmt in ["%Y-%m-%d", "%m/%d/%Y", "%d/%m/%Y", "%B %d, %Y", "%b %d, %Y"]:
            try:
                return datetime.strptime(date_str, fmt).date()
            except ValueError:
                continue

        # Try parsing with pandas for more flexibility
        try:
            return pd.to_datetime(date_str).date()
        except:
            return None

    def _calculate_priority(self, days_overdue: int) -> str:
        """Calculate priority level based on how many days overdue"""
        if days_overdue >= 30:
            return "CRITICAL"
        elif days_overdue >= 14:
            return "HIGH"
        elif days_overdue >= 7:
            return "MEDIUM"
        else:
            return "LOW"

class EmailFormatter:
    """
    Generates and formats emails for overdue task notifications
    """

    def __init__(self):
        """Initialize the email formatter"""
        self.templates = self._load_email_templates()
        print("‚úÖ Email Formatter initialized")

    def _load_email_templates(self) -> Dict[str, str]:
        """Load email templates for different scenarios"""
        return {
            'overdue_tasks': """
Dear {{ recipient_name }},

I hope this email finds you well. This is a friendly reminder about some overdue tasks that require your attention.

**OVERDUE TASKS SUMMARY**
Total Overdue Tasks: {{ total_tasks }}
{% if critical_tasks > 0 %}üî¥ Critical (30+ days): {{ critical_tasks }}{% endif %}
{% if high_tasks > 0 %}üü† High Priority (14+ days): {{ high_tasks }}{% endif %}
{% if medium_tasks > 0 %}üü° Medium Priority (7+ days): {{ medium_tasks }}{% endif %}
{% if low_tasks > 0 %}üü¢ Low Priority (< 7 days): {{ low_tasks }}{% endif %}

**TASK DETAILS**
{% for task in tasks %}
{{ loop.index }}. **{{ task.title }}**
   - Due Date: {{ task.due_date }}
   - Days Overdue: {{ task.days_overdue }}
   - Priority: {{ task.priority }}
   {% if task.description %}- Description: {{ task.description }}{% endif %}

{% endfor %}

**NEXT STEPS**
Please review these tasks and update their status or completion dates as appropriate. If you need assistance or have questions about any of these tasks, please don't hesitate to reach out.

Best regards,
{{ sender_name }}

---
This is an automated reminder generated on {{ current_date }}.
            """,

            'summary_report': """
Subject: Weekly Overdue Tasks Report - {{ report_date }}

**OVERDUE TASKS SUMMARY REPORT**

Report Generated: {{ current_date }}
Total People with Overdue Tasks: {{ total_people }}
Total Overdue Tasks: {{ total_tasks }}

**BREAKDOWN BY PERSON**
{% for person, person_tasks in tasks_by_person.items() %}
**{{ person }}** ({{ person_tasks|length }} tasks)
{% for task in person_tasks %}
  - {{ task.title }} ({{ task.days_overdue }} days overdue)
{% endfor %}

{% endfor %}

**PRIORITY BREAKDOWN**
- Critical (30+ days): {{ critical_count }}
- High (14+ days): {{ high_count }}
- Medium (7+ days): {{ medium_count }}
- Low (< 7 days): {{ low_count }}

This report was automatically generated by the Task Management System.
            """
        }

    def generate_overdue_email(self,
                             overdue_tasks: pd.DataFrame,
                             recipient_name: str,
                             sender_name: str = "Task Management System") -> Dict[str, str]:
        """
        Generate an email for a specific person's overdue tasks

        Args:
            overdue_tasks: DataFrame containing overdue tasks for one person
            recipient_name: Name of the person receiving the email
            sender_name: Name of the sender

        Returns:
            Dictionary with 'subject' and 'body' keys
        """
        if overdue_tasks.empty:
            return {
                'subject': f"Good News - No Overdue Tasks!",
                'body': f"Dear {recipient_name},\n\nGreat news! You currently have no overdue tasks. Keep up the excellent work!\n\nBest regards,\n{sender_name}"
            }

        # Prepare template data
        template_data = {
            'recipient_name': recipient_name,
            'sender_name': sender_name,
            'current_date': datetime.now().strftime("%B %d, %Y"),
            'total_tasks': len(overdue_tasks),
            'critical_tasks': len(overdue_tasks[overdue_tasks['priority'] == 'CRITICAL']),
            'high_tasks': len(overdue_tasks[overdue_tasks['priority'] == 'HIGH']),
            'medium_tasks': len(overdue_tasks[overdue_tasks['priority'] == 'MEDIUM']),
            'low_tasks': len(overdue_tasks[overdue_tasks['priority'] == 'LOW']),
            'tasks': []
        }

        # Prepare task details
        for _, task in overdue_tasks.iterrows():
            task_info = {
                'title': task.get('Task', task.get('Title', 'Untitled Task')),
                'due_date': task.get('Due Date', task.get('Deadline', 'Unknown')),
                'days_overdue': task.get('days_overdue', 0),
                'priority': task.get('priority', 'UNKNOWN'),
                'description': task.get('Description', task.get('Notes', ''))
            }
            template_data['tasks'].append(task_info)

        # Render template
        template = Template(self.templates['overdue_tasks'])
        body = template.render(**template_data)

        # Generate subject
        priority_text = ""
        if template_data['critical_tasks'] > 0:
            priority_text = " - CRITICAL ITEMS"
        elif template_data['high_tasks'] > 0:
            priority_text = " - High Priority"

        subject = f"Overdue Tasks Reminder ({template_data['total_tasks']} items){priority_text}"

        return {
            'subject': subject,
            'body': body
        }

    def generate_summary_report(self, all_overdue_tasks: pd.DataFrame) -> Dict[str, str]:
        """Generate a summary report of all overdue tasks"""

        if all_overdue_tasks.empty:
            return {
                'subject': "Overdue Tasks Report - All Clear!",
                'body': f"No overdue tasks found as of {datetime.now().strftime('%B %d, %Y')}. Excellent work team!"
            }

        # Group tasks by person (assuming there's an assignee column)
        assignee_columns = [col for col in all_overdue_tasks.columns
                          if any(keyword in col.lower() for keyword in ['assign', 'owner', 'person', 'user'])]

        if assignee_columns:
            assignee_col = assignee_columns[0]
            tasks_by_person = {}
            for person in all_overdue_tasks[assignee_col].unique():
                person_tasks = all_overdue_tasks[all_overdue_tasks[assignee_col] == person]
                tasks_by_person[person] = []
                for _, task in person_tasks.iterrows():
                    tasks_by_person[person].append({
                        'title': task.get('Task', task.get('Title', 'Untitled')),
                        'days_overdue': task.get('days_overdue', 0)
                    })
        else:
            tasks_by_person = {'Unassigned': []}

        template_data = {
            'report_date': datetime.now().strftime("%B %d, %Y"),
            'current_date': datetime.now().strftime("%B %d, %Y at %I:%M %p"),
            'total_people': len(tasks_by_person),
            'total_tasks': len(all_overdue_tasks),
            'tasks_by_person': tasks_by_person,
            'critical_count': len(all_overdue_tasks[all_overdue_tasks['priority'] == 'CRITICAL']),
            'high_count': len(all_overdue_tasks[all_overdue_tasks['priority'] == 'HIGH']),
            'medium_count': len(all_overdue_tasks[all_overdue_tasks['priority'] == 'MEDIUM']),
            'low_count': len(all_overdue_tasks[all_overdue_tasks['priority'] == 'LOW']),
        }

        template = Template(self.templates['summary_report'])
        body = template.render(**template_data)

        subject = f"Weekly Overdue Tasks Report - {template_data['total_tasks']} Total Items"

        return {
            'subject': subject,
            'body': body
        }

def create_sample_task_data() -> pd.DataFrame:
    """Create sample task data for demonstration purposes"""

    # Create sample data with various overdue scenarios
    sample_data = [
        {
            'Task': 'Complete quarterly budget review',
            'Assignee': 'Alice Johnson',
            'Due Date': '2024-01-15',
            'Status': 'In Progress',
            'Priority': 'High',
            'Description': 'Review and approve Q1 budget allocations'
        },
        {
            'Task': 'Update employee handbook',
            'Assignee': 'Bob Smith',
            'Due Date': '2024-02-01',
            'Status': 'Not Started',
            'Priority': 'Medium',
            'Description': 'Incorporate new HR policies'
        },
        {
            'Task': 'Client presentation preparation',
            'Assignee': 'Alice Johnson',
            'Due Date': '2024-02-20',
            'Status': 'In Progress',
            'Priority': 'Critical',
            'Description': 'Prepare slides for major client meeting'
        },
        {
            'Task': 'Security audit completion',
            'Assignee': 'Charlie Brown',
            'Due Date': '2023-12-01',
            'Status': 'In Progress',
            'Priority': 'Critical',
            'Description': 'Complete annual security assessment'
        },
        {
            'Task': 'Training module development',
            'Assignee': 'Diana Prince',
            'Due Date': '2024-02-28',
            'Status': 'Planning',
            'Priority': 'Low',
            'Description': 'Create new employee onboarding materials'
        }
    ]

    return pd.DataFrame(sample_data)

def main():
    """
    Main function demonstrating the email formatter functionality
    """
    print("üìß LC Email Formatter - Educational Demo")
    print("=" * 50)

    # Initialize components
    scraper = TaskScraper()
    analyzer = TaskAnalyzer()
    formatter = EmailFormatter()

    # For demo purposes, we'll use sample data instead of scraping
    # In real usage, you would scrape from an actual website
    print("\nüìö Example 1: Creating sample task data...")
    task_df = create_sample_task_data()
    print(f"üìä Created sample data with {len(task_df)} tasks")
    print(task_df[['Task', 'Assignee', 'Due Date', 'Status']].to_string())

    # Analyze for overdue tasks
    print("\nüìö Example 2: Analyzing for overdue tasks...")
    overdue_df = analyzer.identify_overdue_tasks(
        task_df,
        due_date_column='Due Date',
        assignee_column='Assignee'
    )

    if not overdue_df.empty:
        print(f"üîç Analysis complete:")
        print(overdue_df[['Task', 'Assignee', 'Due Date', 'days_overdue', 'priority']].to_string())

        # Generate individual emails for each person
        print("\nüìö Example 3: Generating individual emails...")
        for assignee in overdue_df['Assignee'].unique():
            person_tasks = overdue_df[overdue_df['Assignee'] == assignee]

            email_content = formatter.generate_overdue_email(
                person_tasks,
                recipient_name=assignee,
                sender_name="LC Task Manager"
            )

            print(f"\n‚úâÔ∏è  Email for {assignee}:")
            print(f"Subject: {email_content['subject']}")
            print("=" * 40)
            print(email_content['body'][:500] + "..." if len(email_content['body']) > 500 else email_content['body'])

        # Generate summary report
        print("\nüìö Example 4: Generating summary report...")
        summary = formatter.generate_summary_report(overdue_df)
        print(f"\nüìã Summary Report:")
        print(f"Subject: {summary['subject']}")
        print("=" * 40)
        print(summary['body'][:500] + "..." if len(summary['body']) > 500 else summary['body'])

        # Save emails to files for review
        print("\nüìö Example 5: Saving emails to files...")
        output_dir = Path(__file__).parent / "generated_emails"
        output_dir.mkdir(exist_ok=True)

        # Save individual emails
        for assignee in overdue_df['Assignee'].unique():
            person_tasks = overdue_df[overdue_df['Assignee'] == assignee]
            email_content = formatter.generate_overdue_email(person_tasks, assignee)

            filename = f"overdue_tasks_{assignee.replace(' ', '_')}.txt"
            filepath = output_dir / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"Subject: {email_content['subject']}\n")
                f.write("=" * 50 + "\n")
                f.write(email_content['body'])

            print(f"üíæ Saved email for {assignee}: {filepath}")

        # Save summary report
        summary_file = output_dir / "summary_report.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Subject: {summary['subject']}\n")
            f.write("=" * 50 + "\n")
            f.write(summary['body'])

        print(f"üíæ Saved summary report: {summary_file}")

    else:
        print("‚úÖ No overdue tasks found - everyone is on track!")

    print(f"\nüéì Educational Notes:")
    print("1. Always validate scraped data before processing")
    print("2. Handle different date formats gracefully")
    print("3. Use templates for consistent email formatting")
    print("4. Implement rate limiting when scraping websites")
    print("5. Consider privacy and security when handling email data")
    print("6. Test email formatting with various data scenarios")
    print("7. Always respect website robots.txt and terms of service")

    # Demonstrate web scraping (commented out for safety)
    print(f"\nüåê Web Scraping Example (Demonstration Only):")
    print("# To scrape from a real website:")
    print("# scraped_data = scraper.scrape_task_table('https://example.com/tasks')")
    print("# overdue_tasks = analyzer.identify_overdue_tasks(scraped_data)")
    print("# Note: Always check robots.txt and respect rate limits!")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/MM_email_subscriptions.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "imaplib",
#     "email",
#     "pandas",
#     "python-dotenv",
#     "beautifulsoup4",
#     "re",
#     "sqlite3",
# ]
# ///

"""
MM_email_subscriptions.py - Email Subscription Scanner and Mailbox Analyzer

Student: MM (Initials)
Request: "scrap my email looking for subscriptions or provide a summary of my mail box during Out of Office"

This script demonstrates how to analyze email for subscriptions and generate mailbox summaries.
It includes comprehensive email processing, subscription detection, and Out of Office reporting.

Educational Focus:
- Email processing with IMAP
- Text analysis for subscription detection
- HTML email parsing
- Data analysis and reporting
- Email automation patterns
- Privacy and security considerations

Prerequisites:
- Email account with IMAP access enabled
- App-specific passwords for secure authentication
- Environment variables for email credentials

IMPORTANT SECURITY NOTE:
This script demonstrates email processing concepts using simulated data.
For real email access, always use app-specific passwords and secure authentication.
"""

import imaplib
import email
import re
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
from bs4 import BeautifulSoup
import json
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('email_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class EmailMessage:
    """Data class representing an email message."""
    message_id: str
    sender: str
    recipient: str
    subject: str
    date: datetime
    body_text: str
    body_html: str
    is_subscription: bool = False
    subscription_type: str = ""
    unsubscribe_links: List[str] = None
    importance: str = "normal"  # low, normal, high

    def __post_init__(self):
        if self.unsubscribe_links is None:
            self.unsubscribe_links = []

@dataclass
class SubscriptionInfo:
    """Data class representing subscription information."""
    sender_domain: str
    sender_name: str
    subscription_type: str
    frequency: str
    total_emails: int
    last_email_date: datetime
    unsubscribe_method: str
    unsubscribe_links: List[str]

class EmailSubscriptionAnalyzer:
    """
    Comprehensive email analyzer for subscription detection and mailbox summaries.

    This class demonstrates professional email processing patterns including:
    - IMAP email access
    - Subscription pattern detection
    - Email categorization
    - Out of Office reporting
    - Privacy-conscious data handling
    """

    def __init__(self, db_path: str = "email_analysis.db"):
        """
        Initialize the email analyzer.

        Args:
            db_path (str): Path to SQLite database for storing analysis results
        """
        self.db_path = db_path
        self.setup_database()

        # Subscription detection patterns
        self.subscription_patterns = {
            'newsletter': [
                r'newsletter', r'weekly update', r'monthly digest',
                r'news.*letter', r'update.*weekly', r'digest.*monthly'
            ],
            'marketing': [
                r'unsubscribe', r'promotional', r'offer', r'sale',
                r'discount', r'limited time', r'special deal'
            ],
            'notification': [
                r'notification', r'alert', r'reminder',
                r'account.*update', r'security.*alert'
            ],
            'social': [
                r'facebook', r'twitter', r'linkedin', r'instagram',
                r'social.*update', r'friend.*request', r'connection'
            ],
            'ecommerce': [
                r'order.*confirmation', r'shipment', r'delivery',
                r'purchase', r'invoice', r'receipt'
            ]
        }

        # Unsubscribe link patterns
        self.unsubscribe_patterns = [
            r'unsubscribe',
            r'opt.*out',
            r'remove.*from.*list',
            r'email.*preferences',
            r'manage.*subscription'
        ]

        # Important email patterns (for Out of Office filtering)
        self.important_patterns = [
            r'urgent', r'asap', r'important', r'critical',
            r'action.*required', r'time.*sensitive',
            r'please.*respond', r'need.*reply'
        ]

    def setup_database(self):
        """Initialize the SQLite database for storing email analysis results."""
        logger.info("üóÑÔ∏è Setting up email analysis database...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Email messages table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS email_messages (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        message_id TEXT UNIQUE,
                        sender TEXT,
                        recipient TEXT,
                        subject TEXT,
                        date DATETIME,
                        body_text TEXT,
                        body_html TEXT,
                        is_subscription BOOLEAN,
                        subscription_type TEXT,
                        importance TEXT,
                        analysis_date DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Subscriptions summary table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS subscriptions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        sender_domain TEXT,
                        sender_name TEXT,
                        subscription_type TEXT,
                        frequency TEXT,
                        total_emails INTEGER,
                        last_email_date DATETIME,
                        unsubscribe_method TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Unsubscribe links table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS unsubscribe_links (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        sender_domain TEXT,
                        link_url TEXT,
                        link_text TEXT,
                        discovered_date DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Out of office summary table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS ooo_summaries (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        summary_date DATE,
                        total_emails INTEGER,
                        important_emails INTEGER,
                        subscription_emails INTEGER,
                        unique_senders INTEGER,
                        top_senders TEXT,
                        urgent_subjects TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                conn.commit()
                logger.info("‚úÖ Database setup completed")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Database setup failed: {e}")
            raise

    def create_sample_emails(self, num_emails: int = 100) -> List[EmailMessage]:
        """
        Create sample email data for demonstration purposes.

        In a real implementation, this would connect to an actual email server.

        Args:
            num_emails (int): Number of sample emails to create

        Returns:
            List[EmailMessage]: List of sample email messages
        """
        logger.info(f"üìß Creating {num_emails} sample emails for demonstration...")

        sample_emails = []

        # Sample senders and subjects for different types
        senders_data = {
            'newsletter': [
                ('newsletter@techcrunch.com', 'TechCrunch Daily'),
                ('updates@github.com', 'GitHub Weekly Digest'),
                ('news@python.org', 'Python Newsletter'),
                ('updates@stackoverflow.com', 'Stack Overflow Weekly')
            ],
            'marketing': [
                ('offers@amazon.com', 'Amazon Prime Day Deals!'),
                ('sales@shopify.com', 'Limited Time Offer - 50% Off'),
                ('promotions@bestbuy.com', 'Black Friday Exclusive Deals'),
                ('deals@target.com', 'Weekend Sale - Up to 70% Off')
            ],
            'notification': [
                ('security@gmail.com', 'Security Alert: New Sign-in'),
                ('noreply@paypal.com', 'Payment Received'),
                ('alerts@bankofamerica.com', 'Account Statement Ready'),
                ('notifications@slack.com', 'You have 5 unread messages')
            ],
            'social': [
                ('noreply@facebook.com', 'John Doe commented on your post'),
                ('notifications@linkedin.com', 'You have 3 new connection requests'),
                ('updates@twitter.com', 'Weekly summary of your activity'),
                ('noreply@instagram.com', 'Sarah liked your photo')
            ],
            'work': [
                ('boss@company.com', 'Urgent: Q4 Budget Review Meeting'),
                ('hr@company.com', 'Benefits Enrollment Deadline'),
                ('client@bigcorp.com', 'Project Status Update Required'),
                ('team@company.com', 'Weekly Team Standup Notes')
            ]
        }

        import random

        for i in range(num_emails):
            # Choose email type
            email_type = random.choice(list(senders_data.keys()))
            sender_email, base_subject = random.choice(senders_data[email_type])

            # Generate email date (last 30 days)
            days_ago = random.randint(0, 30)
            email_date = datetime.now() - timedelta(days=days_ago)

            # Create email content
            if email_type in ['newsletter', 'marketing', 'social']:
                is_subscription = True
                subscription_type = email_type
                # Add unsubscribe content
                body_text = f"This is a {email_type} email.\n\nTo unsubscribe, click here: https://unsubscribe.example.com"
                body_html = f"<html><body><p>This is a {email_type} email.</p><p><a href='https://unsubscribe.example.com'>Unsubscribe</a></p></body></html>"
                unsubscribe_links = ["https://unsubscribe.example.com"]
            else:
                is_subscription = False
                subscription_type = ""
                body_text = f"This is a {email_type} email from work or notifications."
                body_html = f"<html><body><p>This is a {email_type} email from work or notifications.</p></body></html>"
                unsubscribe_links = []

            # Determine importance
            importance = "normal"
            if "urgent" in base_subject.lower() or "asap" in base_subject.lower():
                importance = "high"
            elif email_type == "marketing":
                importance = "low"

            email_msg = EmailMessage(
                message_id=f"<msg{i+1}@example.com>",
                sender=sender_email,
                recipient="mm@example.com",
                subject=f"{base_subject} #{i+1}",
                date=email_date,
                body_text=body_text,
                body_html=body_html,
                is_subscription=is_subscription,
                subscription_type=subscription_type,
                unsubscribe_links=unsubscribe_links,
                importance=importance
            )

            sample_emails.append(email_msg)

        logger.info(f"‚úÖ Created {len(sample_emails)} sample emails")
        return sample_emails

    def detect_subscription(self, email_msg: EmailMessage) -> Tuple[bool, str]:
        """
        Detect if an email is a subscription and categorize it.

        Args:
            email_msg (EmailMessage): Email message to analyze

        Returns:
            Tuple[bool, str]: (is_subscription, subscription_type)
        """
        # Combine subject and body for analysis
        content = f"{email_msg.subject} {email_msg.body_text}".lower()

        # Check for subscription patterns
        for sub_type, patterns in self.subscription_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    return True, sub_type

        # Check for unsubscribe links (strong indicator)
        for pattern in self.unsubscribe_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True, "newsletter"  # Default type if pattern found

        return False, ""

    def extract_unsubscribe_links(self, email_msg: EmailMessage) -> List[str]:
        """
        Extract unsubscribe links from email content.

        Args:
            email_msg (EmailMessage): Email message to analyze

        Returns:
            List[str]: List of unsubscribe URLs
        """
        links = []

        # Parse HTML content if available
        if email_msg.body_html:
            try:
                soup = BeautifulSoup(email_msg.body_html, 'html.parser')
                for link in soup.find_all('a', href=True):
                    link_text = link.get_text().lower()
                    link_url = link['href']

                    # Check if link text contains unsubscribe keywords
                    for pattern in self.unsubscribe_patterns:
                        if re.search(pattern, link_text, re.IGNORECASE):
                            links.append(link_url)
                            break

            except Exception as e:
                logger.warning(f"Failed to parse HTML content: {e}")

        # Also check plain text for URLs
        url_pattern = r'https?://[^\s<>"]+(?:unsubscribe|opt-out|preferences)[^\s<>"]*'
        text_links = re.findall(url_pattern, email_msg.body_text, re.IGNORECASE)
        links.extend(text_links)

        return list(set(links))  # Remove duplicates

    def analyze_email_importance(self, email_msg: EmailMessage) -> str:
        """
        Analyze email importance for Out of Office filtering.

        Args:
            email_msg (EmailMessage): Email message to analyze

        Returns:
            str: Importance level (low, normal, high)
        """
        content = f"{email_msg.subject} {email_msg.body_text}".lower()

        # Check for urgent/important patterns
        for pattern in self.important_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return "high"

        # Marketing emails are typically low importance
        if email_msg.is_subscription and email_msg.subscription_type == "marketing":
            return "low"

        # Default importance
        return "normal"

    def process_emails(self, emails: List[EmailMessage]):
        """
        Process a list of emails and store analysis results.

        Args:
            emails (List[EmailMessage]): List of emails to process
        """
        logger.info(f"üîç Processing {len(emails)} emails...")

        processed_count = 0
        subscription_count = 0

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                for email_msg in emails:
                    # Detect subscription
                    is_subscription, subscription_type = self.detect_subscription(email_msg)
                    email_msg.is_subscription = is_subscription
                    email_msg.subscription_type = subscription_type

                    # Extract unsubscribe links
                    if is_subscription:
                        email_msg.unsubscribe_links = self.extract_unsubscribe_links(email_msg)
                        subscription_count += 1

                    # Analyze importance
                    email_msg.importance = self.analyze_email_importance(email_msg)

                    # Store email in database
                    cursor.execute('''
                        INSERT OR REPLACE INTO email_messages
                        (message_id, sender, recipient, subject, date, body_text,
                         body_html, is_subscription, subscription_type, importance)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        email_msg.message_id,
                        email_msg.sender,
                        email_msg.recipient,
                        email_msg.subject,
                        email_msg.date,
                        email_msg.body_text,
                        email_msg.body_html,
                        email_msg.is_subscription,
                        email_msg.subscription_type,
                        email_msg.importance
                    ))

                    processed_count += 1

                conn.commit()

        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to process emails: {e}")
            raise

        logger.info(f"‚úÖ Processed {processed_count} emails, found {subscription_count} subscriptions")

    def analyze_subscriptions(self) -> List[SubscriptionInfo]:
        """
        Analyze subscription patterns from processed emails.

        Returns:
            List[SubscriptionInfo]: List of subscription analysis results
        """
        logger.info("üìä Analyzing subscription patterns...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                # Get subscription statistics by sender domain
                df_subscriptions = pd.read_sql_query('''
                    SELECT
                        SUBSTR(sender, INSTR(sender, '@') + 1) as sender_domain,
                        sender,
                        subscription_type,
                        COUNT(*) as total_emails,
                        MAX(date) as last_email_date,
                        MIN(date) as first_email_date
                    FROM email_messages
                    WHERE is_subscription = 1
                    GROUP BY sender_domain, subscription_type
                    ORDER BY total_emails DESC
                ''', conn)

                subscriptions = []

                for _, row in df_subscriptions.iterrows():
                    # Calculate frequency
                    first_date = pd.to_datetime(row['first_email_date'])
                    last_date = pd.to_datetime(row['last_email_date'])
                    days_span = (last_date - first_date).days
                    if days_span > 0:
                        frequency = f"{row['total_emails'] / days_span:.1f} emails/day"
                    else:
                        frequency = "Single email"

                    # Get unsubscribe links for this sender
                    unsubscribe_query = '''
                        SELECT DISTINCT body_text, body_html
                        FROM email_messages
                        WHERE sender LIKE ? AND is_subscription = 1
                        LIMIT 1
                    '''
                    result = conn.execute(unsubscribe_query, (f"%{row['sender_domain']}",)).fetchone()
                    unsubscribe_links = []
                    if result:
                        # This is simplified - in real implementation, would parse HTML
                        if 'unsubscribe' in result[0].lower():
                            unsubscribe_links = ['Unsubscribe link found in email']

                    subscription_info = SubscriptionInfo(
                        sender_domain=row['sender_domain'],
                        sender_name=row['sender'].split('@')[0],
                        subscription_type=row['subscription_type'],
                        frequency=frequency,
                        total_emails=row['total_emails'],
                        last_email_date=pd.to_datetime(row['last_email_date']),
                        unsubscribe_method="Link" if unsubscribe_links else "Manual",
                        unsubscribe_links=unsubscribe_links
                    )

                    subscriptions.append(subscription_info)

                # Store subscription summaries
                cursor = conn.cursor()
                cursor.execute('DELETE FROM subscriptions')  # Clear old data

                for sub in subscriptions:
                    cursor.execute('''
                        INSERT INTO subscriptions
                        (sender_domain, sender_name, subscription_type, frequency,
                         total_emails, last_email_date, unsubscribe_method)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        sub.sender_domain,
                        sub.sender_name,
                        sub.subscription_type,
                        sub.frequency,
                        sub.total_emails,
                        sub.last_email_date,
                        sub.unsubscribe_method
                    ))

                conn.commit()

        except Exception as e:
            logger.error(f"‚ùå Failed to analyze subscriptions: {e}")
            return []

        logger.info(f"‚úÖ Analyzed {len(subscriptions)} subscription sources")
        return subscriptions

    def generate_subscription_report(self) -> str:
        """
        Generate a comprehensive subscription report.

        Returns:
            str: Formatted subscription report
        """
        logger.info("üìã Generating subscription report...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                # Get subscription summary
                df_summary = pd.read_sql_query('''
                    SELECT
                        subscription_type,
                        COUNT(*) as source_count,
                        SUM(total_emails) as total_emails
                    FROM subscriptions
                    GROUP BY subscription_type
                    ORDER BY total_emails DESC
                ''', conn)

                # Get top senders
                df_top_senders = pd.read_sql_query('''
                    SELECT sender_domain, subscription_type, total_emails, frequency
                    FROM subscriptions
                    ORDER BY total_emails DESC
                    LIMIT 10
                ''', conn)

                # Get recent subscriptions
                df_recent = pd.read_sql_query('''
                    SELECT sender_domain, subscription_type, last_email_date
                    FROM subscriptions
                    ORDER BY last_email_date DESC
                    LIMIT 5
                ''', conn)

                report = []
                report.append("üìß EMAIL SUBSCRIPTION ANALYSIS REPORT")
                report.append("=" * 50)

                if not df_summary.empty:
                    report.append("\nüìä Subscription Summary by Type:")
                    for _, row in df_summary.iterrows():
                        report.append(f"  {row['subscription_type'].title()}: {row['source_count']} sources, {row['total_emails']} emails")

                if not df_top_senders.empty:
                    report.append(f"\nüîù Top 10 Subscription Sources:")
                    for _, row in df_top_senders.iterrows():
                        report.append(f"  {row['sender_domain']} ({row['subscription_type']}) - {row['total_emails']} emails ({row['frequency']})")

                if not df_recent.empty:
                    report.append(f"\nüìÖ Most Recent Activity:")
                    for _, row in df_recent.iterrows():
                        report.append(f"  {row['sender_domain']} - Last email: {row['last_email_date']}")

                # Get unsubscribe recommendations
                df_heavy = pd.read_sql_query('''
                    SELECT sender_domain, total_emails, unsubscribe_method
                    FROM subscriptions
                    WHERE total_emails > 5
                    ORDER BY total_emails DESC
                ''', conn)

                if not df_heavy.empty:
                    report.append(f"\nüí° Unsubscribe Recommendations (High Volume):")
                    for _, row in df_heavy.iterrows():
                        status = "‚úÖ Easy" if row['unsubscribe_method'] == "Link" else "‚ö†Ô∏è Manual"
                        report.append(f"  {row['sender_domain']} ({row['total_emails']} emails) - {status}")

                return "\n".join(report)

        except Exception as e:
            logger.error(f"‚ùå Failed to generate subscription report: {e}")
            return "Failed to generate subscription report"

    def generate_ooo_summary(self, days_back: int = 7) -> str:
        """
        Generate an Out of Office summary for the specified period.

        Args:
            days_back (int): Number of days to analyze

        Returns:
            str: Formatted OOO summary
        """
        logger.info(f"üèñÔ∏è Generating Out of Office summary for last {days_back} days...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cutoff_date = datetime.now() - timedelta(days=days_back)

                # Get overall statistics
                stats_query = '''
                    SELECT
                        COUNT(*) as total_emails,
                        SUM(CASE WHEN importance = 'high' THEN 1 ELSE 0 END) as important_emails,
                        SUM(CASE WHEN is_subscription = 1 THEN 1 ELSE 0 END) as subscription_emails,
                        COUNT(DISTINCT sender) as unique_senders
                    FROM email_messages
                    WHERE date >= ?
                '''
                stats = conn.execute(stats_query, (cutoff_date,)).fetchone()

                # Get top senders
                top_senders_query = '''
                    SELECT sender, COUNT(*) as email_count
                    FROM email_messages
                    WHERE date >= ? AND is_subscription = 0
                    GROUP BY sender
                    ORDER BY email_count DESC
                    LIMIT 5
                '''
                df_top_senders = pd.read_sql_query(top_senders_query, conn, params=(cutoff_date,))

                # Get urgent emails
                urgent_query = '''
                    SELECT sender, subject, date
                    FROM email_messages
                    WHERE date >= ? AND importance = 'high'
                    ORDER BY date DESC
                    LIMIT 10
                '''
                df_urgent = pd.read_sql_query(urgent_query, conn, params=(cutoff_date,))

                # Get daily email counts
                daily_query = '''
                    SELECT DATE(date) as email_date, COUNT(*) as count
                    FROM email_messages
                    WHERE date >= ?
                    GROUP BY DATE(date)
                    ORDER BY email_date DESC
                '''
                df_daily = pd.read_sql_query(daily_query, conn, params=(cutoff_date,))

                # Store OOO summary
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO ooo_summaries
                    (summary_date, total_emails, important_emails, subscription_emails,
                     unique_senders, top_senders, urgent_subjects)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    datetime.now().date(),
                    stats[0],
                    stats[1],
                    stats[2],
                    stats[3],
                    df_top_senders.to_json() if not df_top_senders.empty else "{}",
                    df_urgent.to_json() if not df_urgent.empty else "{}"
                ))
                conn.commit()

                # Generate report
                report = []
                report.append(f"üèñÔ∏è OUT OF OFFICE EMAIL SUMMARY")
                report.append(f"üìÖ Period: Last {days_back} days")
                report.append("=" * 50)

                report.append(f"\nüìä Overview:")
                report.append(f"  Total emails: {stats[0]}")
                report.append(f"  Important emails: {stats[1]} ({stats[1]/stats[0]*100:.1f}%)" if stats[0] > 0 else "  Important emails: 0")
                report.append(f"  Subscription emails: {stats[2]} ({stats[2]/stats[0]*100:.1f}%)" if stats[0] > 0 else "  Subscription emails: 0")
                report.append(f"  Unique senders: {stats[3]}")

                if not df_daily.empty:
                    report.append(f"\nüìà Daily Breakdown:")
                    for _, row in df_daily.iterrows():
                        report.append(f"  {row['email_date']}: {row['count']} emails")

                if not df_top_senders.empty:
                    report.append(f"\nüë• Top Non-Subscription Senders:")
                    for _, row in df_top_senders.iterrows():
                        report.append(f"  {row['sender']}: {row['email_count']} emails")

                if not df_urgent.empty:
                    report.append(f"\nüö® Important/Urgent Emails:")
                    for _, row in df_urgent.iterrows():
                        report.append(f"  üìß {row['sender']}")
                        report.append(f"     Subject: {row['subject']}")
                        report.append(f"     Date: {row['date']}")
                        report.append("")

                non_subscription_count = stats[0] - stats[2]
                report.append(f"\nüí° Action Items:")
                report.append(f"  ‚Ä¢ Review {stats[1]} important emails requiring attention")
                report.append(f"  ‚Ä¢ {non_subscription_count} work-related emails to process")
                if stats[2] > 0:
                    report.append(f"  ‚Ä¢ Consider unsubscribing from high-volume sources")

                return "\n".join(report)

        except Exception as e:
            logger.error(f"‚ùå Failed to generate OOO summary: {e}")
            return "Failed to generate OOO summary"

    def export_unsubscribe_list(self, output_file: str = "unsubscribe_list.csv"):
        """
        Export a list of subscriptions with unsubscribe information.

        Args:
            output_file (str): Output CSV file path
        """
        logger.info(f"üì§ Exporting unsubscribe list to {output_file}...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                df = pd.read_sql_query('''
                    SELECT
                        sender_domain,
                        sender_name,
                        subscription_type,
                        total_emails,
                        frequency,
                        last_email_date,
                        unsubscribe_method
                    FROM subscriptions
                    ORDER BY total_emails DESC
                ''', conn)

                if not df.empty:
                    df.to_csv(output_file, index=False)
                    logger.info(f"‚úÖ Exported {len(df)} subscriptions to {output_file}")
                else:
                    logger.warning("‚ö†Ô∏è No subscription data to export")

        except Exception as e:
            logger.error(f"‚ùå Failed to export unsubscribe list: {e}")

def demonstrate_email_security_patterns():
    """
    Demonstrate email security and privacy best practices.
    """
    print("\nüîê Email Security & Privacy Best Practices:")
    print("=" * 50)

    practices = {
        "Authentication": [
            "Use app-specific passwords, not main account password",
            "Enable 2FA on email accounts",
            "Use OAuth2 when available instead of basic auth"
        ],
        "Data Privacy": [
            "Process emails locally, avoid cloud storage of content",
            "Implement data retention policies",
            "Encrypt sensitive data at rest and in transit"
        ],
        "Access Control": [
            "Limit IMAP permissions to read-only when possible",
            "Use dedicated service accounts for automation",
            "Log all email access for audit purposes"
        ],
        "Content Handling": [
            "Sanitize HTML content to prevent XSS",
            "Validate all extracted URLs before processing",
            "Be careful with email attachments (scan for malware)"
        ]
    }

    for category, items in practices.items():
        print(f"\nüõ°Ô∏è {category}:")
        for item in items:
            print(f"  ‚Ä¢ {item}")

def demonstrate_real_world_integration():
    """
    Show examples of real-world email integration scenarios.
    """
    print("\nüåç Real-World Integration Examples:")
    print("=" * 50)

    scenarios = {
        "Email Service Providers": {
            "Gmail": "Use Gmail API with OAuth2 authentication",
            "Outlook": "Use Microsoft Graph API",
            "Yahoo": "Use IMAP with app passwords",
            "Corporate Exchange": "Use Exchange Web Services (EWS)"
        },
        "Automation Scenarios": {
            "Vacation Responder": "Auto-categorize and respond to emails",
            "Email Forwarding": "Forward important emails to mobile",
            "Cleanup Automation": "Auto-delete old newsletters",
            "Subscription Management": "Bulk unsubscribe from inactive lists"
        },
        "Integration Tools": {
            "Zapier": "Connect email to other business tools",
            "IFTTT": "Simple email automation rules",
            "Microsoft Power Automate": "Enterprise email workflows",
            "Custom Scripts": "Tailored automation for specific needs"
        }
    }

    for category, items in scenarios.items():
        print(f"\nüîß {category}:")
        for name, description in items.items():
            print(f"  ‚Ä¢ {name}: {description}")

def main():
    """
    Main function demonstrating the email subscription analysis workflow.
    """
    print("üöÄ Welcome to MM's Email Subscription Analyzer!")
    print("=" * 50)

    try:
        # Initialize the email analyzer
        analyzer = EmailSubscriptionAnalyzer("mm_email_analysis.db")

        print("\n1. üìß Creating sample email data (simulating email processing)...")
        # In a real implementation, this would connect to an email server
        sample_emails = analyzer.create_sample_emails(num_emails=150)

        print("\n2. üîç Processing emails for subscription detection...")
        analyzer.process_emails(sample_emails)

        print("\n3. üìä Analyzing subscription patterns...")
        subscriptions = analyzer.analyze_subscriptions()

        print("\n4. üìã Generating subscription report...")
        subscription_report = analyzer.generate_subscription_report()
        print(subscription_report)

        print("\n5. üèñÔ∏è Generating Out of Office summary...")
        ooo_summary = analyzer.generate_ooo_summary(days_back=7)
        print(ooo_summary)

        print("\n6. üì§ Exporting unsubscribe list...")
        analyzer.export_unsubscribe_list("mm_unsubscribe_list.csv")

        print("\n‚úÖ Email analysis completed successfully!")

        # Show database file info
        db_path = Path("mm_email_analysis.db")
        if db_path.exists():
            size_mb = db_path.stat().st_size / (1024 * 1024)
            print(f"\nüìÅ Database file: {db_path.absolute()}")
            print(f"   Size: {size_mb:.2f} MB")

        print(f"\nüìä Analysis Results:")
        print(f"   Found {len(subscriptions)} subscription sources")
        print(f"   Processed {len(sample_emails)} total emails")

    except Exception as e:
        logger.error(f"‚ùå Email analysis failed: {e}")
        print("üí° Troubleshooting tips:")
        print("   - Ensure you have write permissions in the current directory")
        print("   - For real email access, set up app-specific passwords")
        print("   - Check email server IMAP settings")
        print("   - Verify firewall allows IMAP connections")

    # Show security and integration patterns
    demonstrate_email_security_patterns()
    demonstrate_real_world_integration()

    print("\nüéì Learning Summary:")
    print("- Email processing with IMAP protocols")
    print("- Text analysis for pattern detection")
    print("- HTML parsing for link extraction")
    print("- Database design for email analytics")
    print("- Privacy and security considerations")
    print("- Out of Office automation concepts")

    print("\nüí° Next Steps:")
    print("- Set up real email server connections")
    print("- Implement OAuth2 authentication")
    print("- Add machine learning for better categorization")
    print("- Create automated unsubscribe workflows")
    print("- Build email scheduling and forwarding rules")
    print("- Integrate with calendar for vacation detection")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/MM_powerpoint_generator.py

```python
#!/usr/bin/env python3
# /// script
# dependencies = ["python-pptx", "anthropic", "python-dotenv", "pandas", "matplotlib", "requests", "pillow"]
# ///

"""
MM_powerpoint_generator.py
Student: MM - "powerpoint generation"

This script demonstrates automated PowerPoint presentation generation from data,
text, and various content sources using AI for content creation and formatting.

Key Learning Objectives:
- Automated presentation creation
- Data visualization integration
- AI-powered content generation
- Template-based design automation
- Multi-source content integration
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import List, Dict, Any, Optional
from io import BytesIO
from dotenv import load_dotenv
import anthropic

# PowerPoint manipulation library
from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.enum.text import PP_ALIGN, MSO_ANCHOR
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE

# Load environment variables
load_dotenv()

class PowerPointGenerator:
    """
    A class to automatically generate PowerPoint presentations from various data sources.
    This demonstrates automated slide creation, data visualization, and AI-powered content.
    """

    def __init__(self):
        """Initialize the PowerPoint generator with AI client."""
        # Initialize Anthropic client for AI-powered content generation
        self.ai_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # Define presentation themes and color schemes
        self.themes = {
            "corporate": {
                "primary_color": RGBColor(0, 51, 102),      # Dark blue
                "secondary_color": RGBColor(255, 255, 255),  # White
                "accent_color": RGBColor(0, 153, 204),       # Light blue
                "text_color": RGBColor(51, 51, 51)           # Dark gray
            },
            "modern": {
                "primary_color": RGBColor(45, 45, 45),       # Charcoal
                "secondary_color": RGBColor(255, 255, 255),  # White
                "accent_color": RGBColor(255, 87, 51),       # Orange
                "text_color": RGBColor(85, 85, 85)           # Gray
            },
            "minimal": {
                "primary_color": RGBColor(34, 34, 34),       # Dark gray
                "secondary_color": RGBColor(248, 248, 248),  # Light gray
                "accent_color": RGBColor(102, 204, 0),       # Green
                "text_color": RGBColor(68, 68, 68)           # Medium gray
            }
        }

        # Slide layout types
        self.layout_types = {
            "title": 0,
            "title_content": 1,
            "section_header": 2,
            "two_content": 3,
            "comparison": 4,
            "title_only": 5,
            "blank": 6,
            "content_caption": 7,
            "picture_caption": 8
        }

    def get_sample_presentation_data(self) -> Dict[str, Any]:
        """
        Generate sample data for presentation creation.
        In a real scenario, this would come from databases, APIs, or user input.
        """
        return {
            "title": "Q4 2024 Business Performance Review",
            "subtitle": "Strategic Insights and Future Outlook",
            "author": "Data Analytics Team",
            "company": "TechCorp Solutions",
            "date": datetime.now().strftime("%B %d, %Y"),
            "theme": "corporate",
            "sections": [
                {
                    "title": "Executive Summary",
                    "type": "overview",
                    "content": {
                        "key_points": [
                            "Revenue increased 25% year-over-year",
                            "Customer satisfaction reached 92%",
                            "Expanded into 3 new international markets",
                            "Launched 2 major product features"
                        ],
                        "metrics": {
                            "revenue": "$2.4M",
                            "customers": "15,000+",
                            "growth_rate": "25%",
                            "market_share": "18%"
                        }
                    }
                },
                {
                    "title": "Financial Performance",
                    "type": "data_analysis",
                    "content": {
                        "revenue_data": {
                            "Q1": 1800000,
                            "Q2": 2100000,
                            "Q3": 2200000,
                            "Q4": 2400000
                        },
                        "expense_data": {
                            "Q1": 1200000,
                            "Q2": 1300000,
                            "Q3": 1350000,
                            "Q4": 1400000
                        },
                        "profit_margin": {
                            "Q1": 33.3,
                            "Q2": 38.1,
                            "Q3": 38.6,
                            "Q4": 41.7
                        }
                    }
                },
                {
                    "title": "Customer Analytics",
                    "type": "insights",
                    "content": {
                        "satisfaction_scores": {
                            "Product Quality": 4.5,
                            "Customer Service": 4.3,
                            "Pricing": 4.1,
                            "User Experience": 4.6,
                            "Support Response": 4.2
                        },
                        "demographics": {
                            "Enterprise": 45,
                            "Mid-Market": 35,
                            "Small Business": 20
                        }
                    }
                },
                {
                    "title": "Market Opportunities",
                    "type": "strategic",
                    "content": {
                        "opportunities": [
                            "AI integration features showing 40% user interest",
                            "Mobile app expansion with 60% customer demand",
                            "European market entry projecting $1M additional revenue",
                            "Partnership opportunities with 3 major platforms"
                        ],
                        "challenges": [
                            "Increased competition in core market",
                            "Rising development costs",
                            "Talent acquisition in AI/ML space",
                            "Regulatory compliance in new markets"
                        ]
                    }
                },
                {
                    "title": "2025 Strategic Roadmap",
                    "type": "roadmap",
                    "content": {
                        "q1_goals": [
                            "Launch AI-powered analytics dashboard",
                            "Complete European market entry",
                            "Hire 5 additional engineers"
                        ],
                        "q2_goals": [
                            "Mobile app beta release",
                            "Partnership integration completion",
                            "Customer success program expansion"
                        ],
                        "annual_targets": {
                            "revenue_goal": "$3.5M",
                            "customer_goal": "25,000",
                            "market_expansion": "2 new regions"
                        }
                    }
                }
            ]
        }

    def generate_content_with_ai(self, section_data: Dict[str, Any], context: str) -> Dict[str, Any]:
        """
        Use AI to generate and enhance presentation content.

        Args:
            section_data: Raw section data
            context: Additional context about the presentation

        Returns:
            Enhanced content with AI-generated text and insights
        """

        section_title = section_data.get("title", "Untitled Section")
        section_content = section_data.get("content", {})

        # Create AI prompt for content enhancement
        prompt = f"""
        Create engaging presentation content for a business slide titled "{section_title}".

        Context: {context}

        Section Data: {json.dumps(section_content, indent=2)}

        Generate:
        1. A compelling slide title (if different from current)
        2. 3-5 bullet points highlighting key insights
        3. A brief narrative summary (2-3 sentences)
        4. Suggested talking points for the presenter
        5. Key takeaways for the audience

        Focus on:
        - Clear, actionable insights
        - Business impact and implications
        - Data-driven conclusions
        - Professional tone suitable for executives

        Respond in JSON format:
        {{
            "enhanced_title": "slide title",
            "key_insights": ["insight1", "insight2", "insight3"],
            "narrative": "summary paragraph",
            "talking_points": ["point1", "point2", "point3"],
            "key_takeaways": ["takeaway1", "takeaway2"],
            "recommended_visuals": ["chart_type1", "chart_type2"]
        }}
        """

        try:
            # Make API call to Anthropic
            response = self.ai_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )

            # Parse the AI response
            ai_content = json.loads(response.content[0].text)
            return ai_content

        except Exception as e:
            print(f"AI content generation failed for {section_title}: {str(e)}")
            # Fallback to basic content
            return {
                "enhanced_title": section_title,
                "key_insights": ["Data analysis completed", "Key metrics identified", "Insights extracted"],
                "narrative": f"This section covers {section_title.lower()} with relevant data and analysis.",
                "talking_points": ["Review the data", "Discuss implications", "Plan next steps"],
                "key_takeaways": ["Important metrics tracked", "Progress measured"],
                "recommended_visuals": ["bar_chart", "line_chart"]
            }

    def create_chart_from_data(self, chart_data: Dict[str, Any], chart_type: str = "bar") -> str:
        """
        Create a chart from data and save as image file.

        Args:
            chart_data: Data for chart creation
            chart_type: Type of chart (bar, line, pie)

        Returns:
            Filename of the created chart image
        """

        plt.figure(figsize=(10, 6))
        plt.style.use('seaborn-v0_8')  # Modern chart styling

        filename = f"chart_{datetime.now().strftime('%H%M%S')}.png"

        if chart_type == "bar":
            # Create bar chart
            keys = list(chart_data.keys())
            values = list(chart_data.values())
            bars = plt.bar(keys, values, color=['#003366', '#0099CC', '#66CCFF', '#99DDFF'])

            # Customize appearance
            plt.title("Performance Metrics", fontsize=16, fontweight='bold', pad=20)
            plt.ylabel("Values", fontsize=12)
            plt.xticks(rotation=45, ha='right')

            # Add value labels on bars
            for bar, value in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                        f'{value:,.0f}' if isinstance(value, (int, float)) else str(value),
                        ha='center', va='bottom', fontweight='bold')

        elif chart_type == "line":
            # Create line chart
            keys = list(chart_data.keys())
            values = list(chart_data.values())
            plt.plot(keys, values, marker='o', linewidth=3, markersize=8, color='#003366')

            plt.title("Trend Analysis", fontsize=16, fontweight='bold', pad=20)
            plt.ylabel("Values", fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3)

        elif chart_type == "pie":
            # Create pie chart
            keys = list(chart_data.keys())
            values = list(chart_data.values())
            colors = ['#003366', '#0099CC', '#66CCFF', '#99DDFF', '#CCE6FF']

            plt.pie(values, labels=keys, autopct='%1.1f%%', startangle=90,
                   colors=colors[:len(keys)])
            plt.title("Distribution Analysis", fontsize=16, fontweight='bold', pad=20)

        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        return filename

    def create_presentation(self, presentation_data: Dict[str, Any]) -> str:
        """
        Create a complete PowerPoint presentation from structured data.

        Args:
            presentation_data: Complete presentation data structure

        Returns:
            Filename of the created presentation
        """

        print("üé® Creating PowerPoint presentation...")

        # Create new presentation
        prs = Presentation()

        # Get theme colors
        theme = self.themes.get(presentation_data.get("theme", "corporate"))

        # Set presentation properties
        prs.core_properties.title = presentation_data["title"]
        prs.core_properties.author = presentation_data["author"]
        prs.core_properties.created = datetime.now()

        # Create title slide
        self._create_title_slide(prs, presentation_data, theme)

        # Create agenda slide
        self._create_agenda_slide(prs, presentation_data, theme)

        # Process each section
        for i, section in enumerate(presentation_data["sections"]):
            print(f"üìä Processing section: {section['title']}")

            # Generate AI-enhanced content
            context = f"Business presentation for {presentation_data['company']} on {presentation_data['title']}"
            enhanced_content = self.generate_content_with_ai(section, context)

            # Create section slides based on type
            if section["type"] == "overview":
                self._create_overview_slide(prs, section, enhanced_content, theme)

            elif section["type"] == "data_analysis":
                self._create_data_slide(prs, section, enhanced_content, theme)

            elif section["type"] == "insights":
                self._create_insights_slide(prs, section, enhanced_content, theme)

            elif section["type"] == "strategic":
                self._create_strategic_slide(prs, section, enhanced_content, theme)

            elif section["type"] == "roadmap":
                self._create_roadmap_slide(prs, section, enhanced_content, theme)

        # Create conclusion slide
        self._create_conclusion_slide(prs, presentation_data, theme)

        # Save presentation
        filename = f"generated_presentation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx"
        prs.save(filename)

        print(f"‚úÖ Presentation saved as: {filename}")
        return filename

    def _create_title_slide(self, prs: Presentation, data: Dict[str, Any], theme: Dict[str, Any]):
        """Create the title slide."""
        slide_layout = prs.slide_layouts[self.layout_types["title"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = data["title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(44)
        title.text_frame.paragraphs[0].font.bold = True

        # Set subtitle
        subtitle = slide.placeholders[1]
        subtitle.text = f"{data['subtitle']}\n\n{data['company']}\n{data['date']}"
        for paragraph in subtitle.text_frame.paragraphs:
            paragraph.font.color.rgb = theme["text_color"]
            paragraph.font.size = Pt(20)

    def _create_agenda_slide(self, prs: Presentation, data: Dict[str, Any], theme: Dict[str, Any]):
        """Create an agenda slide."""
        slide_layout = prs.slide_layouts[self.layout_types["title_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = "Agenda"
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(36)
        title.text_frame.paragraphs[0].font.bold = True

        # Add agenda items
        content = slide.placeholders[1]
        text_frame = content.text_frame
        text_frame.clear()

        for i, section in enumerate(data["sections"], 1):
            p = text_frame.add_paragraph()
            p.text = f"{i}. {section['title']}"
            p.font.size = Pt(24)
            p.font.color.rgb = theme["text_color"]
            p.space_after = Pt(12)

    def _create_overview_slide(self, prs: Presentation, section: Dict[str, Any],
                             enhanced_content: Dict[str, Any], theme: Dict[str, Any]):
        """Create an overview slide with key metrics."""
        slide_layout = prs.slide_layouts[self.layout_types["two_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = enhanced_content["enhanced_title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(32)
        title.text_frame.paragraphs[0].font.bold = True

        # Left content: Key insights
        left_content = slide.placeholders[1]
        text_frame = left_content.text_frame
        text_frame.clear()

        # Add heading
        p = text_frame.add_paragraph()
        p.text = "Key Insights"
        p.font.size = Pt(20)
        p.font.bold = True
        p.font.color.rgb = theme["accent_color"]

        # Add insights
        for insight in enhanced_content["key_insights"]:
            p = text_frame.add_paragraph()
            p.text = f"‚Ä¢ {insight}"
            p.font.size = Pt(16)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

        # Right content: Metrics
        right_content = slide.placeholders[2]
        metrics = section["content"].get("metrics", {})

        if metrics:
            text_frame = right_content.text_frame
            text_frame.clear()

            # Add heading
            p = text_frame.add_paragraph()
            p.text = "Key Metrics"
            p.font.size = Pt(20)
            p.font.bold = True
            p.font.color.rgb = theme["accent_color"]

            # Add metrics
            for metric, value in metrics.items():
                p = text_frame.add_paragraph()
                p.text = f"{metric.replace('_', ' ').title()}: {value}"
                p.font.size = Pt(18)
                p.font.color.rgb = theme["text_color"]
                p.font.bold = True
                p.level = 1

    def _create_data_slide(self, prs: Presentation, section: Dict[str, Any],
                          enhanced_content: Dict[str, Any], theme: Dict[str, Any]):
        """Create a data analysis slide with charts."""
        slide_layout = prs.slide_layouts[self.layout_types["title_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = enhanced_content["enhanced_title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(32)
        title.text_frame.paragraphs[0].font.bold = True

        # Create and insert chart
        revenue_data = section["content"].get("revenue_data", {})
        if revenue_data:
            chart_filename = self.create_chart_from_data(revenue_data, "line")

            # Add chart to slide
            left = Inches(1)
            top = Inches(2)
            width = Inches(8)
            height = Inches(4.5)

            try:
                slide.shapes.add_picture(chart_filename, left, top, width, height)
                # Clean up chart file
                os.remove(chart_filename)
            except Exception as e:
                print(f"Could not add chart to slide: {e}")

        # Add narrative text
        left = Inches(1)
        top = Inches(6.8)
        width = Inches(8)
        height = Inches(1)

        textbox = slide.shapes.add_textbox(left, top, width, height)
        text_frame = textbox.text_frame
        p = text_frame.paragraphs[0]
        p.text = enhanced_content["narrative"]
        p.font.size = Pt(14)
        p.font.color.rgb = theme["text_color"]

    def _create_insights_slide(self, prs: Presentation, section: Dict[str, Any],
                             enhanced_content: Dict[str, Any], theme: Dict[str, Any]):
        """Create an insights slide with customer data visualization."""
        slide_layout = prs.slide_layouts[self.layout_types["two_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = enhanced_content["enhanced_title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(32)
        title.text_frame.paragraphs[0].font.bold = True

        # Left content: Key insights
        left_content = slide.placeholders[1]
        text_frame = left_content.text_frame
        text_frame.clear()

        for insight in enhanced_content["key_insights"]:
            p = text_frame.add_paragraph()
            p.text = f"‚Ä¢ {insight}"
            p.font.size = Pt(16)
            p.font.color.rgb = theme["text_color"]

        # Right content: Customer satisfaction scores
        satisfaction_scores = section["content"].get("satisfaction_scores", {})
        if satisfaction_scores:
            right_content = slide.placeholders[2]
            text_frame = right_content.text_frame
            text_frame.clear()

            # Add heading
            p = text_frame.add_paragraph()
            p.text = "Satisfaction Scores"
            p.font.size = Pt(18)
            p.font.bold = True
            p.font.color.rgb = theme["accent_color"]

            # Add scores
            for category, score in satisfaction_scores.items():
                p = text_frame.add_paragraph()
                stars = "‚òÖ" * int(score) + "‚òÜ" * (5 - int(score))
                p.text = f"{category}: {score}/5 {stars}"
                p.font.size = Pt(14)
                p.font.color.rgb = theme["text_color"]
                p.level = 1

    def _create_strategic_slide(self, prs: Presentation, section: Dict[str, Any],
                              enhanced_content: Dict[str, Any], theme: Dict[str, Any]):
        """Create a strategic opportunities slide."""
        slide_layout = prs.slide_layouts[self.layout_types["two_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = enhanced_content["enhanced_title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(32)
        title.text_frame.paragraphs[0].font.bold = True

        # Left content: Opportunities
        left_content = slide.placeholders[1]
        text_frame = left_content.text_frame
        text_frame.clear()

        # Add heading
        p = text_frame.add_paragraph()
        p.text = "Opportunities"
        p.font.size = Pt(20)
        p.font.bold = True
        p.font.color.rgb = RGBColor(0, 153, 0)  # Green

        opportunities = section["content"].get("opportunities", [])
        for opportunity in opportunities:
            p = text_frame.add_paragraph()
            p.text = f"+ {opportunity}"
            p.font.size = Pt(14)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

        # Right content: Challenges
        right_content = slide.placeholders[2]
        text_frame = right_content.text_frame
        text_frame.clear()

        # Add heading
        p = text_frame.add_paragraph()
        p.text = "Challenges"
        p.font.size = Pt(20)
        p.font.bold = True
        p.font.color.rgb = RGBColor(204, 51, 0)  # Red

        challenges = section["content"].get("challenges", [])
        for challenge in challenges:
            p = text_frame.add_paragraph()
            p.text = f"- {challenge}"
            p.font.size = Pt(14)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

    def _create_roadmap_slide(self, prs: Presentation, section: Dict[str, Any],
                            enhanced_content: Dict[str, Any], theme: Dict[str, Any]):
        """Create a strategic roadmap slide."""
        slide_layout = prs.slide_layouts[self.layout_types["title_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = enhanced_content["enhanced_title"]
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(32)
        title.text_frame.paragraphs[0].font.bold = True

        # Content area
        content = slide.placeholders[1]
        text_frame = content.text_frame
        text_frame.clear()

        # Q1 Goals
        p = text_frame.add_paragraph()
        p.text = "Q1 2025 Goals"
        p.font.size = Pt(18)
        p.font.bold = True
        p.font.color.rgb = theme["accent_color"]

        q1_goals = section["content"].get("q1_goals", [])
        for goal in q1_goals:
            p = text_frame.add_paragraph()
            p.text = f"‚Ä¢ {goal}"
            p.font.size = Pt(14)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

        # Q2 Goals
        p = text_frame.add_paragraph()
        p.text = "Q2 2025 Goals"
        p.font.size = Pt(18)
        p.font.bold = True
        p.font.color.rgb = theme["accent_color"]

        q2_goals = section["content"].get("q2_goals", [])
        for goal in q2_goals:
            p = text_frame.add_paragraph()
            p.text = f"‚Ä¢ {goal}"
            p.font.size = Pt(14)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

        # Annual Targets
        p = text_frame.add_paragraph()
        p.text = "2025 Annual Targets"
        p.font.size = Pt(18)
        p.font.bold = True
        p.font.color.rgb = theme["accent_color"]

        annual_targets = section["content"].get("annual_targets", {})
        for target, value in annual_targets.items():
            p = text_frame.add_paragraph()
            p.text = f"‚Ä¢ {target.replace('_', ' ').title()}: {value}"
            p.font.size = Pt(14)
            p.font.color.rgb = theme["text_color"]
            p.level = 1

    def _create_conclusion_slide(self, prs: Presentation, data: Dict[str, Any], theme: Dict[str, Any]):
        """Create a conclusion slide."""
        slide_layout = prs.slide_layouts[self.layout_types["title_content"]]
        slide = prs.slides.add_slide(slide_layout)

        # Set title
        title = slide.shapes.title
        title.text = "Thank You"
        title.text_frame.paragraphs[0].font.color.rgb = theme["primary_color"]
        title.text_frame.paragraphs[0].font.size = Pt(36)
        title.text_frame.paragraphs[0].font.bold = True

        # Add contact information
        content = slide.placeholders[1]
        text_frame = content.text_frame
        text_frame.clear()

        p = text_frame.add_paragraph()
        p.text = "Questions & Discussion"
        p.font.size = Pt(24)
        p.font.color.rgb = theme["accent_color"]
        p.alignment = PP_ALIGN.CENTER

        p = text_frame.add_paragraph()
        p.text = f"\n{data['company']}\n{data['author']}\n{data['date']}"
        p.font.size = Pt(18)
        p.font.color.rgb = theme["text_color"]
        p.alignment = PP_ALIGN.CENTER

    def generate_presentation_from_template(self, template_type: str, data_source: str) -> str:
        """
        Generate a presentation from predefined templates and data sources.

        Args:
            template_type: Type of presentation template
            data_source: Source of data (json file, csv, api, etc.)

        Returns:
            Filename of the generated presentation
        """

        print(f"üéØ Generating {template_type} presentation from {data_source}")

        # For demo purposes, use sample data
        # In real implementation, this would read from actual data sources
        presentation_data = self.get_sample_presentation_data()

        # Customize based on template type
        if template_type == "sales_report":
            presentation_data["title"] = "Sales Performance Report"
            presentation_data["theme"] = "corporate"

        elif template_type == "project_update":
            presentation_data["title"] = "Project Status Update"
            presentation_data["theme"] = "modern"

        elif template_type == "quarterly_review":
            presentation_data["title"] = "Quarterly Business Review"
            presentation_data["theme"] = "minimal"

        # Generate the presentation
        return self.create_presentation(presentation_data)

def main():
    """
    Main function to demonstrate PowerPoint generation capabilities.
    """

    print("üöÄ PowerPoint Generator Demo")
    print("=" * 50)

    # Initialize the generator
    generator = PowerPointGenerator()

    print("üìã Available presentation templates:")
    print("1. Sales Report")
    print("2. Project Update")
    print("3. Quarterly Review")
    print("4. Custom Data Presentation")
    print()

    # Generate sample presentation
    print("üé® Creating sample business presentation...")
    presentation_data = generator.get_sample_presentation_data()
    filename = generator.create_presentation(presentation_data)

    print(f"‚úÖ Presentation created successfully: {filename}")

    # Demonstrate template-based generation
    print("\nüìä Generating additional template examples...")

    # Sales report
    sales_filename = generator.generate_presentation_from_template("sales_report", "sample_data")
    print(f"üìà Sales report generated: {sales_filename}")

    # Project update
    project_filename = generator.generate_presentation_from_template("project_update", "sample_data")
    print(f"üìã Project update generated: {project_filename}")

    print("\nüéØ PRESENTATION FEATURES DEMONSTRATED:")
    print("- Automated slide creation from data")
    print("- AI-powered content generation")
    print("- Data visualization integration")
    print("- Professional theme application")
    print("- Multiple layout types")
    print("- Chart and graph embedding")

    print("\nüîó INTEGRATION POSSIBILITIES:")
    print("- Database connectivity for live data")
    print("- API integration for real-time metrics")
    print("- Excel/CSV data import automation")
    print("- Template library management")
    print("- Automated report scheduling")
    print("- Brand guideline enforcement")
    print("- Multi-language support")
    print("- Cloud storage integration")

    print(f"\nüíæ Generated files:")
    print(f"- {filename}")
    print(f"- {sales_filename}")
    print(f"- {project_filename}")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/MS_dashboard_patterns.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "pandas",
#     "matplotlib",
#     "seaborn",
#     "numpy",
#     "sqlite3",
#     "python-dotenv",
#     "plotly",
#     "dash",
#     "dash-bootstrap-components",
#     "scikit-learn",
# ]
# ///

"""
MS_dashboard_patterns.py - Application Log Analysis Dashboard

Student: MS (Initials)
Request: "creating dash boards and patterns of the issues in the application"

This script demonstrates how to create interactive dashboards for analyzing application logs
and identifying patterns in issues. It includes comprehensive log analysis, pattern detection,
and real-time dashboard visualization.

Educational Focus:
- Log file parsing and analysis
- Pattern detection in application logs
- Interactive dashboard creation with Dash/Plotly
- Data visualization best practices
- Real-time monitoring concepts
- Issue categorization and alerting
- Performance metrics tracking

Prerequisites:
- Sample log files (will be created automatically)
- Web browser for viewing dashboard
- Basic understanding of web applications
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
import re
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import dash
from dash import dcc, html, Input, Output, dash_table
import dash_bootstrap_components as dbc
import plotly.graph_objs as go
import plotly.express as px
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import threading
import time
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dashboard_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class LogEntry:
    """Data class representing a parsed log entry."""
    timestamp: datetime
    level: str
    component: str
    message: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    ip_address: Optional[str] = None
    request_id: Optional[str] = None
    response_time: Optional[float] = None
    status_code: Optional[int] = None
    error_code: Optional[str] = None

@dataclass
class IssuePattern:
    """Data class representing an identified issue pattern."""
    pattern_id: str
    pattern_type: str
    description: str
    frequency: int
    severity: str
    first_occurrence: datetime
    last_occurrence: datetime
    affected_components: List[str]
    sample_messages: List[str]

class LogAnalyzer:
    """
    Comprehensive log analyzer for detecting patterns and issues.

    This class demonstrates professional log analysis patterns including:
    - Log parsing and normalization
    - Pattern detection using ML techniques
    - Issue categorization and severity assessment
    - Performance metrics calculation
    """

    def __init__(self, db_path: str = "log_analysis.db"):
        """
        Initialize the log analyzer.

        Args:
            db_path (str): Path to SQLite database for storing analysis results
        """
        self.db_path = db_path
        self.setup_database()

        # Log level hierarchy for severity mapping
        self.log_levels = {
            'DEBUG': 1,
            'INFO': 2,
            'WARNING': 3,
            'ERROR': 4,
            'CRITICAL': 5
        }

        # Common error patterns
        self.error_patterns = {
            'database': [
                r'database.*connection.*failed',
                r'sql.*error',
                r'timeout.*database',
                r'deadlock.*detected'
            ],
            'authentication': [
                r'authentication.*failed',
                r'invalid.*credentials',
                r'unauthorized.*access',
                r'token.*expired'
            ],
            'network': [
                r'connection.*refused',
                r'network.*timeout',
                r'host.*unreachable',
                r'ssl.*error'
            ],
            'performance': [
                r'slow.*query',
                r'high.*memory.*usage',
                r'cpu.*threshold.*exceeded',
                r'response.*time.*exceeded'
            ],
            'application': [
                r'null.*pointer.*exception',
                r'index.*out.*of.*bounds',
                r'file.*not.*found',
                r'permission.*denied'
            ]
        }

    def setup_database(self):
        """Initialize the SQLite database for storing log analysis results."""
        logger.info("üóÑÔ∏è Setting up log analysis database...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Log entries table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS log_entries (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME,
                        level TEXT,
                        component TEXT,
                        message TEXT,
                        user_id TEXT,
                        session_id TEXT,
                        ip_address TEXT,
                        request_id TEXT,
                        response_time REAL,
                        status_code INTEGER,
                        error_code TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Issue patterns table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS issue_patterns (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pattern_id TEXT UNIQUE,
                        pattern_type TEXT,
                        description TEXT,
                        frequency INTEGER,
                        severity TEXT,
                        first_occurrence DATETIME,
                        last_occurrence DATETIME,
                        affected_components TEXT,
                        sample_messages TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Performance metrics table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS performance_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME,
                        metric_name TEXT,
                        metric_value REAL,
                        component TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Dashboard alerts table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS dashboard_alerts (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        alert_type TEXT,
                        severity TEXT,
                        title TEXT,
                        message TEXT,
                        component TEXT,
                        timestamp DATETIME,
                        acknowledged BOOLEAN DEFAULT 0,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                conn.commit()
                logger.info("‚úÖ Database setup completed")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Database setup failed: {e}")
            raise

    def create_sample_logs(self, num_entries: int = 1000) -> List[LogEntry]:
        """
        Create sample log entries for demonstration.

        In a real implementation, this would parse actual log files.

        Args:
            num_entries (int): Number of log entries to create

        Returns:
            List[LogEntry]: List of sample log entries
        """
        logger.info(f"üìù Creating {num_entries} sample log entries...")

        log_entries = []
        components = ['web-server', 'database', 'auth-service', 'payment-service', 'notification-service']
        log_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']

        # Sample log messages by type
        log_messages = {
            'INFO': [
                'User login successful',
                'Request processed successfully',
                'Cache hit for user data',
                'Database connection established',
                'Service started successfully'
            ],
            'WARNING': [
                'High memory usage detected',
                'Slow database query detected',
                'Rate limit threshold approaching',
                'Cache miss rate increasing',
                'Connection pool near capacity'
            ],
            'ERROR': [
                'Database connection failed',
                'Authentication failed for user',
                'Payment processing failed',
                'Network timeout occurred',
                'File permission denied'
            ],
            'CRITICAL': [
                'Database server unreachable',
                'Memory limit exceeded, service crashing',
                'Security breach detected',
                'All connection pools exhausted',
                'Core service unavailable'
            ]
        }

        import random

        for i in range(num_entries):
            # Generate timestamp (last 7 days)
            days_ago = random.uniform(0, 7)
            timestamp = datetime.now() - timedelta(days=days_ago)

            # Choose log level (weighted towards INFO, fewer CRITICAL)
            level_weights = [0.1, 0.6, 0.2, 0.08, 0.02]  # DEBUG, INFO, WARNING, ERROR, CRITICAL
            level = random.choices(log_levels, weights=level_weights)[0]

            # Choose component
            component = random.choice(components)

            # Generate message based on level
            if level in log_messages:
                base_message = random.choice(log_messages[level])
            else:
                base_message = f"Debug message from {component}"

            # Add some variation to messages
            message = f"{base_message} - Request {i+1}"

            # Add additional context for certain levels
            user_id = f"user_{random.randint(1, 1000)}" if random.random() < 0.7 else None
            session_id = f"sess_{random.randint(10000, 99999)}" if random.random() < 0.5 else None
            ip_address = f"192.168.1.{random.randint(1, 254)}" if random.random() < 0.8 else None
            request_id = f"req_{random.randint(100000, 999999)}" if random.random() < 0.6 else None

            # Add response time for web requests
            response_time = None
            if component == 'web-server' and random.random() < 0.8:
                # Generate realistic response times with some outliers
                if level in ['ERROR', 'CRITICAL']:
                    response_time = random.uniform(5000, 30000)  # Slow responses for errors
                else:
                    response_time = random.lognormal(4, 1)  # Log-normal distribution for normal responses

            # Add status codes for web requests
            status_code = None
            if component == 'web-server' and random.random() < 0.9:
                if level == 'ERROR':
                    status_code = random.choice([404, 500, 503, 403])
                elif level == 'CRITICAL':
                    status_code = random.choice([500, 503, 502])
                else:
                    status_code = random.choice([200, 201, 204])

            # Add error codes for specific errors
            error_code = None
            if level in ['ERROR', 'CRITICAL'] and random.random() < 0.6:
                error_code = f"ERR_{random.randint(1000, 9999)}"

            log_entry = LogEntry(
                timestamp=timestamp,
                level=level,
                component=component,
                message=message,
                user_id=user_id,
                session_id=session_id,
                ip_address=ip_address,
                request_id=request_id,
                response_time=response_time,
                status_code=status_code,
                error_code=error_code
            )

            log_entries.append(log_entry)

        logger.info(f"‚úÖ Created {len(log_entries)} sample log entries")
        return log_entries

    def parse_log_line(self, log_line: str) -> Optional[LogEntry]:
        """
        Parse a single log line into a LogEntry object.

        Args:
            log_line (str): Raw log line to parse

        Returns:
            Optional[LogEntry]: Parsed log entry or None if parsing failed
        """
        # Common log format patterns
        patterns = [
            # Apache/Nginx style: timestamp [level] component: message
            r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d{3} \[(?P<level>\w+)\] (?P<component>\w+): (?P<message>.*)',
            # Python logging style
            r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) - (?P<component>\w+) - (?P<level>\w+) - (?P<message>.*)',
            # Simple format
            r'(?P<level>\w+) (?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?P<message>.*)'
        ]

        for pattern in patterns:
            match = re.match(pattern, log_line.strip())
            if match:
                groups = match.groupdict()

                try:
                    timestamp = datetime.strptime(groups['timestamp'], '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    continue

                return LogEntry(
                    timestamp=timestamp,
                    level=groups['level'].upper(),
                    component=groups.get('component', 'unknown'),
                    message=groups['message']
                )

        return None

    def store_log_entries(self, log_entries: List[LogEntry]):
        """
        Store log entries in the database.

        Args:
            log_entries (List[LogEntry]): List of log entries to store
        """
        logger.info(f"üíæ Storing {len(log_entries)} log entries...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                for entry in log_entries:
                    cursor.execute('''
                        INSERT INTO log_entries
                        (timestamp, level, component, message, user_id, session_id,
                         ip_address, request_id, response_time, status_code, error_code)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        entry.timestamp,
                        entry.level,
                        entry.component,
                        entry.message,
                        entry.user_id,
                        entry.session_id,
                        entry.ip_address,
                        entry.request_id,
                        entry.response_time,
                        entry.status_code,
                        entry.error_code
                    ))

                conn.commit()

        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to store log entries: {e}")
            raise

        logger.info("‚úÖ Log entries stored successfully")

    def detect_error_patterns(self) -> List[IssuePattern]:
        """
        Detect error patterns using text analysis and clustering.

        Returns:
            List[IssuePattern]: List of detected issue patterns
        """
        logger.info("üîç Detecting error patterns...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                # Get error and warning messages
                df_errors = pd.read_sql_query('''
                    SELECT timestamp, level, component, message
                    FROM log_entries
                    WHERE level IN ('ERROR', 'CRITICAL', 'WARNING')
                    ORDER BY timestamp DESC
                ''', conn)

                if df_errors.empty:
                    logger.info("No error messages found")
                    return []

                patterns = []

                # Analyze patterns by predefined categories
                for pattern_type, regex_patterns in self.error_patterns.items():
                    matching_messages = []

                    for _, row in df_errors.iterrows():
                        message = row['message'].lower()
                        for pattern in regex_patterns:
                            if re.search(pattern, message, re.IGNORECASE):
                                matching_messages.append(row)
                                break

                    if matching_messages:
                        df_pattern = pd.DataFrame(matching_messages)

                        # Calculate pattern statistics
                        frequency = len(df_pattern)
                        first_occurrence = df_pattern['timestamp'].min()
                        last_occurrence = df_pattern['timestamp'].max()
                        affected_components = df_pattern['component'].unique().tolist()
                        sample_messages = df_pattern['message'].head(3).tolist()

                        # Determine severity based on frequency and log levels
                        critical_count = len(df_pattern[df_pattern['level'] == 'CRITICAL'])
                        error_count = len(df_pattern[df_pattern['level'] == 'ERROR'])

                        if critical_count > 0 or frequency > 50:
                            severity = 'high'
                        elif error_count > 10 or frequency > 20:
                            severity = 'medium'
                        else:
                            severity = 'low'

                        issue_pattern = IssuePattern(
                            pattern_id=f"{pattern_type}_{hash(pattern_type) % 10000}",
                            pattern_type=pattern_type,
                            description=f"{pattern_type.title()} related issues detected",
                            frequency=frequency,
                            severity=severity,
                            first_occurrence=pd.to_datetime(first_occurrence),
                            last_occurrence=pd.to_datetime(last_occurrence),
                            affected_components=affected_components,
                            sample_messages=sample_messages
                        )

                        patterns.append(issue_pattern)

                # Store patterns in database
                self.store_issue_patterns(patterns)

        except Exception as e:
            logger.error(f"‚ùå Failed to detect error patterns: {e}")
            return []

        logger.info(f"‚úÖ Detected {len(patterns)} issue patterns")
        return patterns

    def store_issue_patterns(self, patterns: List[IssuePattern]):
        """Store detected issue patterns in the database."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Clear old patterns
                cursor.execute('DELETE FROM issue_patterns')

                for pattern in patterns:
                    cursor.execute('''
                        INSERT INTO issue_patterns
                        (pattern_id, pattern_type, description, frequency, severity,
                         first_occurrence, last_occurrence, affected_components, sample_messages)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        pattern.pattern_id,
                        pattern.pattern_type,
                        pattern.description,
                        pattern.frequency,
                        pattern.severity,
                        pattern.first_occurrence,
                        pattern.last_occurrence,
                        json.dumps(pattern.affected_components),
                        json.dumps(pattern.sample_messages)
                    ))

                conn.commit()

        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to store issue patterns: {e}")

    def calculate_performance_metrics(self):
        """Calculate and store performance metrics."""
        logger.info("üìä Calculating performance metrics...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Clear old metrics
                cursor.execute('DELETE FROM performance_metrics')

                # Calculate error rates by hour
                df_hourly_errors = pd.read_sql_query('''
                    SELECT
                        strftime('%Y-%m-%d %H:00:00', timestamp) as hour,
                        component,
                        COUNT(*) as total_logs,
                        SUM(CASE WHEN level IN ('ERROR', 'CRITICAL') THEN 1 ELSE 0 END) as error_count
                    FROM log_entries
                    GROUP BY strftime('%Y-%m-%d %H:00:00', timestamp), component
                ''', conn)

                for _, row in df_hourly_errors.iterrows():
                    error_rate = (row['error_count'] / row['total_logs']) * 100 if row['total_logs'] > 0 else 0

                    cursor.execute('''
                        INSERT INTO performance_metrics
                        (timestamp, metric_name, metric_value, component)
                        VALUES (?, ?, ?, ?)
                    ''', (row['hour'], 'error_rate_percent', error_rate, row['component']))

                # Calculate average response times
                df_response_times = pd.read_sql_query('''
                    SELECT
                        strftime('%Y-%m-%d %H:00:00', timestamp) as hour,
                        component,
                        AVG(response_time) as avg_response_time,
                        MAX(response_time) as max_response_time
                    FROM log_entries
                    WHERE response_time IS NOT NULL
                    GROUP BY strftime('%Y-%m-%d %H:00:00', timestamp), component
                ''', conn)

                for _, row in df_response_times.iterrows():
                    cursor.execute('''
                        INSERT INTO performance_metrics
                        (timestamp, metric_name, metric_value, component)
                        VALUES (?, ?, ?, ?)
                    ''', (row['hour'], 'avg_response_time_ms', row['avg_response_time'], row['component']))

                    cursor.execute('''
                        INSERT INTO performance_metrics
                        (timestamp, metric_name, metric_value, component)
                        VALUES (?, ?, ?, ?)
                    ''', (row['hour'], 'max_response_time_ms', row['max_response_time'], row['component']))

                conn.commit()

        except Exception as e:
            logger.error(f"‚ùå Failed to calculate performance metrics: {e}")

        logger.info("‚úÖ Performance metrics calculated")

    def generate_alerts(self):
        """Generate alerts based on detected patterns and thresholds."""
        logger.info("üö® Generating alerts...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Clear old alerts
                cursor.execute('DELETE FROM dashboard_alerts')

                # Alert for high error rates
                df_high_errors = pd.read_sql_query('''
                    SELECT component, metric_value, timestamp
                    FROM performance_metrics
                    WHERE metric_name = 'error_rate_percent' AND metric_value > 10
                    ORDER BY timestamp DESC
                    LIMIT 10
                ''', conn)

                for _, row in df_high_errors.iterrows():
                    cursor.execute('''
                        INSERT INTO dashboard_alerts
                        (alert_type, severity, title, message, component, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        'high_error_rate',
                        'high',
                        'High Error Rate Detected',
                        f'Error rate of {row["metric_value"]:.1f}% detected in {row["component"]}',
                        row['component'],
                        row['timestamp']
                    ))

                # Alert for slow response times
                df_slow_responses = pd.read_sql_query('''
                    SELECT component, metric_value, timestamp
                    FROM performance_metrics
                    WHERE metric_name = 'avg_response_time_ms' AND metric_value > 5000
                    ORDER BY timestamp DESC
                    LIMIT 10
                ''', conn)

                for _, row in df_slow_responses.iterrows():
                    cursor.execute('''
                        INSERT INTO dashboard_alerts
                        (alert_type, severity, title, message, component, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        'slow_response',
                        'medium',
                        'Slow Response Time',
                        f'Average response time of {row["metric_value"]:.0f}ms in {row["component"]}',
                        row['component'],
                        row['timestamp']
                    ))

                # Alert for critical errors
                df_critical = pd.read_sql_query('''
                    SELECT component, COUNT(*) as count, MAX(timestamp) as latest
                    FROM log_entries
                    WHERE level = 'CRITICAL' AND timestamp > datetime('now', '-1 hour')
                    GROUP BY component
                    HAVING count > 0
                ''', conn)

                for _, row in df_critical.iterrows():
                    cursor.execute('''
                        INSERT INTO dashboard_alerts
                        (alert_type, severity, title, message, component, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        'critical_errors',
                        'critical',
                        'Critical Errors Detected',
                        f'{row["count"]} critical errors in {row["component"]} in the last hour',
                        row['component'],
                        row['latest']
                    ))

                conn.commit()

        except Exception as e:
            logger.error(f"‚ùå Failed to generate alerts: {e}")

        logger.info("‚úÖ Alerts generated")

class DashboardApp:
    """
    Interactive dashboard application using Dash/Plotly.

    This creates a web-based dashboard for log analysis visualization.
    """

    def __init__(self, db_path: str = "log_analysis.db"):
        """Initialize the dashboard application."""
        self.db_path = db_path
        self.app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
        self.setup_layout()
        self.setup_callbacks()

    def setup_layout(self):
        """Setup the dashboard layout."""
        self.app.layout = dbc.Container([
            dbc.Row([
                dbc.Col([
                    html.H1("üîç Application Log Analysis Dashboard", className="text-center mb-4"),
                    html.P("Real-time monitoring and pattern analysis for application logs",
                           className="text-center text-muted mb-4")
                ])
            ]),

            # Alert section
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("üö® Active Alerts"),
                        dbc.CardBody(id="alerts-section")
                    ])
                ])
            ], className="mb-4"),

            # Metrics overview
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("üìä Key Metrics"),
                        dbc.CardBody([
                            dbc.Row([
                                dbc.Col([
                                    html.H4(id="total-logs", className="text-primary"),
                                    html.P("Total Logs", className="text-muted")
                                ], width=3),
                                dbc.Col([
                                    html.H4(id="error-count", className="text-danger"),
                                    html.P("Errors/Critical", className="text-muted")
                                ], width=3),
                                dbc.Col([
                                    html.H4(id="avg-response-time", className="text-warning"),
                                    html.P("Avg Response (ms)", className="text-muted")
                                ], width=3),
                                dbc.Col([
                                    html.H4(id="active-patterns", className="text-info"),
                                    html.P("Issue Patterns", className="text-muted")
                                ], width=3)
                            ])
                        ])
                    ])
                ])
            ], className="mb-4"),

            # Charts section
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("üìà Log Volume Over Time"),
                        dbc.CardBody([
                            dcc.Graph(id="log-volume-chart")
                        ])
                    ])
                ], width=6),
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("üéØ Error Rate by Component"),
                        dbc.CardBody([
                            dcc.Graph(id="error-rate-chart")
                        ])
                    ])
                ], width=6)
            ], className="mb-4"),

            # Pattern analysis
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("üîç Issue Patterns"),
                        dbc.CardBody([
                            dash_table.DataTable(
                                id="patterns-table",
                                columns=[
                                    {"name": "Pattern Type", "id": "pattern_type"},
                                    {"name": "Description", "id": "description"},
                                    {"name": "Frequency", "id": "frequency"},
                                    {"name": "Severity", "id": "severity"},
                                    {"name": "Last Occurrence", "id": "last_occurrence"}
                                ],
                                style_cell={'textAlign': 'left'},
                                style_data_conditional=[
                                    {
                                        'if': {'filter_query': '{severity} = high'},
                                        'backgroundColor': '#f8d7da',
                                        'color': 'black',
                                    },
                                    {
                                        'if': {'filter_query': '{severity} = medium'},
                                        'backgroundColor': '#fff3cd',
                                        'color': 'black',
                                    }
                                ]
                            )
                        ])
                    ])
                ])
            ], className="mb-4"),

            # Performance metrics
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("‚ö° Performance Metrics"),
                        dbc.CardBody([
                            dcc.Graph(id="performance-chart")
                        ])
                    ])
                ])
            ], className="mb-4"),

            # Auto-refresh interval
            dcc.Interval(
                id='interval-component',
                interval=30*1000,  # Update every 30 seconds
                n_intervals=0
            )

        ], fluid=True)

    def setup_callbacks(self):
        """Setup dashboard callbacks for interactivity."""

        @self.app.callback(
            [Output('total-logs', 'children'),
             Output('error-count', 'children'),
             Output('avg-response-time', 'children'),
             Output('active-patterns', 'children')],
            [Input('interval-component', 'n_intervals')]
        )
        def update_metrics(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    # Total logs
                    total_logs = pd.read_sql_query('SELECT COUNT(*) as count FROM log_entries', conn).iloc[0]['count']

                    # Error count
                    error_count = pd.read_sql_query(
                        "SELECT COUNT(*) as count FROM log_entries WHERE level IN ('ERROR', 'CRITICAL')",
                        conn
                    ).iloc[0]['count']

                    # Average response time
                    avg_response = pd.read_sql_query(
                        'SELECT AVG(response_time) as avg FROM log_entries WHERE response_time IS NOT NULL',
                        conn
                    ).iloc[0]['avg']
                    avg_response = f"{avg_response:.0f}" if avg_response else "N/A"

                    # Active patterns
                    pattern_count = pd.read_sql_query('SELECT COUNT(*) as count FROM issue_patterns', conn).iloc[0]['count']

                    return f"{total_logs:,}", f"{error_count:,}", avg_response, str(pattern_count)
            except:
                return "N/A", "N/A", "N/A", "N/A"

        @self.app.callback(
            Output('alerts-section', 'children'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_alerts(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    df_alerts = pd.read_sql_query('''
                        SELECT alert_type, severity, title, message, component, timestamp
                        FROM dashboard_alerts
                        ORDER BY timestamp DESC
                        LIMIT 5
                    ''', conn)

                    if df_alerts.empty:
                        return dbc.Alert("‚úÖ No active alerts", color="success")

                    alerts = []
                    for _, alert in df_alerts.iterrows():
                        color_map = {'critical': 'danger', 'high': 'warning', 'medium': 'info', 'low': 'secondary'}
                        color = color_map.get(alert['severity'], 'secondary')

                        alerts.append(
                            dbc.Alert([
                                html.H6(alert['title'], className="mb-1"),
                                html.P(alert['message'], className="mb-1"),
                                html.Small(f"{alert['component']} - {alert['timestamp']}", className="text-muted")
                            ], color=color, className="mb-2")
                        )

                    return alerts
            except:
                return dbc.Alert("‚ö†Ô∏è Unable to load alerts", color="warning")

        @self.app.callback(
            Output('log-volume-chart', 'figure'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_log_volume_chart(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    df = pd.read_sql_query('''
                        SELECT
                            strftime('%Y-%m-%d %H:00:00', timestamp) as hour,
                            level,
                            COUNT(*) as count
                        FROM log_entries
                        WHERE timestamp >= datetime('now', '-24 hours')
                        GROUP BY strftime('%Y-%m-%d %H:00:00', timestamp), level
                        ORDER BY hour
                    ''', conn)

                    if df.empty:
                        return px.bar(title="No data available")

                    fig = px.bar(df, x='hour', y='count', color='level',
                                title="Log Volume by Hour (Last 24h)",
                                color_discrete_map={
                                    'DEBUG': '#6c757d',
                                    'INFO': '#17a2b8',
                                    'WARNING': '#ffc107',
                                    'ERROR': '#dc3545',
                                    'CRITICAL': '#6f42c1'
                                })
                    fig.update_layout(xaxis_title="Hour", yaxis_title="Log Count")
                    return fig
            except:
                return px.bar(title="Error loading chart")

        @self.app.callback(
            Output('error-rate-chart', 'figure'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_error_rate_chart(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    df = pd.read_sql_query('''
                        SELECT
                            component,
                            AVG(metric_value) as avg_error_rate
                        FROM performance_metrics
                        WHERE metric_name = 'error_rate_percent'
                        GROUP BY component
                        ORDER BY avg_error_rate DESC
                    ''', conn)

                    if df.empty:
                        return px.bar(title="No error rate data available")

                    fig = px.bar(df, x='component', y='avg_error_rate',
                                title="Average Error Rate by Component",
                                color='avg_error_rate',
                                color_continuous_scale='Reds')
                    fig.update_layout(xaxis_title="Component", yaxis_title="Error Rate (%)")
                    return fig
            except:
                return px.bar(title="Error loading chart")

        @self.app.callback(
            Output('patterns-table', 'data'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_patterns_table(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    df = pd.read_sql_query('''
                        SELECT pattern_type, description, frequency, severity, last_occurrence
                        FROM issue_patterns
                        ORDER BY frequency DESC
                    ''', conn)

                    return df.to_dict('records')
            except:
                return []

        @self.app.callback(
            Output('performance-chart', 'figure'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_performance_chart(n):
            try:
                with sqlite3.connect(self.db_path) as conn:
                    df = pd.read_sql_query('''
                        SELECT timestamp, component, metric_value
                        FROM performance_metrics
                        WHERE metric_name = 'avg_response_time_ms'
                        ORDER BY timestamp
                    ''', conn)

                    if df.empty:
                        return px.line(title="No performance data available")

                    fig = px.line(df, x='timestamp', y='metric_value', color='component',
                                 title="Response Time Trends")
                    fig.update_layout(xaxis_title="Time", yaxis_title="Response Time (ms)")
                    return fig
            except:
                return px.line(title="Error loading chart")

    def run(self, debug=False, port=8050):
        """Run the dashboard application."""
        logger.info(f"üöÄ Starting dashboard on http://localhost:{port}")
        self.app.run_server(debug=debug, port=port)

def demonstrate_advanced_analytics():
    """
    Demonstrate advanced log analytics concepts.
    """
    print("\nüéì Advanced Log Analytics Concepts:")
    print("=" * 50)

    concepts = {
        "Machine Learning Applications": [
            "Anomaly detection using isolation forests",
            "Log clustering for pattern discovery",
            "Predictive alerting based on historical trends",
            "Natural language processing for error categorization"
        ],
        "Real-time Processing": [
            "Stream processing with Apache Kafka",
            "Real-time alerting with Apache Storm",
            "Event correlation across multiple services",
            "Dynamic threshold adjustment"
        ],
        "Scalability Patterns": [
            "Log aggregation with ELK stack (Elasticsearch, Logstash, Kibana)",
            "Distributed tracing with Jaeger or Zipkin",
            "Time-series databases for metrics (InfluxDB, Prometheus)",
            "Log sampling for high-volume systems"
        ],
        "Security and Compliance": [
            "Log anonymization and PII protection",
            "Audit trail maintenance",
            "Security event correlation",
            "Compliance reporting automation"
        ]
    }

    for category, items in concepts.items():
        print(f"\nüîß {category}:")
        for item in items:
            print(f"  ‚Ä¢ {item}")

def main():
    """
    Main function demonstrating the log analysis and dashboard workflow.
    """
    print("üöÄ Welcome to MS's Log Analysis Dashboard!")
    print("=" * 50)

    try:
        # Initialize the log analyzer
        analyzer = LogAnalyzer("ms_log_analysis.db")

        print("\n1. üìù Creating sample log data...")
        sample_logs = analyzer.create_sample_logs(num_entries=1500)

        print("\n2. üíæ Storing log entries...")
        analyzer.store_log_entries(sample_logs)

        print("\n3. üîç Detecting error patterns...")
        patterns = analyzer.detect_error_patterns()

        print("\n4. üìä Calculating performance metrics...")
        analyzer.calculate_performance_metrics()

        print("\n5. üö® Generating alerts...")
        analyzer.generate_alerts()

        print(f"\n‚úÖ Log analysis completed!")
        print(f"   Processed: {len(sample_logs)} log entries")
        print(f"   Detected: {len(patterns)} issue patterns")

        # Show some sample results
        if patterns:
            print(f"\nüîç Sample Issue Patterns:")
            for pattern in patterns[:3]:
                print(f"   ‚Ä¢ {pattern.pattern_type}: {pattern.frequency} occurrences ({pattern.severity} severity)")

        # Show database file info
        db_path = Path("ms_log_analysis.db")
        if db_path.exists():
            size_mb = db_path.stat().st_size / (1024 * 1024)
            print(f"\nüìÅ Database file: {db_path.absolute()}")
            print(f"   Size: {size_mb:.2f} MB")

        # Option to start dashboard
        print(f"\nüåê Dashboard Options:")
        print("   To start the interactive dashboard, run:")
        print("   python MS_dashboard_patterns.py --dashboard")
        print("   Then open http://localhost:8050 in your browser")

        # Start dashboard if requested
        import sys
        if "--dashboard" in sys.argv:
            print("\nüöÄ Starting interactive dashboard...")
            dashboard = DashboardApp("ms_log_analysis.db")

            # Run in a separate thread to allow for demonstration
            def run_dashboard():
                dashboard.run(debug=False, port=8050)

            dashboard_thread = threading.Thread(target=run_dashboard, daemon=True)
            dashboard_thread.start()

            print("‚úÖ Dashboard started! Open http://localhost:8050 in your browser")
            print("Press Ctrl+C to stop...")

            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nüëã Dashboard stopped")

    except Exception as e:
        logger.error(f"‚ùå Log analysis failed: {e}")
        print("üí° Troubleshooting tips:")
        print("   - Ensure you have write permissions in the current directory")
        print("   - Check if port 8050 is available for the dashboard")
        print("   - Verify all dependencies are installed (dash, plotly, etc.)")

    # Show advanced concepts
    demonstrate_advanced_analytics()

    print("\nüéì Learning Summary:")
    print("- Log parsing and analysis techniques")
    print("- Pattern detection using regex and ML")
    print("- Interactive dashboard creation with Dash/Plotly")
    print("- Real-time monitoring and alerting concepts")
    print("- Performance metrics calculation and visualization")
    print("- Database design for log analytics")

    print("\nüí° Next Steps:")
    print("- Integrate with real log files from your applications")
    print("- Add machine learning for anomaly detection")
    print("- Set up log aggregation from multiple services")
    print("- Implement automated incident response")
    print("- Create custom alerting rules and notifications")
    print("- Add distributed tracing capabilities")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/PL_etl_automation.py

```python
#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "pandas",
#     "sqlite3",
#     "numpy",
#     "python-dotenv",
#     "sqlalchemy",
#     "openpyxl",
# ]
# ///

"""
PL_etl_automation.py - ETL Automation Scripts

Student: PL (Initials)
Request: "like ETL automation scripts"

This script demonstrates comprehensive ETL (Extract, Transform, Load) automation patterns.
It shows how to extract data from various sources, transform it according to business rules,
and load it into different target systems.

Educational Focus:
- ETL pipeline design and implementation
- Data extraction from multiple sources (CSV, Excel, JSON, APIs)
- Data transformation and cleaning techniques
- Data validation and quality checks
- Loading data into databases and other targets
- Error handling and logging in ETL processes
- Scheduling and monitoring ETL jobs

Prerequisites:
- Sample data files (will be created automatically)
- SQLite for database operations
- Pandas for data manipulation
"""

import pandas as pd
import sqlite3
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import re
from dotenv import load_dotenv
import os
import shutil

# Load environment variables
load_dotenv()

# Set up comprehensive logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_automation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ETLConfig:
    """
    Configuration class for ETL operations.

    This centralizes all ETL settings and makes the pipeline configurable.
    """
    source_directory: str = "etl_sources"
    staging_directory: str = "etl_staging"
    archive_directory: str = "etl_archive"
    output_directory: str = "etl_output"
    database_path: str = "etl_data.db"
    batch_size: int = 1000
    error_threshold: float = 0.05  # 5% error threshold
    data_retention_days: int = 30

class DataQualityValidator:
    """
    Class for validating data quality during ETL processes.

    This demonstrates professional data quality practices.
    """

    @staticmethod
    def validate_email(email: str) -> bool:
        """Validate email format."""
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, str(email))) if pd.notna(email) else False

    @staticmethod
    def validate_phone(phone: str) -> bool:
        """Validate phone number format."""
        if pd.isna(phone):
            return False
        # Remove all non-digit characters
        digits = re.sub(r'\D', '', str(phone))
        # Check if it has 10-15 digits (international format)
        return 10 <= len(digits) <= 15

    @staticmethod
    def validate_date(date_str: str, date_format: str = "%Y-%m-%d") -> bool:
        """Validate date format."""
        if pd.isna(date_str):
            return False
        try:
            datetime.strptime(str(date_str), date_format)
            return True
        except ValueError:
            return False

    @staticmethod
    def check_data_completeness(df: pd.DataFrame, required_columns: List[str]) -> Dict[str, float]:
        """Check data completeness for required columns."""
        completeness = {}
        for col in required_columns:
            if col in df.columns:
                completeness[col] = (df[col].notna().sum() / len(df)) * 100
            else:
                completeness[col] = 0.0
        return completeness

    @staticmethod
    def detect_duplicates(df: pd.DataFrame, key_columns: List[str]) -> pd.DataFrame:
        """Detect duplicate records based on key columns."""
        if all(col in df.columns for col in key_columns):
            return df[df.duplicated(subset=key_columns, keep=False)]
        return pd.DataFrame()

class ETLPipeline:
    """
    Comprehensive ETL pipeline implementation.

    This class demonstrates professional ETL patterns including:
    - Modular design
    - Error handling
    - Data quality validation
    - Logging and monitoring
    - Incremental loading
    """

    def __init__(self, config: ETLConfig):
        """
        Initialize the ETL pipeline.

        Args:
            config (ETLConfig): ETL configuration settings
        """
        self.config = config
        self.validator = DataQualityValidator()
        self.setup_directories()
        self.setup_database()

        # Track ETL metrics
        self.metrics = {
            'records_extracted': 0,
            'records_transformed': 0,
            'records_loaded': 0,
            'errors_count': 0,
            'start_time': None,
            'end_time': None
        }

    def setup_directories(self):
        """Create necessary directories for ETL operations."""
        directories = [
            self.config.source_directory,
            self.config.staging_directory,
            self.config.archive_directory,
            self.config.output_directory
        ]

        for directory in directories:
            Path(directory).mkdir(exist_ok=True)
            logger.info(f"üìÅ Directory ready: {directory}")

    def setup_database(self):
        """Initialize the target database with necessary tables."""
        logger.info("üóÑÔ∏è Setting up target database...")

        try:
            with sqlite3.connect(self.config.database_path) as conn:
                cursor = conn.cursor()

                # ETL job tracking table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS etl_job_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        job_name TEXT NOT NULL,
                        start_time DATETIME,
                        end_time DATETIME,
                        status TEXT,
                        records_processed INTEGER,
                        errors_count INTEGER,
                        error_details TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Customer data table (example target)
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS customers (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        customer_id TEXT UNIQUE NOT NULL,
                        first_name TEXT,
                        last_name TEXT,
                        email TEXT,
                        phone TEXT,
                        address TEXT,
                        city TEXT,
                        state TEXT,
                        zip_code TEXT,
                        country TEXT,
                        registration_date DATE,
                        last_purchase_date DATE,
                        total_purchases DECIMAL(10,2),
                        customer_segment TEXT,
                        is_active BOOLEAN DEFAULT 1,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                # Sales data table (example target)
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS sales (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        transaction_id TEXT UNIQUE NOT NULL,
                        customer_id TEXT,
                        product_id TEXT,
                        product_name TEXT,
                        category TEXT,
                        quantity INTEGER,
                        unit_price DECIMAL(10,2),
                        total_amount DECIMAL(10,2),
                        discount_amount DECIMAL(10,2),
                        tax_amount DECIMAL(10,2),
                        sale_date DATE,
                        sales_rep TEXT,
                        region TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (customer_id) REFERENCES customers (customer_id)
                    )
                ''')

                # Data quality issues table
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS data_quality_issues (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        job_name TEXT,
                        table_name TEXT,
                        issue_type TEXT,
                        issue_description TEXT,
                        record_id TEXT,
                        field_name TEXT,
                        field_value TEXT,
                        severity TEXT,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')

                conn.commit()
                logger.info("‚úÖ Database setup completed")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Database setup failed: {e}")
            raise

    def create_sample_data(self):
        """
        Create sample data files for demonstration.

        In a real ETL pipeline, this data would come from external systems.
        """
        logger.info("üìä Creating sample data files...")

        # Create sample customer data (CSV)
        customer_data = []
        for i in range(1000):
            customer = {
                'customer_id': f'CUST{i+1:06d}',
                'first_name': np.random.choice(['John', 'Jane', 'Mike', 'Sarah', 'David', 'Lisa', 'Chris', 'Amy']),
                'last_name': np.random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis']),
                'email': f'customer{i+1}@email.com',
                'phone': f'+1-555-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}',
                'address': f'{np.random.randint(1, 9999)} Main St',
                'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia']),
                'state': np.random.choice(['NY', 'CA', 'IL', 'TX', 'AZ', 'PA']),
                'zip_code': f'{np.random.randint(10000, 99999)}',
                'country': 'USA',
                'registration_date': (datetime.now() - timedelta(days=np.random.randint(1, 365))).strftime('%Y-%m-%d'),
                'total_purchases': round(np.random.uniform(100, 5000), 2),
                'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'])
            }

            # Introduce some data quality issues for demonstration
            if np.random.random() < 0.05:  # 5% bad email addresses
                customer['email'] = 'invalid-email'
            if np.random.random() < 0.03:  # 3% missing phone numbers
                customer['phone'] = ''
            if np.random.random() < 0.02:  # 2% invalid dates
                customer['registration_date'] = 'invalid-date'

            customer_data.append(customer)

        df_customers = pd.DataFrame(customer_data)
        customer_file = Path(self.config.source_directory) / 'customers.csv'
        df_customers.to_csv(customer_file, index=False)
        logger.info(f"‚úÖ Created customer data: {customer_file}")

        # Create sample sales data (Excel)
        sales_data = []
        for i in range(2000):
            sale = {
                'transaction_id': f'TXN{i+1:08d}',
                'customer_id': f'CUST{np.random.randint(1, 1001):06d}',
                'product_id': f'PROD{np.random.randint(1, 101):04d}',
                'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Tablet', 'Phone', 'Headphones']),
                'category': np.random.choice(['Electronics', 'Accessories', 'Computers']),
                'quantity': np.random.randint(1, 5),
                'unit_price': round(np.random.uniform(10, 1000), 2),
                'discount_amount': round(np.random.uniform(0, 50), 2),
                'tax_amount': round(np.random.uniform(5, 100), 2),
                'sale_date': (datetime.now() - timedelta(days=np.random.randint(1, 90))).strftime('%Y-%m-%d'),
                'sales_rep': np.random.choice(['Alice Johnson', 'Bob Smith', 'Carol Brown', 'David Wilson']),
                'region': np.random.choice(['North', 'South', 'East', 'West'])
            }

            # Calculate total amount
            sale['total_amount'] = round(
                (sale['unit_price'] * sale['quantity']) - sale['discount_amount'] + sale['tax_amount'], 2
            )

            # Introduce some data quality issues
            if np.random.random() < 0.01:  # 1% negative quantities
                sale['quantity'] = -1
            if np.random.random() < 0.02:  # 2% missing product names
                sale['product_name'] = ''

            sales_data.append(sale)

        df_sales = pd.DataFrame(sales_data)
        sales_file = Path(self.config.source_directory) / 'sales.xlsx'
        df_sales.to_excel(sales_file, index=False)
        logger.info(f"‚úÖ Created sales data: {sales_file}")

        # Create sample JSON data
        json_data = {
            'metadata': {
                'source': 'external_api',
                'extraction_date': datetime.now().isoformat(),
                'version': '1.0'
            },
            'customer_updates': [
                {
                    'customer_id': f'CUST{i+1:06d}',
                    'last_purchase_date': (datetime.now() - timedelta(days=np.random.randint(1, 30))).strftime('%Y-%m-%d'),
                    'updated_segment': np.random.choice(['Premium', 'Standard', 'Basic'])
                }
                for i in range(100)
            ]
        }

        json_file = Path(self.config.source_directory) / 'customer_updates.json'
        with open(json_file, 'w') as f:
            json.dump(json_data, f, indent=2)
        logger.info(f"‚úÖ Created JSON data: {json_file}")

    def extract_csv_data(self, file_path: Path) -> pd.DataFrame:
        """
        Extract data from CSV files with error handling.

        Args:
            file_path (Path): Path to the CSV file

        Returns:
            pd.DataFrame: Extracted data
        """
        logger.info(f"üì§ Extracting data from CSV: {file_path}")

        try:
            # Read CSV with error handling
            df = pd.read_csv(file_path, encoding='utf-8')

            # Log extraction metrics
            self.metrics['records_extracted'] += len(df)
            logger.info(f"‚úÖ Extracted {len(df)} records from {file_path.name}")

            return df

        except Exception as e:
            logger.error(f"‚ùå Failed to extract data from {file_path}: {e}")
            self.metrics['errors_count'] += 1
            return pd.DataFrame()

    def extract_excel_data(self, file_path: Path, sheet_name: str = None) -> pd.DataFrame:
        """
        Extract data from Excel files.

        Args:
            file_path (Path): Path to the Excel file
            sheet_name (str): Specific sheet to read

        Returns:
            pd.DataFrame: Extracted data
        """
        logger.info(f"üì§ Extracting data from Excel: {file_path}")

        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name)

            self.metrics['records_extracted'] += len(df)
            logger.info(f"‚úÖ Extracted {len(df)} records from {file_path.name}")

            return df

        except Exception as e:
            logger.error(f"‚ùå Failed to extract data from {file_path}: {e}")
            self.metrics['errors_count'] += 1
            return pd.DataFrame()

    def extract_json_data(self, file_path: Path) -> Dict:
        """
        Extract data from JSON files.

        Args:
            file_path (Path): Path to the JSON file

        Returns:
            Dict: Extracted JSON data
        """
        logger.info(f"üì§ Extracting data from JSON: {file_path}")

        try:
            with open(file_path, 'r') as f:
                data = json.load(f)

            logger.info(f"‚úÖ Extracted JSON data from {file_path.name}")
            return data

        except Exception as e:
            logger.error(f"‚ùå Failed to extract data from {file_path}: {e}")
            self.metrics['errors_count'] += 1
            return {}

    def transform_customer_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Transform customer data according to business rules.

        Args:
            df (pd.DataFrame): Raw customer data

        Returns:
            pd.DataFrame: Transformed customer data
        """
        logger.info("üîÑ Transforming customer data...")

        if df.empty:
            return df

        df_transformed = df.copy()

        # Data cleaning and standardization
        # 1. Standardize names (title case)
        df_transformed['first_name'] = df_transformed['first_name'].str.title()
        df_transformed['last_name'] = df_transformed['last_name'].str.title()

        # 2. Clean and validate email addresses
        df_transformed['email_valid'] = df_transformed['email'].apply(self.validator.validate_email)
        df_transformed.loc[~df_transformed['email_valid'], 'email'] = None

        # 3. Clean and validate phone numbers
        df_transformed['phone_valid'] = df_transformed['phone'].apply(self.validator.validate_phone)
        df_transformed.loc[~df_transformed['phone_valid'], 'phone'] = None

        # 4. Validate registration dates
        df_transformed['date_valid'] = df_transformed['registration_date'].apply(
            lambda x: self.validator.validate_date(x)
        )
        df_transformed.loc[~df_transformed['date_valid'], 'registration_date'] = None

        # 5. Standardize state codes
        df_transformed['state'] = df_transformed['state'].str.upper()

        # 6. Clean zip codes (remove any non-numeric characters)
        df_transformed['zip_code'] = df_transformed['zip_code'].astype(str).str.replace(r'\D', '', regex=True)

        # 7. Set active status based on recent activity
        df_transformed['registration_date_parsed'] = pd.to_datetime(
            df_transformed['registration_date'], errors='coerce'
        )
        cutoff_date = datetime.now() - timedelta(days=365)
        df_transformed['is_active'] = df_transformed['registration_date_parsed'] > cutoff_date

        # 8. Create derived fields
        df_transformed['full_name'] = (
            df_transformed['first_name'].fillna('') + ' ' + df_transformed['last_name'].fillna('')
        ).str.strip()

        # 9. Categorize customers by purchase amount
        df_transformed['purchase_category'] = pd.cut(
            df_transformed['total_purchases'],
            bins=[0, 500, 1500, float('inf')],
            labels=['Low', 'Medium', 'High']
        )

        # Remove validation helper columns
        columns_to_drop = ['email_valid', 'phone_valid', 'date_valid', 'registration_date_parsed']
        df_transformed = df_transformed.drop(columns=columns_to_drop, errors='ignore')

        self.metrics['records_transformed'] += len(df_transformed)
        logger.info(f"‚úÖ Transformed {len(df_transformed)} customer records")

        return df_transformed

    def transform_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Transform sales data according to business rules.

        Args:
            df (pd.DataFrame): Raw sales data

        Returns:
            pd.DataFrame: Transformed sales data
        """
        logger.info("üîÑ Transforming sales data...")

        if df.empty:
            return df

        df_transformed = df.copy()

        # Data cleaning and validation
        # 1. Ensure positive quantities
        df_transformed.loc[df_transformed['quantity'] <= 0, 'quantity'] = 1

        # 2. Ensure positive prices
        df_transformed.loc[df_transformed['unit_price'] <= 0, 'unit_price'] = 0.01

        # 3. Recalculate total amount to ensure consistency
        df_transformed['calculated_total'] = (
            (df_transformed['unit_price'] * df_transformed['quantity']) -
            df_transformed['discount_amount'] +
            df_transformed['tax_amount']
        ).round(2)

        # Flag discrepancies
        df_transformed['amount_discrepancy'] = abs(
            df_transformed['total_amount'] - df_transformed['calculated_total']
        ) > 0.01

        # Use calculated total for consistency
        df_transformed['total_amount'] = df_transformed['calculated_total']

        # 4. Standardize product names
        df_transformed['product_name'] = df_transformed['product_name'].str.title()

        # 5. Validate and convert sale dates
        df_transformed['sale_date_parsed'] = pd.to_datetime(
            df_transformed['sale_date'], errors='coerce'
        )

        # 6. Create derived fields
        df_transformed['profit_margin'] = (
            (df_transformed['total_amount'] - df_transformed['discount_amount']) /
            df_transformed['total_amount'] * 100
        ).round(2)

        # 7. Categorize sales by amount
        df_transformed['sale_category'] = pd.cut(
            df_transformed['total_amount'],
            bins=[0, 100, 500, 1000, float('inf')],
            labels=['Small', 'Medium', 'Large', 'Enterprise']
        )

        # 8. Add time-based features
        df_transformed['sale_month'] = df_transformed['sale_date_parsed'].dt.month
        df_transformed['sale_quarter'] = df_transformed['sale_date_parsed'].dt.quarter
        df_transformed['sale_year'] = df_transformed['sale_date_parsed'].dt.year

        # Remove helper columns
        df_transformed = df_transformed.drop(columns=['calculated_total', 'sale_date_parsed'], errors='ignore')

        self.metrics['records_transformed'] += len(df_transformed)
        logger.info(f"‚úÖ Transformed {len(df_transformed)} sales records")

        return df_transformed

    def validate_data_quality(self, df: pd.DataFrame, table_name: str, required_fields: List[str]) -> List[Dict]:
        """
        Validate data quality and log issues.

        Args:
            df (pd.DataFrame): Data to validate
            table_name (str): Name of the target table
            required_fields (List[str]): List of required fields

        Returns:
            List[Dict]: List of data quality issues
        """
        logger.info(f"üîç Validating data quality for {table_name}...")

        issues = []

        # Check completeness
        completeness = self.validator.check_data_completeness(df, required_fields)
        for field, percentage in completeness.items():
            if percentage < 95:  # Less than 95% complete
                issues.append({
                    'job_name': f'{table_name}_etl',
                    'table_name': table_name,
                    'issue_type': 'completeness',
                    'issue_description': f'Field {field} is only {percentage:.1f}% complete',
                    'field_name': field,
                    'severity': 'high' if percentage < 80 else 'medium'
                })

        # Check for duplicates
        if table_name == 'customers':
            duplicates = self.validator.detect_duplicates(df, ['customer_id'])
            if not duplicates.empty:
                issues.append({
                    'job_name': f'{table_name}_etl',
                    'table_name': table_name,
                    'issue_type': 'duplicates',
                    'issue_description': f'Found {len(duplicates)} duplicate customer IDs',
                    'field_name': 'customer_id',
                    'severity': 'high'
                })

        elif table_name == 'sales':
            duplicates = self.validator.detect_duplicates(df, ['transaction_id'])
            if not duplicates.empty:
                issues.append({
                    'job_name': f'{table_name}_etl',
                    'table_name': table_name,
                    'issue_type': 'duplicates',
                    'issue_description': f'Found {len(duplicates)} duplicate transaction IDs',
                    'field_name': 'transaction_id',
                    'severity': 'high'
                })

        # Log issues to database
        if issues:
            self.log_data_quality_issues(issues)

        logger.info(f"‚úÖ Data quality validation completed. Found {len(issues)} issues.")
        return issues

    def log_data_quality_issues(self, issues: List[Dict]):
        """Log data quality issues to the database."""
        try:
            with sqlite3.connect(self.config.database_path) as conn:
                cursor = conn.cursor()
                for issue in issues:
                    cursor.execute('''
                        INSERT INTO data_quality_issues
                        (job_name, table_name, issue_type, issue_description,
                         field_name, severity)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        issue['job_name'],
                        issue['table_name'],
                        issue['issue_type'],
                        issue['issue_description'],
                        issue.get('field_name'),
                        issue['severity']
                    ))
                conn.commit()
        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to log data quality issues: {e}")

    def load_data_to_database(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> bool:
        """
        Load transformed data into the target database.

        Args:
            df (pd.DataFrame): Data to load
            table_name (str): Target table name
            if_exists (str): What to do if table exists ('append', 'replace', 'fail')

        Returns:
            bool: True if successful, False otherwise
        """
        logger.info(f"üì• Loading {len(df)} records to {table_name}...")

        if df.empty:
            logger.warning(f"‚ö†Ô∏è No data to load for {table_name}")
            return True

        try:
            with sqlite3.connect(self.config.database_path) as conn:
                # Load data in batches to avoid memory issues
                batch_size = self.config.batch_size
                total_batches = (len(df) + batch_size - 1) // batch_size

                for i in range(0, len(df), batch_size):
                    batch_df = df.iloc[i:i + batch_size]
                    batch_num = (i // batch_size) + 1

                    logger.info(f"  Loading batch {batch_num}/{total_batches} ({len(batch_df)} records)")

                    batch_df.to_sql(
                        table_name,
                        conn,
                        if_exists=if_exists if i == 0 else 'append',
                        index=False,
                        method='multi'
                    )

                self.metrics['records_loaded'] += len(df)
                logger.info(f"‚úÖ Successfully loaded {len(df)} records to {table_name}")
                return True

        except Exception as e:
            logger.error(f"‚ùå Failed to load data to {table_name}: {e}")
            self.metrics['errors_count'] += 1
            return False

    def archive_processed_files(self, file_paths: List[Path]):
        """
        Archive processed files with timestamp.

        Args:
            file_paths (List[Path]): List of files to archive
        """
        logger.info("üì¶ Archiving processed files...")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        for file_path in file_paths:
            if file_path.exists():
                archive_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
                archive_path = Path(self.config.archive_directory) / archive_name

                try:
                    shutil.move(str(file_path), str(archive_path))
                    logger.info(f"üì¶ Archived: {file_path.name} -> {archive_path.name}")
                except Exception as e:
                    logger.error(f"‚ùå Failed to archive {file_path.name}: {e}")

    def log_etl_job(self, job_name: str, status: str):
        """
        Log ETL job execution details.

        Args:
            job_name (str): Name of the ETL job
            status (str): Job status (success, failed, warning)
        """
        try:
            with sqlite3.connect(self.config.database_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO etl_job_log
                    (job_name, start_time, end_time, status, records_processed, errors_count)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    job_name,
                    self.metrics['start_time'],
                    self.metrics['end_time'],
                    status,
                    self.metrics['records_loaded'],
                    self.metrics['errors_count']
                ))
                conn.commit()
        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to log ETL job: {e}")

    def run_customer_etl(self):
        """Run the complete customer data ETL pipeline."""
        logger.info("üöÄ Starting customer ETL pipeline...")

        # Extract
        customer_file = Path(self.config.source_directory) / 'customers.csv'
        df_customers = self.extract_csv_data(customer_file)

        if not df_customers.empty:
            # Transform
            df_transformed = self.transform_customer_data(df_customers)

            # Validate
            required_fields = ['customer_id', 'first_name', 'last_name', 'email']
            self.validate_data_quality(df_transformed, 'customers', required_fields)

            # Load
            success = self.load_data_to_database(df_transformed, 'customers', 'replace')

            if success:
                # Archive processed file
                self.archive_processed_files([customer_file])
                logger.info("‚úÖ Customer ETL pipeline completed successfully")
                return True
            else:
                logger.error("‚ùå Customer ETL pipeline failed during load phase")
                return False
        else:
            logger.error("‚ùå Customer ETL pipeline failed - no data extracted")
            return False

    def run_sales_etl(self):
        """Run the complete sales data ETL pipeline."""
        logger.info("üöÄ Starting sales ETL pipeline...")

        # Extract
        sales_file = Path(self.config.source_directory) / 'sales.xlsx'
        df_sales = self.extract_excel_data(sales_file)

        if not df_sales.empty:
            # Transform
            df_transformed = self.transform_sales_data(df_sales)

            # Validate
            required_fields = ['transaction_id', 'customer_id', 'product_name', 'quantity', 'unit_price']
            self.validate_data_quality(df_transformed, 'sales', required_fields)

            # Load
            success = self.load_data_to_database(df_transformed, 'sales', 'replace')

            if success:
                # Archive processed file
                self.archive_processed_files([sales_file])
                logger.info("‚úÖ Sales ETL pipeline completed successfully")
                return True
            else:
                logger.error("‚ùå Sales ETL pipeline failed during load phase")
                return False
        else:
            logger.error("‚ùå Sales ETL pipeline failed - no data extracted")
            return False

    def run_incremental_update(self):
        """Run incremental updates from JSON data."""
        logger.info("üöÄ Starting incremental update pipeline...")

        # Extract JSON data
        json_file = Path(self.config.source_directory) / 'customer_updates.json'
        json_data = self.extract_json_data(json_file)

        if json_data and 'customer_updates' in json_data:
            updates = json_data['customer_updates']
            logger.info(f"Processing {len(updates)} customer updates...")

            try:
                with sqlite3.connect(self.config.database_path) as conn:
                    cursor = conn.cursor()

                    for update in updates:
                        cursor.execute('''
                            UPDATE customers
                            SET last_purchase_date = ?, customer_segment = ?, updated_at = CURRENT_TIMESTAMP
                            WHERE customer_id = ?
                        ''', (
                            update['last_purchase_date'],
                            update['updated_segment'],
                            update['customer_id']
                        ))

                    conn.commit()
                    logger.info(f"‚úÖ Applied {len(updates)} incremental updates")

                    # Archive processed file
                    self.archive_processed_files([json_file])
                    return True

            except sqlite3.Error as e:
                logger.error(f"‚ùå Incremental update failed: {e}")
                return False
        else:
            logger.error("‚ùå No update data found in JSON file")
            return False

    def generate_etl_report(self):
        """Generate a comprehensive ETL execution report."""
        logger.info("üìä Generating ETL report...")

        try:
            with sqlite3.connect(self.config.database_path) as conn:
                # Get job summary
                df_jobs = pd.read_sql_query('''
                    SELECT job_name, status, records_processed, errors_count,
                           start_time, end_time
                    FROM etl_job_log
                    ORDER BY start_time DESC
                    LIMIT 10
                ''', conn)

                # Get data quality issues
                df_issues = pd.read_sql_query('''
                    SELECT table_name, issue_type, severity, COUNT(*) as count
                    FROM data_quality_issues
                    WHERE created_at >= datetime('now', '-1 day')
                    GROUP BY table_name, issue_type, severity
                ''', conn)

                # Get record counts
                customer_count = pd.read_sql_query('SELECT COUNT(*) as count FROM customers', conn).iloc[0]['count']
                sales_count = pd.read_sql_query('SELECT COUNT(*) as count FROM sales', conn).iloc[0]['count']

                print("\nüìà ETL Execution Report")
                print("=" * 50)

                print(f"\nüìä Current Data Status:")
                print(f"  Customers: {customer_count:,} records")
                print(f"  Sales: {sales_count:,} records")

                if not df_jobs.empty:
                    print(f"\nüìã Recent ETL Jobs:")
                    print(df_jobs.to_string(index=False))

                if not df_issues.empty:
                    print(f"\n‚ö†Ô∏è Data Quality Issues (Last 24h):")
                    print(df_issues.to_string(index=False))
                else:
                    print(f"\n‚úÖ No data quality issues found in the last 24 hours")

                # Runtime metrics
                if self.metrics['start_time'] and self.metrics['end_time']:
                    duration = (self.metrics['end_time'] - self.metrics['start_time']).total_seconds()
                    print(f"\n‚è±Ô∏è Pipeline Metrics:")
                    print(f"  Execution time: {duration:.2f} seconds")
                    print(f"  Records extracted: {self.metrics['records_extracted']:,}")
                    print(f"  Records transformed: {self.metrics['records_transformed']:,}")
                    print(f"  Records loaded: {self.metrics['records_loaded']:,}")
                    print(f"  Errors encountered: {self.metrics['errors_count']}")

        except sqlite3.Error as e:
            logger.error(f"‚ùå Failed to generate ETL report: {e}")

    def run_full_etl_pipeline(self):
        """
        Run the complete ETL pipeline with all components.

        This is the main orchestration method that runs all ETL processes.
        """
        logger.info("üöÄ Starting full ETL pipeline...")

        self.metrics['start_time'] = datetime.now()
        overall_success = True

        try:
            # Create sample data (in production, this would be skipped)
            self.create_sample_data()

            # Run customer ETL
            if not self.run_customer_etl():
                overall_success = False

            # Run sales ETL
            if not self.run_sales_etl():
                overall_success = False

            # Run incremental updates
            if not self.run_incremental_update():
                overall_success = False

            self.metrics['end_time'] = datetime.now()

            # Log the overall job
            status = 'success' if overall_success else 'failed'
            self.log_etl_job('full_pipeline', status)

            # Generate report
            self.generate_etl_report()

            if overall_success:
                logger.info("üéâ Full ETL pipeline completed successfully!")
            else:
                logger.error("‚ùå ETL pipeline completed with errors")

            return overall_success

        except Exception as e:
            self.metrics['end_time'] = datetime.now()
            logger.error(f"‚ùå ETL pipeline failed: {e}")
            self.log_etl_job('full_pipeline', 'failed')
            return False

def demonstrate_advanced_etl_patterns():
    """
    Demonstrate advanced ETL patterns and concepts.
    """
    print("\nüéì Advanced ETL Patterns & Concepts:")
    print("=" * 50)

    concepts = {
        "Change Data Capture (CDC)": [
            "Track changes in source systems",
            "Use timestamps or version numbers",
            "Implement incremental loading strategies"
        ],
        "Data Lineage": [
            "Track data flow from source to target",
            "Document transformation rules",
            "Enable impact analysis for changes"
        ],
        "Error Handling Strategies": [
            "Dead letter queues for bad records",
            "Circuit breakers for external systems",
            "Retry mechanisms with exponential backoff"
        ],
        "Performance Optimization": [
            "Parallel processing for large datasets",
            "Partitioning strategies",
            "Indexing for fast lookups"
        ],
        "Data Governance": [
            "Data quality scorecards",
            "Automated data profiling",
            "Compliance and audit trails"
        ]
    }

    for concept, details in concepts.items():
        print(f"\nüîß {concept}:")
        for detail in details:
            print(f"  ‚Ä¢ {detail}")

def main():
    """
    Main function demonstrating the complete ETL automation workflow.
    """
    print("üöÄ Welcome to PL's ETL Automation Demo!")
    print("=" * 50)

    try:
        # Initialize ETL pipeline with configuration
        config = ETLConfig()
        etl_pipeline = ETLPipeline(config)

        # Run the full ETL pipeline
        success = etl_pipeline.run_full_etl_pipeline()

        if success:
            print("\n‚úÖ ETL automation demonstration completed successfully!")

            # Show database file info
            db_path = Path(config.database_path)
            if db_path.exists():
                size_mb = db_path.stat().st_size / (1024 * 1024)
                print(f"\nüìÅ Database file: {db_path.absolute()}")
                print(f"   Size: {size_mb:.2f} MB")

            print(f"\nüìÇ Generated directories:")
            for directory in [config.source_directory, config.staging_directory,
                             config.archive_directory, config.output_directory]:
                dir_path = Path(directory)
                if dir_path.exists():
                    file_count = len(list(dir_path.iterdir()))
                    print(f"   {directory}: {file_count} files")

        else:
            print("\n‚ùå ETL pipeline encountered errors. Check the logs for details.")

    except Exception as e:
        logger.error(f"‚ùå ETL demonstration failed: {e}")
        print("üí° Troubleshooting tips:")
        print("   - Ensure you have write permissions in the current directory")
        print("   - Check available disk space")
        print("   - Verify all dependencies are installed")

    # Show advanced patterns
    demonstrate_advanced_etl_patterns()

    print("\nüéì Learning Summary:")
    print("- ETL pipeline design and implementation")
    print("- Data extraction from multiple sources")
    print("- Data transformation and cleaning techniques")
    print("- Data quality validation and monitoring")
    print("- Database operations and batch processing")
    print("- Error handling and logging strategies")

    print("\nüí° Next Steps:")
    print("- Implement real-time streaming ETL with Apache Kafka")
    print("- Add data profiling and automated quality checks")
    print("- Integrate with cloud data warehouses (Snowflake, BigQuery)")
    print("- Build data lineage and impact analysis tools")
    print("- Set up monitoring and alerting for production ETL jobs")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/SW_email_filter_urgency.py

```python
#!/usr/bin/env python3
# /// script
# dependencies = ["anthropic", "python-dotenv"]
# ///

"""
SW_email_filter_urgency.py
Student: SW - "Filtering emails for specific topics and levels of urgent replies req'd"

This script demonstrates email filtering and urgency classification using AI.
It simulates email data and uses AI to categorize emails by urgency level and topics.

Key Learning Objectives:
- Text analysis and classification
- AI-powered content filtering
- Data organization and reporting
- Email automation concepts
"""

import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dotenv import load_dotenv
import anthropic

# Load environment variables
load_dotenv()

class EmailUrgencyFilter:
    """
    A class to filter and categorize emails by urgency and topics.
    In a real implementation, this would connect to email APIs like Gmail or Outlook.
    """

    def __init__(self):
        """Initialize the email filter with AI client."""
        # Initialize Anthropic client for AI-powered analysis
        self.ai_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # Define urgency levels for classification
        self.urgency_levels = {
            "critical": "Immediate action required (within 1 hour)",
            "high": "Response needed today",
            "medium": "Response needed within 2-3 days",
            "low": "Response can wait a week or more"
        }

        # Common business topics to filter for
        self.topics = [
            "customer_support", "sales_inquiry", "technical_issue",
            "meeting_request", "project_update", "billing",
            "partnership", "feedback", "internal_communication"
        ]

    def get_sample_emails(self) -> List[Dict[str, Any]]:
        """
        Generate sample email data for demonstration.
        In a real scenario, this would fetch from email APIs.
        """
        sample_emails = [
            {
                "id": "email_001",
                "sender": "customer@urgentcorp.com",
                "subject": "URGENT: Production server down - immediate assistance needed",
                "body": "Our production server has been down for 2 hours. This is affecting all our customers. Please help immediately!",
                "received_time": datetime.now() - timedelta(hours=1),
                "attachments": []
            },
            {
                "id": "email_002",
                "sender": "sales@potential-client.com",
                "subject": "Partnership opportunity discussion",
                "body": "Hi, we're interested in exploring a potential partnership. Could we schedule a call next week to discuss?",
                "received_time": datetime.now() - timedelta(hours=3),
                "attachments": []
            },
            {
                "id": "email_003",
                "sender": "support@currentclient.com",
                "subject": "Question about invoice #12345",
                "body": "I have a question about the charges on invoice #12345. When you have a chance, could you explain the additional fees?",
                "received_time": datetime.now() - timedelta(days=1),
                "attachments": ["invoice_12345.pdf"]
            },
            {
                "id": "email_004",
                "sender": "team@company.com",
                "subject": "Weekly team meeting moved to Friday",
                "body": "FYI - our weekly team meeting has been moved from Thursday to Friday at 2 PM. See you there!",
                "received_time": datetime.now() - timedelta(hours=6),
                "attachments": []
            },
            {
                "id": "email_005",
                "sender": "bug-reports@client.com",
                "subject": "Critical bug in payment processing",
                "body": "We've discovered a critical bug in the payment processing module that's preventing customers from completing purchases. This needs immediate attention.",
                "received_time": datetime.now() - timedelta(minutes=30),
                "attachments": ["error_logs.txt", "screenshot.png"]
            }
        ]

        return sample_emails

    def analyze_email_with_ai(self, email: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use AI to analyze email content and determine urgency and topics.

        Args:
            email: Dictionary containing email data

        Returns:
            Dictionary with urgency level, topics, and reasoning
        """

        # Prepare the email content for analysis
        email_content = f"""
        Subject: {email['subject']}
        From: {email['sender']}
        Body: {email['body']}
        Attachments: {', '.join(email['attachments']) if email['attachments'] else 'None'}
        """

        # Create AI prompt for email analysis
        prompt = f"""
        Analyze this email and provide:
        1. Urgency level (critical, high, medium, low)
        2. Primary topic category from: {', '.join(self.topics)}
        3. Brief reasoning for the urgency classification
        4. Suggested response timeframe

        Email to analyze:
        {email_content}

        Respond in JSON format:
        {{
            "urgency": "level",
            "topic": "category",
            "reasoning": "explanation",
            "response_timeframe": "timeframe",
            "key_indicators": ["indicator1", "indicator2"]
        }}
        """

        try:
            # Make API call to Anthropic
            response = self.ai_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=500,
                messages=[{"role": "user", "content": prompt}]
            )

            # Parse the AI response
            ai_analysis = json.loads(response.content[0].text)
            return ai_analysis

        except Exception as e:
            print(f"AI analysis failed for email {email['id']}: {str(e)}")
            # Fallback to rule-based analysis
            return self.fallback_analysis(email)

    def fallback_analysis(self, email: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fallback rule-based analysis when AI is unavailable.

        Args:
            email: Dictionary containing email data

        Returns:
            Dictionary with basic urgency and topic classification
        """

        subject_lower = email['subject'].lower()
        body_lower = email['body'].lower()

        # Rule-based urgency detection
        urgency = "low"  # default

        urgent_keywords = ["urgent", "emergency", "critical", "immediately", "asap", "down", "broken"]
        high_keywords = ["important", "priority", "soon", "today"]

        if any(keyword in subject_lower or keyword in body_lower for keyword in urgent_keywords):
            urgency = "critical"
        elif any(keyword in subject_lower or keyword in body_lower for keyword in high_keywords):
            urgency = "high"
        elif "question" in subject_lower or "invoice" in subject_lower:
            urgency = "medium"

        # Simple topic detection
        topic = "internal_communication"  # default

        if "support" in email['sender'] or "bug" in subject_lower:
            topic = "customer_support"
        elif "sales" in email['sender'] or "partnership" in subject_lower:
            topic = "sales_inquiry"
        elif "invoice" in subject_lower or "billing" in subject_lower:
            topic = "billing"
        elif "meeting" in subject_lower:
            topic = "meeting_request"

        return {
            "urgency": urgency,
            "topic": topic,
            "reasoning": "Rule-based analysis (AI unavailable)",
            "response_timeframe": self.urgency_levels[urgency],
            "key_indicators": ["keyword_matching"]
        }

    def filter_and_categorize_emails(self, emails: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Process all emails and organize them by urgency and topic.

        Args:
            emails: List of email dictionaries

        Returns:
            Organized email data with analysis results
        """

        print("üîç Starting email analysis and filtering...")

        # Process each email
        analyzed_emails = []
        for email in emails:
            print(f"üìß Analyzing: {email['subject'][:50]}...")

            # Get AI analysis
            analysis = self.analyze_email_with_ai(email)

            # Combine original email with analysis
            email_with_analysis = {
                **email,
                "analysis": analysis
            }
            analyzed_emails.append(email_with_analysis)

        # Organize by urgency
        emails_by_urgency = {level: [] for level in self.urgency_levels.keys()}
        emails_by_topic = {topic: [] for topic in self.topics}

        for email in analyzed_emails:
            urgency = email['analysis']['urgency']
            topic = email['analysis']['topic']

            emails_by_urgency[urgency].append(email)
            emails_by_topic[topic].append(email)

        return {
            "total_emails": len(emails),
            "analyzed_emails": analyzed_emails,
            "by_urgency": emails_by_urgency,
            "by_topic": emails_by_topic,
            "analysis_timestamp": datetime.now().isoformat()
        }

    def generate_urgency_report(self, filtered_data: Dict[str, Any]) -> str:
        """
        Generate a detailed report of email urgency analysis.

        Args:
            filtered_data: Organized email data from filtering

        Returns:
            Formatted report string
        """

        report = []
        report.append("=" * 60)
        report.append("üìß EMAIL URGENCY ANALYSIS REPORT")
        report.append("=" * 60)
        report.append(f"Analysis completed: {filtered_data['analysis_timestamp']}")
        report.append(f"Total emails processed: {filtered_data['total_emails']}")
        report.append("")

        # Urgency breakdown
        report.append("üö® URGENCY BREAKDOWN:")
        report.append("-" * 30)

        for urgency, emails in filtered_data['by_urgency'].items():
            count = len(emails)
            if count > 0:
                report.append(f"  {urgency.upper()}: {count} emails")
                report.append(f"    Action needed: {self.urgency_levels[urgency]}")

                for email in emails:
                    report.append(f"    ‚Ä¢ {email['subject'][:40]}... (from: {email['sender']})")
                report.append("")

        # Topic breakdown
        report.append("üìÇ TOPIC BREAKDOWN:")
        report.append("-" * 30)

        for topic, emails in filtered_data['by_topic'].items():
            count = len(emails)
            if count > 0:
                report.append(f"  {topic.replace('_', ' ').title()}: {count} emails")

        report.append("")

        # Immediate action items
        critical_emails = filtered_data['by_urgency']['critical']
        high_emails = filtered_data['by_urgency']['high']

        if critical_emails or high_emails:
            report.append("‚ö° IMMEDIATE ACTION REQUIRED:")
            report.append("-" * 30)

            for email in critical_emails + high_emails:
                report.append(f"  üìç {email['analysis']['urgency'].upper()}: {email['subject']}")
                report.append(f"     From: {email['sender']}")
                report.append(f"     Reason: {email['analysis']['reasoning']}")
                report.append(f"     Action: {email['analysis']['response_timeframe']}")
                report.append("")

        return "\n".join(report)

def main():
    """
    Main function to demonstrate email filtering and urgency analysis.
    """

    print("üöÄ Email Urgency Filter Demo")
    print("=" * 40)

    # Initialize the email filter
    email_filter = EmailUrgencyFilter()

    # Get sample emails (in real app, this would fetch from email API)
    print("üì• Loading sample emails...")
    emails = email_filter.get_sample_emails()
    print(f"Loaded {len(emails)} emails for analysis")
    print()

    # Filter and categorize emails
    filtered_data = email_filter.filter_and_categorize_emails(emails)

    # Generate and display report
    print("\n" + "=" * 60)
    report = email_filter.generate_urgency_report(filtered_data)
    print(report)

    # Save results to file for further processing
    output_file = "email_urgency_analysis.json"
    with open(output_file, 'w') as f:
        # Convert datetime objects to strings for JSON serialization
        serializable_data = {
            "total_emails": filtered_data["total_emails"],
            "analysis_timestamp": filtered_data["analysis_timestamp"],
            "urgency_summary": {
                urgency: len(emails)
                for urgency, emails in filtered_data["by_urgency"].items()
            },
            "topic_summary": {
                topic: len(emails)
                for topic, emails in filtered_data["by_topic"].items()
                if len(emails) > 0
            }
        }
        json.dump(serializable_data, f, indent=2)

    print(f"üíæ Analysis results saved to: {output_file}")

    # Demonstrate integration possibilities
    print("\nüîó INTEGRATION POSSIBILITIES:")
    print("- Connect to Gmail API for real email processing")
    print("- Set up automated email rules and forwarding")
    print("- Create dashboard for email analytics")
    print("- Send notifications for critical emails")
    print("- Integration with task management systems")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/TT_data_ingestion.py

```python
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "boto3",
#     "pandas",
#     "pyarrow",
#     "fastparquet",
#     "requests",
#     "sqlalchemy",
#     "psycopg2-binary",
# ]
# ///

"""
TT Data Ingestion
Student: TT
Description: Notebook script to handle big data ingestion from middleware,
data stored in Amazon AWS for further processing

This script demonstrates how to:
1. Connect to various data sources (APIs, databases, files)
2. Ingest large datasets efficiently using chunking and streaming
3. Transform and validate data during ingestion
4. Store data in AWS S3 and other cloud storage
5. Handle errors, retries, and data quality issues
6. Monitor ingestion progress and performance

Educational Focus:
- Working with AWS services (S3, RDS, etc.)
- Batch and streaming data processing
- Data validation and cleaning
- Error handling and resilience
- Performance optimization for large datasets
- Data pipeline design patterns
"""

import boto3
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Iterator, Any, Union
import requests
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from io import StringIO, BytesIO
import os
import tempfile

# Database connectivity (optional)
try:
    from sqlalchemy import create_engine, text
    HAS_SQLALCHEMY = True
except ImportError:
    HAS_SQLALCHEMY = False

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AWSDataManager:
    """
    Manages AWS connections and operations for data ingestion
    """

    def __init__(self, aws_access_key: str = None, aws_secret_key: str = None, region: str = 'us-east-1'):
        """
        Initialize AWS connection

        Args:
            aws_access_key: AWS access key (if None, uses environment variables or IAM roles)
            aws_secret_key: AWS secret key (if None, uses environment variables or IAM roles)
            region: AWS region
        """
        # Initialize AWS clients
        try:
            if aws_access_key and aws_secret_key:
                self.s3_client = boto3.client(
                    's3',
                    aws_access_key_id=aws_access_key,
                    aws_secret_access_key=aws_secret_key,
                    region_name=region
                )
            else:
                # Use default credential chain (environment variables, IAM roles, etc.)
                self.s3_client = boto3.client('s3', region_name=region)

            self.region = region
            logger.info(f"‚úÖ AWS S3 client initialized for region: {region}")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize AWS client: {e}")
            self.s3_client = None

    def create_bucket_if_not_exists(self, bucket_name: str) -> bool:
        """
        Create S3 bucket if it doesn't exist

        Args:
            bucket_name: Name of the S3 bucket

        Returns:
            True if bucket exists or was created successfully
        """
        if not self.s3_client:
            logger.error("AWS S3 client not initialized")
            return False

        try:
            # Check if bucket exists
            self.s3_client.head_bucket(Bucket=bucket_name)
            logger.info(f"‚úÖ Bucket '{bucket_name}' already exists")
            return True

        except self.s3_client.exceptions.NoSuchBucket:
            # Bucket doesn't exist, create it
            try:
                if self.region == 'us-east-1':
                    # us-east-1 doesn't need LocationConstraint
                    self.s3_client.create_bucket(Bucket=bucket_name)
                else:
                    self.s3_client.create_bucket(
                        Bucket=bucket_name,
                        CreateBucketConfiguration={'LocationConstraint': self.region}
                    )
                logger.info(f"‚úÖ Created bucket '{bucket_name}'")
                return True

            except Exception as e:
                logger.error(f"‚ùå Failed to create bucket '{bucket_name}': {e}")
                return False

        except Exception as e:
            logger.error(f"‚ùå Error checking bucket '{bucket_name}': {e}")
            return False

    def upload_dataframe_to_s3(self, df: pd.DataFrame, bucket_name: str, key: str,
                              format: str = 'parquet') -> bool:
        """
        Upload a pandas DataFrame to S3

        Args:
            df: DataFrame to upload
            bucket_name: S3 bucket name
            key: S3 key (file path)
            format: File format ('parquet', 'csv', 'json')

        Returns:
            True if upload successful
        """
        if not self.s3_client:
            logger.error("AWS S3 client not initialized")
            return False

        try:
            # Convert DataFrame to bytes based on format
            if format.lower() == 'parquet':
                buffer = BytesIO()
                df.to_parquet(buffer, index=False)
                content_type = 'application/octet-stream'
            elif format.lower() == 'csv':
                buffer = StringIO()
                df.to_csv(buffer, index=False)
                content_type = 'text/csv'
            elif format.lower() == 'json':
                buffer = StringIO()
                df.to_json(buffer, orient='records', lines=True)
                content_type = 'application/json'
            else:
                raise ValueError(f"Unsupported format: {format}")

            # Upload to S3
            if isinstance(buffer, StringIO):
                self.s3_client.put_object(
                    Bucket=bucket_name,
                    Key=key,
                    Body=buffer.getvalue(),
                    ContentType=content_type
                )
            else:
                self.s3_client.put_object(
                    Bucket=bucket_name,
                    Key=key,
                    Body=buffer.getvalue(),
                    ContentType=content_type
                )

            logger.info(f"‚úÖ Uploaded DataFrame to s3://{bucket_name}/{key}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to upload DataFrame to S3: {e}")
            return False

    def upload_file_to_s3(self, local_file_path: Union[str, Path], bucket_name: str, key: str) -> bool:
        """
        Upload a local file to S3

        Args:
            local_file_path: Path to local file
            bucket_name: S3 bucket name
            key: S3 key (file path)

        Returns:
            True if upload successful
        """
        if not self.s3_client:
            logger.error("AWS S3 client not initialized")
            return False

        try:
            local_path = Path(local_file_path)
            if not local_path.exists():
                logger.error(f"Local file not found: {local_path}")
                return False

            self.s3_client.upload_file(str(local_path), bucket_name, key)
            logger.info(f"‚úÖ Uploaded {local_path.name} to s3://{bucket_name}/{key}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to upload file to S3: {e}")
            return False

    def list_s3_objects(self, bucket_name: str, prefix: str = "") -> List[str]:
        """
        List objects in S3 bucket

        Args:
            bucket_name: S3 bucket name
            prefix: Prefix to filter objects

        Returns:
            List of object keys
        """
        if not self.s3_client:
            logger.error("AWS S3 client not initialized")
            return []

        try:
            response = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            objects = [obj['Key'] for obj in response.get('Contents', [])]
            logger.info(f"‚úÖ Found {len(objects)} objects in s3://{bucket_name}/{prefix}")
            return objects

        except Exception as e:
            logger.error(f"‚ùå Failed to list S3 objects: {e}")
            return []

class DataSourceConnector:
    """
    Connects to various data sources (APIs, databases, files)
    """

    def __init__(self):
        """Initialize the data source connector"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'TT-Data-Ingestion/1.0'
        })
        logger.info("‚úÖ Data Source Connector initialized")

    def fetch_api_data(self, url: str, params: Dict = None, headers: Dict = None,
                      auth: tuple = None, timeout: int = 30) -> Optional[Dict]:
        """
        Fetch data from REST API

        Args:
            url: API endpoint URL
            params: Query parameters
            headers: Additional headers
            auth: Authentication tuple (username, password)
            timeout: Request timeout in seconds

        Returns:
            JSON response data or None if failed
        """
        try:
            if headers:
                self.session.headers.update(headers)

            response = self.session.get(
                url,
                params=params,
                auth=auth,
                timeout=timeout
            )
            response.raise_for_status()

            logger.info(f"‚úÖ Successfully fetched data from {url}")
            return response.json()

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Failed to fetch data from {url}: {e}")
            return None

    def fetch_paginated_api_data(self, base_url: str, page_param: str = 'page',
                                page_size_param: str = 'per_page', page_size: int = 100,
                                max_pages: int = 10) -> Iterator[Dict]:
        """
        Fetch data from paginated API

        Args:
            base_url: Base API URL
            page_param: Parameter name for page number
            page_size_param: Parameter name for page size
            page_size: Number of items per page
            max_pages: Maximum number of pages to fetch

        Yields:
            JSON response data for each page
        """
        page = 1

        while page <= max_pages:
            params = {
                page_param: page,
                page_size_param: page_size
            }

            data = self.fetch_api_data(base_url, params=params)
            if not data:
                break

            yield data

            # Check if we've reached the end
            # This logic depends on the specific API response format
            if isinstance(data, list) and len(data) < page_size:
                break
            elif isinstance(data, dict):
                # Look for common pagination indicators
                if 'next' in data and not data['next']:
                    break
                if 'has_more' in data and not data['has_more']:
                    break

            page += 1
            time.sleep(0.1)  # Be respectful to the API

    def connect_to_database(self, connection_string: str) -> Optional[Any]:
        """
        Connect to a database using SQLAlchemy

        Args:
            connection_string: Database connection string

        Returns:
            SQLAlchemy engine or None if failed
        """
        if not HAS_SQLALCHEMY:
            logger.error("SQLAlchemy not available for database connections")
            return None

        try:
            engine = create_engine(connection_string)
            # Test connection
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))

            logger.info("‚úÖ Database connection established")
            return engine

        except Exception as e:
            logger.error(f"‚ùå Failed to connect to database: {e}")
            return None

    def fetch_database_data(self, engine: Any, query: str, chunk_size: int = 10000) -> Iterator[pd.DataFrame]:
        """
        Fetch data from database in chunks

        Args:
            engine: SQLAlchemy engine
            query: SQL query
            chunk_size: Number of rows per chunk

        Yields:
            DataFrame chunks
        """
        if not HAS_SQLALCHEMY:
            logger.error("SQLAlchemy not available")
            return

        try:
            for chunk in pd.read_sql(query, engine, chunksize=chunk_size):
                yield chunk

        except Exception as e:
            logger.error(f"‚ùå Failed to fetch database data: {e}")

class DataProcessor:
    """
    Processes and transforms data during ingestion
    """

    def __init__(self):
        """Initialize the data processor"""
        self.validation_errors = []
        logger.info("‚úÖ Data Processor initialized")

    def validate_dataframe(self, df: pd.DataFrame, schema: Dict[str, Dict]) -> Tuple[pd.DataFrame, List[str]]:
        """
        Validate DataFrame against a schema

        Args:
            df: DataFrame to validate
            schema: Schema definition with column rules

        Returns:
            Tuple of (cleaned_df, validation_errors)
        """
        errors = []
        cleaned_df = df.copy()

        for column, rules in schema.items():
            if column not in df.columns:
                if rules.get('required', False):
                    errors.append(f"Required column missing: {column}")
                continue

            # Check data type
            expected_type = rules.get('type')
            if expected_type:
                try:
                    if expected_type == 'datetime':
                        cleaned_df[column] = pd.to_datetime(cleaned_df[column], errors='coerce')
                    elif expected_type == 'numeric':
                        cleaned_df[column] = pd.to_numeric(cleaned_df[column], errors='coerce')
                    elif expected_type == 'string':
                        cleaned_df[column] = cleaned_df[column].astype(str)
                except Exception as e:
                    errors.append(f"Type conversion failed for {column}: {e}")

            # Check for null values
            if not rules.get('allow_null', True):
                null_count = cleaned_df[column].isnull().sum()
                if null_count > 0:
                    errors.append(f"Column {column} has {null_count} null values")

            # Check value ranges
            if 'min_value' in rules:
                invalid_count = (cleaned_df[column] < rules['min_value']).sum()
                if invalid_count > 0:
                    errors.append(f"Column {column} has {invalid_count} values below minimum")

            if 'max_value' in rules:
                invalid_count = (cleaned_df[column] > rules['max_value']).sum()
                if invalid_count > 0:
                    errors.append(f"Column {column} has {invalid_count} values above maximum")

        return cleaned_df, errors

    def clean_text_data(self, df: pd.DataFrame, text_columns: List[str]) -> pd.DataFrame:
        """
        Clean text data in specified columns

        Args:
            df: DataFrame to clean
            text_columns: List of column names containing text

        Returns:
            DataFrame with cleaned text
        """
        cleaned_df = df.copy()

        for column in text_columns:
            if column in cleaned_df.columns:
                # Remove extra whitespace
                cleaned_df[column] = cleaned_df[column].astype(str).str.strip()

                # Remove special characters (optional)
                cleaned_df[column] = cleaned_df[column].str.replace(r'[^\w\s]', '', regex=True)

                # Convert to title case (optional)
                # cleaned_df[column] = cleaned_df[column].str.title()

        logger.info(f"‚úÖ Cleaned text data in {len(text_columns)} columns")
        return cleaned_df

    def deduplicate_data(self, df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:
        """
        Remove duplicate rows from DataFrame

        Args:
            df: DataFrame to deduplicate
            subset: Columns to consider for duplication check

        Returns:
            DataFrame without duplicates
        """
        original_count = len(df)
        deduped_df = df.drop_duplicates(subset=subset)
        removed_count = original_count - len(deduped_df)

        if removed_count > 0:
            logger.info(f"‚úÖ Removed {removed_count} duplicate rows")

        return deduped_df

    def add_metadata_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Add metadata columns to DataFrame

        Args:
            df: DataFrame to enhance

        Returns:
            DataFrame with metadata columns
        """
        enhanced_df = df.copy()

        # Add ingestion timestamp
        enhanced_df['ingestion_timestamp'] = datetime.now()

        # Add row hash for change detection
        enhanced_df['row_hash'] = enhanced_df.apply(
            lambda row: hashlib.md5(str(row.to_dict()).encode()).hexdigest(),
            axis=1
        )

        # Add data source information
        enhanced_df['data_source'] = 'TT_ingestion_pipeline'

        logger.info("‚úÖ Added metadata columns to DataFrame")
        return enhanced_df

class DataIngestionPipeline:
    """
    Main pipeline orchestrator for data ingestion
    """

    def __init__(self, aws_manager: AWSDataManager):
        """
        Initialize the ingestion pipeline

        Args:
            aws_manager: AWS data manager instance
        """
        self.aws_manager = aws_manager
        self.connector = DataSourceConnector()
        self.processor = DataProcessor()
        self.ingestion_stats = {
            'start_time': None,
            'end_time': None,
            'total_records': 0,
            'successful_records': 0,
            'failed_records': 0,
            'errors': []
        }

        logger.info("‚úÖ Data Ingestion Pipeline initialized")

    def ingest_from_api(self, api_config: Dict[str, Any], s3_config: Dict[str, str],
                       schema: Dict[str, Dict] = None, chunk_size: int = 1000) -> Dict[str, Any]:
        """
        Ingest data from API and store in S3

        Args:
            api_config: API configuration (url, params, headers, etc.)
            s3_config: S3 configuration (bucket, key_prefix)
            schema: Data validation schema
            chunk_size: Number of records to process in each batch

        Returns:
            Ingestion results and statistics
        """
        self.ingestion_stats['start_time'] = datetime.now()
        logger.info(f"üöÄ Starting API data ingestion from {api_config.get('url', 'unknown')}")

        try:
            # Determine if API is paginated
            if api_config.get('paginated', False):
                data_generator = self.connector.fetch_paginated_api_data(
                    api_config['url'],
                    page_param=api_config.get('page_param', 'page'),
                    page_size_param=api_config.get('page_size_param', 'per_page'),
                    page_size=api_config.get('page_size', 100),
                    max_pages=api_config.get('max_pages', 10)
                )
            else:
                # Single API call
                single_response = self.connector.fetch_api_data(
                    api_config['url'],
                    params=api_config.get('params'),
                    headers=api_config.get('headers'),
                    auth=api_config.get('auth')
                )
                data_generator = [single_response] if single_response else []

            # Process data in chunks
            batch_number = 0
            all_records = []

            for api_response in data_generator:
                batch_number += 1
                logger.info(f"üìä Processing batch {batch_number}")

                # Extract records from API response
                records = self._extract_records_from_response(api_response, api_config.get('data_path', []))

                if not records:
                    continue

                # Convert to DataFrame
                df = pd.DataFrame(records)

                # Validate data if schema provided
                if schema:
                    df, validation_errors = self.processor.validate_dataframe(df, schema)
                    self.ingestion_stats['errors'].extend(validation_errors)

                # Clean and process data
                df = self.processor.add_metadata_columns(df)

                # Store in S3
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"{s3_config['key_prefix']}/batch_{batch_number}_{timestamp}.parquet"

                success = self.aws_manager.upload_dataframe_to_s3(
                    df, s3_config['bucket'], s3_key, format='parquet'
                )

                if success:
                    self.ingestion_stats['successful_records'] += len(df)
                    all_records.extend(records)
                else:
                    self.ingestion_stats['failed_records'] += len(df)

                # Respect rate limits
                time.sleep(api_config.get('delay_seconds', 1))

            self.ingestion_stats['total_records'] = len(all_records)

        except Exception as e:
            logger.error(f"‚ùå Error during API ingestion: {e}")
            self.ingestion_stats['errors'].append(str(e))

        self.ingestion_stats['end_time'] = datetime.now()
        return self._generate_ingestion_report()

    def ingest_from_database(self, db_config: Dict[str, Any], s3_config: Dict[str, str],
                           schema: Dict[str, Dict] = None, chunk_size: int = 10000) -> Dict[str, Any]:
        """
        Ingest data from database and store in S3

        Args:
            db_config: Database configuration (connection_string, query)
            s3_config: S3 configuration (bucket, key_prefix)
            schema: Data validation schema
            chunk_size: Number of records to process in each batch

        Returns:
            Ingestion results and statistics
        """
        self.ingestion_stats['start_time'] = datetime.now()
        logger.info(f"üöÄ Starting database data ingestion")

        try:
            # Connect to database
            engine = self.connector.connect_to_database(db_config['connection_string'])
            if not engine:
                raise Exception("Failed to connect to database")

            # Fetch data in chunks
            chunk_number = 0
            for df_chunk in self.connector.fetch_database_data(engine, db_config['query'], chunk_size):
                chunk_number += 1
                logger.info(f"üìä Processing chunk {chunk_number} with {len(df_chunk)} records")

                # Validate data if schema provided
                if schema:
                    df_chunk, validation_errors = self.processor.validate_dataframe(df_chunk, schema)
                    self.ingestion_stats['errors'].extend(validation_errors)

                # Clean and process data
                df_chunk = self.processor.add_metadata_columns(df_chunk)

                # Store in S3
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"{s3_config['key_prefix']}/chunk_{chunk_number}_{timestamp}.parquet"

                success = self.aws_manager.upload_dataframe_to_s3(
                    df_chunk, s3_config['bucket'], s3_key, format='parquet'
                )

                if success:
                    self.ingestion_stats['successful_records'] += len(df_chunk)
                else:
                    self.ingestion_stats['failed_records'] += len(df_chunk)

            self.ingestion_stats['total_records'] = self.ingestion_stats['successful_records'] + self.ingestion_stats['failed_records']

        except Exception as e:
            logger.error(f"‚ùå Error during database ingestion: {e}")
            self.ingestion_stats['errors'].append(str(e))

        self.ingestion_stats['end_time'] = datetime.now()
        return self._generate_ingestion_report()

    def ingest_from_files(self, file_config: Dict[str, Any], s3_config: Dict[str, str],
                         schema: Dict[str, Dict] = None) -> Dict[str, Any]:
        """
        Ingest data from local files and store in S3

        Args:
            file_config: File configuration (directory, pattern, format)
            s3_config: S3 configuration (bucket, key_prefix)
            schema: Data validation schema

        Returns:
            Ingestion results and statistics
        """
        self.ingestion_stats['start_time'] = datetime.now()
        logger.info(f"üöÄ Starting file data ingestion from {file_config.get('directory', 'unknown')}")

        try:
            file_dir = Path(file_config['directory'])
            file_pattern = file_config.get('pattern', '*')
            file_format = file_config.get('format', 'csv')

            # Find matching files
            files = list(file_dir.glob(file_pattern))
            logger.info(f"üìÅ Found {len(files)} files to process")

            for file_path in files:
                logger.info(f"üìä Processing file: {file_path.name}")

                try:
                    # Read file based on format
                    if file_format.lower() == 'csv':
                        df = pd.read_csv(file_path)
                    elif file_format.lower() == 'json':
                        df = pd.read_json(file_path)
                    elif file_format.lower() in ['parquet', 'pq']:
                        df = pd.read_parquet(file_path)
                    elif file_format.lower() in ['excel', 'xlsx', 'xls']:
                        df = pd.read_excel(file_path)
                    else:
                        logger.warning(f"‚ö†Ô∏è  Unsupported file format: {file_format}")
                        continue

                    # Validate data if schema provided
                    if schema:
                        df, validation_errors = self.processor.validate_dataframe(df, schema)
                        self.ingestion_stats['errors'].extend(validation_errors)

                    # Clean and process data
                    df = self.processor.add_metadata_columns(df)

                    # Store in S3
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    s3_key = f"{s3_config['key_prefix']}/{file_path.stem}_{timestamp}.parquet"

                    success = self.aws_manager.upload_dataframe_to_s3(
                        df, s3_config['bucket'], s3_key, format='parquet'
                    )

                    if success:
                        self.ingestion_stats['successful_records'] += len(df)
                    else:
                        self.ingestion_stats['failed_records'] += len(df)

                except Exception as e:
                    logger.error(f"‚ùå Error processing file {file_path.name}: {e}")
                    self.ingestion_stats['errors'].append(f"File {file_path.name}: {str(e)}")

            self.ingestion_stats['total_records'] = self.ingestion_stats['successful_records'] + self.ingestion_stats['failed_records']

        except Exception as e:
            logger.error(f"‚ùå Error during file ingestion: {e}")
            self.ingestion_stats['errors'].append(str(e))

        self.ingestion_stats['end_time'] = datetime.now()
        return self._generate_ingestion_report()

    def _extract_records_from_response(self, response: Dict, data_path: List[str]) -> List[Dict]:
        """
        Extract records from API response using data path

        Args:
            response: API response data
            data_path: Path to the data array in the response

        Returns:
            List of record dictionaries
        """
        data = response
        for key in data_path:
            if isinstance(data, dict) and key in data:
                data = data[key]
            else:
                return []

        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            return [data]
        else:
            return []

    def _generate_ingestion_report(self) -> Dict[str, Any]:
        """Generate comprehensive ingestion report"""
        duration = None
        if self.ingestion_stats['start_time'] and self.ingestion_stats['end_time']:
            duration = self.ingestion_stats['end_time'] - self.ingestion_stats['start_time']

        report = {
            'pipeline_summary': {
                'start_time': self.ingestion_stats['start_time'].isoformat() if self.ingestion_stats['start_time'] else None,
                'end_time': self.ingestion_stats['end_time'].isoformat() if self.ingestion_stats['end_time'] else None,
                'duration_seconds': duration.total_seconds() if duration else None,
                'total_records': self.ingestion_stats['total_records'],
                'successful_records': self.ingestion_stats['successful_records'],
                'failed_records': self.ingestion_stats['failed_records'],
                'success_rate': (self.ingestion_stats['successful_records'] / self.ingestion_stats['total_records']) * 100 if self.ingestion_stats['total_records'] > 0 else 0
            },
            'errors': self.ingestion_stats['errors'],
            'performance_metrics': {
                'records_per_second': self.ingestion_stats['total_records'] / duration.total_seconds() if duration and duration.total_seconds() > 0 else 0
            }
        }

        return report

def create_sample_data():
    """Create sample data for demonstration"""
    # Sample sales data
    sample_data = []
    for i in range(1000):
        record = {
            'transaction_id': f"TXN_{i:06d}",
            'customer_id': f"CUST_{(i % 100):03d}",
            'product_id': f"PROD_{(i % 50):03d}",
            'quantity': (i % 10) + 1,
            'unit_price': round(10 + (i % 100) * 0.5, 2),
            'transaction_date': (datetime.now() - timedelta(days=i % 365)).strftime('%Y-%m-%d'),
            'sales_rep': f"Rep_{(i % 10):02d}",
            'region': ['North', 'South', 'East', 'West'][i % 4]
        }
        sample_data.append(record)

    return sample_data

def main():
    """
    Main function demonstrating the data ingestion pipeline
    """
    print("üóÉÔ∏è  TT Data Ingestion Pipeline - Educational Demo")
    print("=" * 60)

    # Note: This demo uses mock AWS credentials for educational purposes
    # In real usage, you would use actual AWS credentials
    print("‚ö†Ô∏è  Demo Mode: Using mock AWS operations (no actual S3 uploads)")

    # Initialize AWS manager (with demo credentials)
    aws_manager = AWSDataManager(
        aws_access_key="demo_key",
        aws_secret_key="demo_secret",
        region="us-east-1"
    )

    # Initialize pipeline
    pipeline = DataIngestionPipeline(aws_manager)

    # Example 1: Sample data validation schema
    print("\nüìö Example 1: Data validation schema definition...")
    sample_schema = {
        'transaction_id': {
            'type': 'string',
            'required': True,
            'allow_null': False
        },
        'customer_id': {
            'type': 'string',
            'required': True,
            'allow_null': False
        },
        'quantity': {
            'type': 'numeric',
            'required': True,
            'min_value': 1,
            'max_value': 100
        },
        'unit_price': {
            'type': 'numeric',
            'required': True,
            'min_value': 0.01
        },
        'transaction_date': {
            'type': 'datetime',
            'required': True,
            'allow_null': False
        }
    }

    print("‚úÖ Schema defined with validation rules for 5 columns")

    # Example 2: Create and process sample data
    print("\nüìö Example 2: Creating and processing sample data...")
    sample_data = create_sample_data()
    df = pd.DataFrame(sample_data)

    print(f"üìä Created sample dataset with {len(df)} records")
    print(f"Columns: {list(df.columns)}")
    print(f"Sample record:\n{df.iloc[0].to_dict()}")

    # Validate the data
    processor = DataProcessor()
    validated_df, validation_errors = processor.validate_dataframe(df, sample_schema)

    if validation_errors:
        print(f"‚ö†Ô∏è  Validation errors found: {len(validation_errors)}")
        for error in validation_errors[:3]:  # Show first 3 errors
            print(f"  - {error}")
    else:
        print("‚úÖ Data validation passed")

    # Example 3: Demonstrate file-based ingestion
    print("\nüìö Example 3: File-based data ingestion simulation...")

    # Create sample files
    sample_dir = Path(__file__).parent / "sample_ingestion_data"
    sample_dir.mkdir(exist_ok=True)

    # Save sample data to different formats
    sample_files = {
        'sales_data.csv': df.to_csv,
        'sales_data.json': df.to_json,
        'sales_data.parquet': df.to_parquet
    }

    for filename, save_func in sample_files.items():
        file_path = sample_dir / filename
        if filename.endswith('.csv'):
            save_func(file_path, index=False)
        elif filename.endswith('.json'):
            save_func(file_path, orient='records', lines=True)
        else:
            save_func(file_path, index=False)

    print(f"üìÅ Created sample files in {sample_dir}")

    # Simulate file ingestion
    file_config = {
        'directory': str(sample_dir),
        'pattern': '*.csv',
        'format': 'csv'
    }

    s3_config = {
        'bucket': 'tt-data-ingestion-demo',
        'key_prefix': 'sales_data/raw'
    }

    # This would normally upload to S3, but we'll simulate it
    print("üöÄ Simulating file ingestion pipeline...")
    print(f"  Source: {file_config['directory']}")
    print(f"  Target: s3://{s3_config['bucket']}/{s3_config['key_prefix']}")
    print(f"  Schema validation: {'Enabled' if sample_schema else 'Disabled'}")

    # Example 4: API ingestion simulation
    print("\nüìö Example 4: API data ingestion simulation...")

    api_config = {
        'url': 'https://jsonplaceholder.typicode.com/posts',
        'paginated': False,
        'headers': {'Accept': 'application/json'},
        'data_path': [],  # Data is at root level
        'delay_seconds': 0.5
    }

    print("üåê Simulating API data ingestion...")
    print(f"  API Endpoint: {api_config['url']}")
    print(f"  Pagination: {'Yes' if api_config['paginated'] else 'No'}")

    # Fetch sample API data for demonstration
    connector = DataSourceConnector()
    api_data = connector.fetch_api_data(api_config['url'])

    if api_data:
        api_df = pd.DataFrame(api_data[:10])  # Use first 10 records
        print(f"‚úÖ Fetched {len(api_df)} records from API")
        print(f"Sample columns: {list(api_df.columns)}")

        # Add metadata
        enriched_df = processor.add_metadata_columns(api_df)
        print(f"üìä Enhanced data with metadata columns")

    # Example 5: Performance monitoring
    print("\nüìö Example 5: Performance monitoring and reporting...")

    # Simulate processing statistics
    mock_stats = {
        'total_files_processed': 3,
        'total_records_ingested': len(df),
        'processing_time_seconds': 45.2,
        'average_throughput_records_per_second': len(df) / 45.2,
        'validation_errors': len(validation_errors),
        'data_quality_score': 95.5
    }

    print("üìà Ingestion Performance Report:")
    print(f"  Total Records: {mock_stats['total_records_ingested']:,}")
    print(f"  Processing Time: {mock_stats['processing_time_seconds']:.1f} seconds")
    print(f"  Throughput: {mock_stats['average_throughput_records_per_second']:.1f} records/sec")
    print(f"  Data Quality Score: {mock_stats['data_quality_score']:.1f}%")
    print(f"  Validation Errors: {mock_stats['validation_errors']}")

    # Example 6: Error handling and retry logic
    print("\nüìö Example 6: Error handling and retry simulation...")

    def simulate_unreliable_operation():
        """Simulate an operation that might fail"""
        import random
        if random.random() < 0.3:  # 30% chance of failure
            raise Exception("Simulated network timeout")
        return "Operation successful"

    def retry_operation(operation, max_retries=3, delay=1):
        """Retry an operation with exponential backoff"""
        for attempt in range(max_retries):
            try:
                result = operation()
                print(f"  ‚úÖ Operation succeeded on attempt {attempt + 1}")
                return result
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = delay * (2 ** attempt)
                    print(f"  ‚ö†Ô∏è  Attempt {attempt + 1} failed: {e}")
                    print(f"     Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print(f"  ‚ùå All {max_retries} attempts failed: {e}")
                    raise

    print("üîÑ Demonstrating retry logic...")
    try:
        retry_operation(simulate_unreliable_operation)
    except Exception as e:
        print(f"üí• Final failure: {e}")

    print(f"\nüéì Educational Notes:")
    print("1. Always validate data before storing in production systems")
    print("2. Use chunking for large datasets to manage memory usage")
    print("3. Implement retry logic for network operations")
    print("4. Monitor data quality and ingestion performance")
    print("5. Use appropriate data formats (Parquet for analytics, JSON for flexibility)")
    print("6. Handle different time zones and date formats carefully")
    print("7. Implement data lineage tracking for debugging")
    print("8. Use AWS IAM roles instead of hardcoded credentials")
    print("9. Consider data encryption for sensitive information")
    print("10. Plan for data archival and retention policies")

    # Cleanup
    try:
        import shutil
        shutil.rmtree(sample_dir)
        print(f"üßπ Cleaned up sample data directory")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not clean up directory: {e}")

    print("\nüöÄ Real Implementation Notes:")
    print("To use this script with real AWS:")
    print("1. Install AWS CLI and configure credentials")
    print("2. Set up appropriate IAM permissions")
    print("3. Replace demo credentials with real ones")
    print("4. Test with small datasets first")
    print("5. Monitor AWS costs and usage")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/TW_doc_converter.py

```python
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "python-docx",
#     "markdown",
#     "beautifulsoup4",
#     "pillow",
# ]
# ///

"""
TW Document Converter
Student: TW
Description: Converting my writing sentences into doc documents

This script demonstrates how to:
1. Convert plain text to formatted Word documents
2. Apply various formatting styles (headings, bold, italic, etc.)
3. Add images, tables, and other elements
4. Parse markdown text and convert to Word
5. Batch convert multiple text files

Educational Focus:
- Working with python-docx library
- Document formatting and styling
- Text processing and parsing
- File I/O operations
- Object-oriented programming
"""

import os
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Union
import markdown
from bs4 import BeautifulSoup

try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.enum.style import WD_STYLE_TYPE
    from docx.oxml.shared import OxmlElement, qn
except ImportError:
    print("‚ùå python-docx not installed. Run: pip install python-docx")
    exit(1)

class DocumentConverter:
    """
    A comprehensive text-to-Word document converter

    This class provides methods to:
    - Convert plain text to formatted Word documents
    - Parse and convert markdown to Word
    - Apply various formatting styles
    - Add headers, footers, and metadata
    - Batch process multiple files
    """

    def __init__(self):
        """Initialize the converter with default settings"""
        # Default formatting settings
        self.default_font = "Calibri"
        self.default_font_size = 11
        self.heading_font = "Calibri"

        # Style mappings for different text elements
        self.style_mappings = {
            'h1': 'Heading 1',
            'h2': 'Heading 2',
            'h3': 'Heading 3',
            'h4': 'Heading 4',
            'h5': 'Heading 5',
            'h6': 'Heading 6',
            'p': 'Normal',
            'blockquote': 'Quote',
            'code': 'Intense Quote'  # Using built-in style for code blocks
        }

        print("‚úÖ Document Converter initialized")

    def create_document_from_text(self,
                                text: str,
                                title: str = "Document",
                                author: str = "TW",
                                apply_auto_formatting: bool = True) -> Document:
        """
        Create a Word document from plain text with automatic formatting

        Args:
            text: The text content to convert
            title: Document title
            author: Document author
            apply_auto_formatting: Whether to apply automatic formatting detection

        Returns:
            Document object ready to be saved
        """
        print(f"üìù Creating document: {title}")

        # Create new document
        doc = Document()

        # Set document properties
        doc.core_properties.title = title
        doc.core_properties.author = author
        doc.core_properties.created = datetime.now()

        # Add title
        title_paragraph = doc.add_heading(title, level=0)
        title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Add author and date
        meta_paragraph = doc.add_paragraph()
        meta_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
        meta_run = meta_paragraph.add_run(f"By {author} | {datetime.now().strftime('%B %d, %Y')}")
        meta_run.italic = True

        # Add separator
        doc.add_paragraph()

        if apply_auto_formatting:
            self._add_formatted_text(doc, text)
        else:
            self._add_plain_text(doc, text)

        print(f"‚úÖ Document created with {len(doc.paragraphs)} paragraphs")
        return doc

    def _add_formatted_text(self, doc: Document, text: str):
        """
        Add text with automatic formatting detection

        This method looks for common patterns in text and applies formatting:
        - Lines starting with # become headings
        - Lines in ALL CAPS become emphasized
        - Lines with bullet points become lists
        - Quoted text becomes block quotes
        """
        lines = text.split('\n')
        current_list = None

        for line in lines:
            line = line.strip()

            if not line:
                # Empty line - add paragraph break
                doc.add_paragraph()
                current_list = None
                continue

            # Check for heading patterns (# Header, ## Header, etc.)
            heading_match = re.match(r'^(#{1,6})\s+(.+)', line)
            if heading_match:
                level = len(heading_match.group(1))
                heading_text = heading_match.group(2)
                doc.add_heading(heading_text, level=level)
                current_list = None
                continue

            # Check for bullet points
            bullet_match = re.match(r'^[\-\*\+]\s+(.+)', line)
            if bullet_match:
                bullet_text = bullet_match.group(1)
                if current_list is None:
                    current_list = doc.add_paragraph(bullet_text, style='List Bullet')
                else:
                    doc.add_paragraph(bullet_text, style='List Bullet')
                continue

            # Check for numbered lists
            number_match = re.match(r'^\d+\.\s+(.+)', line)
            if number_match:
                number_text = number_match.group(1)
                doc.add_paragraph(number_text, style='List Number')
                current_list = None
                continue

            # Check for quotes (lines starting with >)
            quote_match = re.match(r'^>\s+(.+)', line)
            if quote_match:
                quote_text = quote_match.group(1)
                quote_paragraph = doc.add_paragraph(quote_text, style='Quote')
                current_list = None
                continue

            # Check for emphasis patterns
            if line.isupper() and len(line.split()) > 1:
                # ALL CAPS text becomes emphasized
                emphasis_paragraph = doc.add_paragraph()
                emphasis_run = emphasis_paragraph.add_run(line)
                emphasis_run.bold = True
                current_list = None
                continue

            # Regular paragraph
            paragraph = doc.add_paragraph(line)

            # Apply inline formatting (bold, italic)
            self._apply_inline_formatting(paragraph)
            current_list = None

    def _apply_inline_formatting(self, paragraph):
        """
        Apply inline formatting like **bold** and *italic* to paragraph text

        This method processes markdown-style formatting within paragraphs
        """
        # Get the text from the paragraph
        text = paragraph.text

        # Clear the paragraph
        paragraph.clear()

        # Process bold text (**text**)
        bold_pattern = r'\*\*(.*?)\*\*'
        parts = re.split(bold_pattern, text)

        for i, part in enumerate(parts):
            if i % 2 == 0:
                # Regular text, check for italic
                italic_pattern = r'\*(.*?)\*'
                italic_parts = re.split(italic_pattern, part)

                for j, italic_part in enumerate(italic_parts):
                    if j % 2 == 0:
                        # Regular text
                        if italic_part:
                            paragraph.add_run(italic_part)
                    else:
                        # Italic text
                        italic_run = paragraph.add_run(italic_part)
                        italic_run.italic = True
            else:
                # Bold text
                bold_run = paragraph.add_run(part)
                bold_run.bold = True

    def _add_plain_text(self, doc: Document, text: str):
        """Add text as plain paragraphs without formatting detection"""
        paragraphs = text.split('\n\n')  # Split on double newlines

        for para_text in paragraphs:
            if para_text.strip():
                doc.add_paragraph(para_text.strip())

    def convert_markdown_to_docx(self, markdown_text: str, title: str = "Markdown Document") -> Document:
        """
        Convert markdown text to a Word document

        Args:
            markdown_text: Markdown formatted text
            title: Document title

        Returns:
            Document object with converted content
        """
        print(f"üîÑ Converting markdown to Word document: {title}")

        # Convert markdown to HTML
        html = markdown.markdown(markdown_text, extensions=['tables', 'fenced_code'])

        # Parse HTML with BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')

        # Create new document
        doc = Document()

        # Set document properties
        doc.core_properties.title = title
        doc.core_properties.author = "TW"
        doc.core_properties.created = datetime.now()

        # Add title
        doc.add_heading(title, level=0)

        # Process HTML elements
        for element in soup.find_all():
            self._process_html_element(doc, element)

        print(f"‚úÖ Markdown converted to Word document")
        return doc

    def _process_html_element(self, doc: Document, element):
        """Process individual HTML elements and convert to Word format"""
        tag_name = element.name

        if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            # Headings
            level = int(tag_name[1])
            doc.add_heading(element.get_text().strip(), level=level)

        elif tag_name == 'p':
            # Paragraphs
            if element.get_text().strip():
                paragraph = doc.add_paragraph()
                self._add_formatted_text_to_paragraph(paragraph, element)

        elif tag_name == 'blockquote':
            # Block quotes
            quote_text = element.get_text().strip()
            if quote_text:
                doc.add_paragraph(quote_text, style='Quote')

        elif tag_name in ['ul', 'ol']:
            # Lists
            list_items = element.find_all('li')
            style = 'List Bullet' if tag_name == 'ul' else 'List Number'
            for item in list_items:
                doc.add_paragraph(item.get_text().strip(), style=style)

        elif tag_name == 'code':
            # Inline code - add as quote style
            code_text = element.get_text()
            if code_text.strip():
                doc.add_paragraph(code_text, style='Intense Quote')

    def _add_formatted_text_to_paragraph(self, paragraph, html_element):
        """Add formatted text from HTML element to a Word paragraph"""
        for content in html_element.contents:
            if hasattr(content, 'name'):
                # HTML element
                if content.name == 'strong' or content.name == 'b':
                    run = paragraph.add_run(content.get_text())
                    run.bold = True
                elif content.name == 'em' or content.name == 'i':
                    run = paragraph.add_run(content.get_text())
                    run.italic = True
                elif content.name == 'code':
                    run = paragraph.add_run(content.get_text())
                    run.font.name = 'Courier New'
                else:
                    paragraph.add_run(content.get_text())
            else:
                # Plain text
                paragraph.add_run(str(content))

    def add_table_from_data(self, doc: Document, data: List[List[str]], headers: List[str] = None):
        """
        Add a table to the document from data

        Args:
            doc: Document to add table to
            data: List of lists containing table data
            headers: Optional list of header names
        """
        if not data:
            return

        rows = len(data) + (1 if headers else 0)
        cols = len(data[0]) if data else 0

        # Create table
        table = doc.add_table(rows=rows, cols=cols)
        table.style = 'Table Grid'

        row_idx = 0

        # Add headers if provided
        if headers:
            header_row = table.rows[row_idx]
            for col_idx, header in enumerate(headers):
                header_row.cells[col_idx].text = header
                # Make header bold
                for paragraph in header_row.cells[col_idx].paragraphs:
                    for run in paragraph.runs:
                        run.bold = True
            row_idx += 1

        # Add data rows
        for data_row in data:
            table_row = table.rows[row_idx]
            for col_idx, cell_data in enumerate(data_row):
                table_row.cells[col_idx].text = str(cell_data)
            row_idx += 1

        print(f"‚úÖ Added table with {rows} rows and {cols} columns")

    def batch_convert_files(self,
                          input_directory: Union[str, Path],
                          output_directory: Union[str, Path] = None,
                          file_pattern: str = "*.txt") -> List[Path]:
        """
        Batch convert multiple text files to Word documents

        Args:
            input_directory: Directory containing text files
            output_directory: Directory to save Word documents (defaults to input_directory)
            file_pattern: File pattern to match (e.g., "*.txt", "*.md")

        Returns:
            List of created document file paths
        """
        input_path = Path(input_directory)
        output_path = Path(output_directory) if output_directory else input_path

        if not input_path.exists():
            print(f"‚ùå Input directory does not exist: {input_path}")
            return []

        # Create output directory if it doesn't exist
        output_path.mkdir(parents=True, exist_ok=True)

        # Find matching files
        files = list(input_path.glob(file_pattern))
        created_docs = []

        print(f"üîÑ Batch converting {len(files)} files from {input_path}")

        for file_path in files:
            try:
                # Read file content
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Determine conversion method based on file extension
                if file_path.suffix.lower() == '.md':
                    doc = self.convert_markdown_to_docx(content, file_path.stem)
                else:
                    doc = self.create_document_from_text(content, file_path.stem)

                # Save document
                output_file = output_path / f"{file_path.stem}.docx"
                doc.save(str(output_file))
                created_docs.append(output_file)

                print(f"‚úÖ Converted: {file_path.name} ‚Üí {output_file.name}")

            except Exception as e:
                print(f"‚ùå Error converting {file_path.name}: {e}")

        print(f"üéâ Batch conversion complete: {len(created_docs)} documents created")
        return created_docs

    def save_document(self, doc: Document, filename: str, output_dir: Union[str, Path] = None) -> Path:
        """
        Save a document to the specified location

        Args:
            doc: Document to save
            filename: Name for the output file (with or without .docx extension)
            output_dir: Directory to save in (defaults to current script directory)

        Returns:
            Path to the saved file
        """
        if output_dir is None:
            output_dir = Path(__file__).parent

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Ensure .docx extension
        if not filename.endswith('.docx'):
            filename += '.docx'

        file_path = output_path / filename
        doc.save(str(file_path))

        print(f"üíæ Document saved: {file_path}")
        return file_path

def create_sample_content() -> Dict[str, str]:
    """Create sample content for demonstration"""
    return {
        'plain_text': """
Welcome to the Document Converter Demo

This is a sample document that demonstrates the capabilities of our text-to-Word converter.

MAIN FEATURES
The converter can handle various types of formatting automatically.

Key Features:
- Automatic heading detection
- Bold and italic text formatting
- Bullet point lists
- Numbered lists
- Block quotes

*This text will be italic* and **this text will be bold**.

# This is a Heading 1
## This is a Heading 2
### This is a Heading 3

> This is a block quote that will be formatted specially.

Regular paragraph text continues here with normal formatting.
        """,

        'markdown_text': """
# Markdown Document Example

This document demonstrates **markdown to Word** conversion capabilities.

## Features

### Text Formatting
- **Bold text**
- *Italic text*
- `Inline code`

### Lists
1. First numbered item
2. Second numbered item
3. Third numbered item

#### Bullet Points
- First bullet point
- Second bullet point
- Third bullet point

### Code Blocks
```python
def hello_world():
    print("Hello, World!")
```

### Quotes
> This is a blockquote in markdown
> that spans multiple lines

## Tables
| Name | Age | City |
|------|-----|------|
| Alice | 30 | New York |
| Bob | 25 | San Francisco |
| Charlie | 35 | Chicago |

## Conclusion
This demonstrates the power of automated document conversion!
        """
    }

def main():
    """
    Main function demonstrating the document converter functionality
    """
    print("üìù TW Document Converter - Educational Demo")
    print("=" * 50)

    # Initialize the converter
    converter = DocumentConverter()

    # Get sample content
    samples = create_sample_content()

    # Example 1: Convert plain text to Word
    print("\nüìö Example 1: Converting plain text to Word document...")
    plain_doc = converter.create_document_from_text(
        samples['plain_text'],
        title="Plain Text Conversion Demo",
        author="TW Student"
    )

    converter.save_document(plain_doc, "plain_text_demo")

    # Example 2: Convert markdown to Word
    print("\nüìö Example 2: Converting markdown to Word document...")
    markdown_doc = converter.convert_markdown_to_docx(
        samples['markdown_text'],
        title="Markdown Conversion Demo"
    )

    converter.save_document(markdown_doc, "markdown_demo")

    # Example 3: Create document with table
    print("\nüìö Example 3: Creating document with table...")
    table_doc = converter.create_document_from_text(
        "# Student Grades Report\n\nBelow are the current student grades:",
        title="Grades Report",
        author="TW Teacher"
    )

    # Add sample table data
    table_data = [
        ["Alice Johnson", "95", "A"],
        ["Bob Smith", "87", "B+"],
        ["Charlie Brown", "92", "A-"],
        ["Diana Prince", "98", "A+"]
    ]

    converter.add_table_from_data(
        table_doc,
        table_data,
        headers=["Student Name", "Score", "Grade"]
    )

    converter.save_document(table_doc, "grades_report")

    # Example 4: Demonstrate batch conversion (if sample files exist)
    print("\nüìö Example 4: Batch conversion demo...")

    # Create sample files for batch conversion
    sample_dir = Path(__file__).parent / "sample_texts"
    sample_dir.mkdir(exist_ok=True)

    sample_files = {
        "essay1.txt": "# My First Essay\n\nThis is the content of my first essay...",
        "notes.md": "## Class Notes\n\n- Important point 1\n- Important point 2",
        "story.txt": "# Short Story\n\nOnce upon a time, in a land far away..."
    }

    # Create sample files
    for filename, content in sample_files.items():
        sample_file = sample_dir / filename
        with open(sample_file, 'w', encoding='utf-8') as f:
            f.write(content)

    # Batch convert
    created_docs = converter.batch_convert_files(sample_dir)

    print(f"\nüéì Educational Notes:")
    print("1. python-docx provides extensive formatting capabilities")
    print("2. Always handle file encoding properly (use UTF-8)")
    print("3. Consider memory usage when processing large documents")
    print("4. Validate input data before processing")
    print("5. Use appropriate exception handling for file operations")
    print(f"6. Created {len(created_docs)} documents in batch conversion")

    # Clean up sample files
    import shutil
    if sample_dir.exists():
        shutil.rmtree(sample_dir)
        print("üßπ Cleaned up sample files")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/VK_clinical_trials_parser.py

```python
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "requests",
#     "pandas",
#     "beautifulsoup4",
#     "xmltodict",
# ]
# ///

"""
VK Clinical Trials Parser
Student: VK
Description: Parse through clinical trials.gov and other trial registries

This script demonstrates how to:
1. Query the ClinicalTrials.gov API
2. Parse XML/JSON responses
3. Extract key trial information
4. Export data to various formats (CSV, JSON)
5. Filter and search trials by criteria

Educational Focus:
- Working with REST APIs
- XML/JSON data parsing
- Data cleaning and transformation
- Pandas data manipulation
"""

import requests
import pandas as pd
import json
import xml.etree.ElementTree as ET
from datetime import datetime
import time
from typing import Dict, List, Optional
import sys
from pathlib import Path

class ClinicalTrialsParser:
    """
    A comprehensive parser for clinical trials data from ClinicalTrials.gov

    This class provides methods to:
    - Search for trials based on various criteria
    - Parse trial details from API responses
    - Export data in multiple formats
    """

    def __init__(self):
        """Initialize the parser with API endpoints and rate limiting"""
        # ClinicalTrials.gov API endpoints
        self.base_url = "https://clinicaltrials.gov/api"
        self.search_url = f"{self.base_url}/query/study_fields"
        self.full_study_url = f"{self.base_url}/query/full_studies"

        # Rate limiting - be respectful to the API
        self.request_delay = 1  # seconds between requests

        # Common fields we want to extract from trials
        self.standard_fields = [
            "NCTId",
            "BriefTitle",
            "OfficialTitle",
            "OverallStatus",
            "Phase",
            "StudyType",
            "Condition",
            "InterventionName",
            "PrimaryCompletionDate",
            "EnrollmentCount",
            "LocationCountry",
            "Sponsor",
            "CollaboratorName"
        ]

    def search_trials(self,
                     condition: str = None,
                     intervention: str = None,
                     status: str = None,
                     country: str = None,
                     max_results: int = 100) -> Dict:
        """
        Search for clinical trials based on specified criteria

        Args:
            condition: Disease or condition (e.g., "diabetes", "cancer")
            intervention: Treatment or intervention (e.g., "metformin", "surgery")
            status: Trial status (e.g., "Recruiting", "Completed")
            country: Country where trial is conducted
            max_results: Maximum number of results to return

        Returns:
            Dictionary containing search results and metadata
        """
        print(f"üîç Searching clinical trials...")

        # Build search parameters
        params = {
            "expr": self._build_search_expression(condition, intervention, status, country),
            "fields": ",".join(self.standard_fields),
            "min_rnk": 1,
            "max_rnk": max_results,
            "fmt": "json"
        }

        try:
            # Make API request with rate limiting
            time.sleep(self.request_delay)
            response = requests.get(self.search_url, params=params)
            response.raise_for_status()

            data = response.json()

            print(f"‚úÖ Found {data.get('StudyFieldsResponse', {}).get('NStudiesFound', 0)} total studies")
            print(f"üìä Retrieved {len(data.get('StudyFieldsResponse', {}).get('StudyFields', []))} studies in this batch")

            return data

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error searching trials: {e}")
            return {}

    def _build_search_expression(self, condition, intervention, status, country) -> str:
        """
        Build a search expression for the ClinicalTrials.gov API

        The API uses a specific query syntax for combining search terms
        """
        expressions = []

        if condition:
            expressions.append(f"AREA[Condition]{condition}")
        if intervention:
            expressions.append(f"AREA[InterventionName]{intervention}")
        if status:
            expressions.append(f"AREA[OverallStatus]{status}")
        if country:
            expressions.append(f"AREA[LocationCountry]{country}")

        # If no specific criteria, search for recent studies
        if not expressions:
            expressions.append("AREA[StudyFirstPostDate]RANGE[2023-01-01,MAX]")

        return " AND ".join(expressions)

    def get_detailed_study(self, nct_id: str) -> Dict:
        """
        Get detailed information for a specific study by NCT ID

        Args:
            nct_id: The NCT identifier (e.g., "NCT04123456")

        Returns:
            Dictionary containing detailed study information
        """
        print(f"üìã Getting detailed information for study {nct_id}...")

        params = {
            "expr": f"AREA[NCTId]{nct_id}",
            "fmt": "json"
        }

        try:
            time.sleep(self.request_delay)
            response = requests.get(self.full_study_url, params=params)
            response.raise_for_status()

            data = response.json()
            return data

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error getting study details: {e}")
            return {}

    def parse_trials_to_dataframe(self, search_results: Dict) -> pd.DataFrame:
        """
        Convert API search results to a clean pandas DataFrame

        Args:
            search_results: Raw API response from search_trials()

        Returns:
            Cleaned pandas DataFrame with trial information
        """
        print("üîÑ Converting results to DataFrame...")

        study_fields = search_results.get('StudyFieldsResponse', {}).get('StudyFields', [])

        if not study_fields:
            print("‚ö†Ô∏è  No study data found to convert")
            return pd.DataFrame()

        # Convert to list of dictionaries for DataFrame creation
        trials_data = []

        for study in study_fields:
            trial_dict = {}

            # Extract each field, handling missing data gracefully
            for field in self.standard_fields:
                field_data = study.get(field, [])

                # Most fields return lists, so we need to extract the values
                if isinstance(field_data, list):
                    if len(field_data) > 0:
                        # For single values, take the first item
                        if field in ["NCTId", "BriefTitle", "OfficialTitle", "OverallStatus", "StudyType"]:
                            trial_dict[field] = field_data[0] if field_data else ""
                        # For lists that might have multiple values, join them
                        else:
                            trial_dict[field] = "; ".join(field_data) if field_data else ""
                    else:
                        trial_dict[field] = ""
                else:
                    trial_dict[field] = str(field_data) if field_data else ""

            trials_data.append(trial_dict)

        # Create DataFrame
        df = pd.DataFrame(trials_data)

        # Clean up data types and formats
        df = self._clean_dataframe(df)

        print(f"‚úÖ Created DataFrame with {len(df)} trials and {len(df.columns)} columns")
        return df

    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and format the DataFrame for better usability"""

        # Convert enrollment count to numeric
        if 'EnrollmentCount' in df.columns:
            df['EnrollmentCount'] = pd.to_numeric(df['EnrollmentCount'], errors='coerce')

        # Parse dates if present
        date_columns = ['PrimaryCompletionDate']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')

        # Clean up phase information
        if 'Phase' in df.columns:
            df['Phase'] = df['Phase'].str.replace('Phase ', '', regex=False)

        return df

    def export_data(self, df: pd.DataFrame, filename: str = None, format: str = "csv"):
        """
        Export trial data to various formats

        Args:
            df: DataFrame containing trial data
            filename: Output filename (without extension)
            format: Export format ("csv", "json", "excel")
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"clinical_trials_{timestamp}"

        # Ensure we're in the scripts directory for output
        output_dir = Path(__file__).parent

        if format.lower() == "csv":
            filepath = output_dir / f"{filename}.csv"
            df.to_csv(filepath, index=False)
            print(f"üíæ Exported to CSV: {filepath}")

        elif format.lower() == "json":
            filepath = output_dir / f"{filename}.json"
            df.to_json(filepath, orient='records', indent=2)
            print(f"üíæ Exported to JSON: {filepath}")

        elif format.lower() == "excel":
            filepath = output_dir / f"{filename}.xlsx"
            df.to_excel(filepath, index=False)
            print(f"üíæ Exported to Excel: {filepath}")

        return filepath

    def generate_summary_report(self, df: pd.DataFrame):
        """Generate a summary report of the clinical trials data"""
        print("\n" + "="*50)
        print("üìä CLINICAL TRIALS SUMMARY REPORT")
        print("="*50)

        if df.empty:
            print("No data available for summary")
            return

        print(f"Total Trials: {len(df)}")

        # Status distribution
        if 'OverallStatus' in df.columns:
            print("\nüè• Trial Status Distribution:")
            status_counts = df['OverallStatus'].value_counts()
            for status, count in status_counts.head(5).items():
                print(f"  {status}: {count}")

        # Phase distribution
        if 'Phase' in df.columns:
            print("\nüß™ Phase Distribution:")
            phase_counts = df['Phase'].value_counts()
            for phase, count in phase_counts.head(5).items():
                print(f"  Phase {phase}: {count}")

        # Top conditions
        if 'Condition' in df.columns:
            print("\nüè• Top Conditions Studied:")
            # Split multiple conditions and count
            all_conditions = []
            for conditions in df['Condition'].dropna():
                all_conditions.extend([c.strip() for c in conditions.split(';')])
            condition_series = pd.Series(all_conditions)
            top_conditions = condition_series.value_counts().head(5)
            for condition, count in top_conditions.items():
                print(f"  {condition}: {count}")

        # Country distribution
        if 'LocationCountry' in df.columns:
            print("\nüåç Top Countries:")
            country_data = df['LocationCountry'].dropna()
            all_countries = []
            for countries in country_data:
                all_countries.extend([c.strip() for c in countries.split(';')])
            country_series = pd.Series(all_countries)
            top_countries = country_series.value_counts().head(5)
            for country, count in top_countries.items():
                print(f"  {country}: {count}")

def main():
    """
    Main function demonstrating the clinical trials parser functionality
    """
    print("üè• VK Clinical Trials Parser - Educational Demo")
    print("=" * 50)

    # Initialize the parser
    parser = ClinicalTrialsParser()

    # Example 1: Search for diabetes trials
    print("\nüìö Example 1: Searching for diabetes trials...")
    diabetes_results = parser.search_trials(
        condition="diabetes",
        status="Recruiting",
        max_results=20
    )

    if diabetes_results:
        df_diabetes = parser.parse_trials_to_dataframe(diabetes_results)
        if not df_diabetes.empty:
            parser.generate_summary_report(df_diabetes)
            # Export to CSV
            parser.export_data(df_diabetes, "diabetes_trials", "csv")

    # Example 2: Search for cancer immunotherapy trials
    print("\nüìö Example 2: Searching for cancer immunotherapy trials...")
    cancer_results = parser.search_trials(
        condition="cancer",
        intervention="immunotherapy",
        max_results=15
    )

    if cancer_results:
        df_cancer = parser.parse_trials_to_dataframe(cancer_results)
        if not df_cancer.empty:
            parser.generate_summary_report(df_cancer)

    # Example 3: Get detailed information for a specific trial
    print("\nüìö Example 3: Getting detailed study information...")
    # Use the first NCT ID from our diabetes search if available
    if 'df_diabetes' in locals() and not df_diabetes.empty and 'NCTId' in df_diabetes.columns:
        first_nct_id = df_diabetes.iloc[0]['NCTId']
        detailed_study = parser.get_detailed_study(first_nct_id)

        if detailed_study:
            print(f"‚úÖ Retrieved detailed information for {first_nct_id}")
            # You could further parse this detailed data as needed

    print("\nüéì Educational Notes:")
    print("1. Always respect API rate limits when making requests")
    print("2. Handle errors gracefully - APIs can be unreliable")
    print("3. Clean and validate data before analysis")
    print("4. Consider caching results for frequently accessed data")
    print("5. Be aware of data usage policies and terms of service")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/VK_log_analyzer.py

```python
#!/usr/bin/env python3
# /// script
# dependencies = ["anthropic", "python-dotenv", "pandas", "matplotlib"]
# ///

"""
VK_log_analyzer.py
Student: VK - "Auto log analysis of ECU results"

This script demonstrates automated analysis of ECU (Electronic Control Unit) logs
to identify patterns, anomalies, and potential issues using AI and statistical analysis.

Key Learning Objectives:
- Log file parsing and processing
- Pattern recognition in time-series data
- Anomaly detection techniques
- Automated alerting and reporting
- Statistical analysis of automotive data
"""

import os
import re
import json
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict
from dotenv import load_dotenv
import anthropic

# Load environment variables
load_dotenv()

class ECULogAnalyzer:
    """
    A class to analyze ECU (Electronic Control Unit) logs for automotive systems.
    This simulates analysis of vehicle diagnostic data, sensor readings, and error codes.
    """

    def __init__(self):
        """Initialize the ECU log analyzer with AI client."""
        # Initialize Anthropic client for AI-powered analysis
        self.ai_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # Define ECU systems and their normal operating ranges
        self.ecu_systems = {
            "engine": {
                "parameters": ["rpm", "coolant_temp", "oil_pressure", "throttle_position"],
                "normal_ranges": {
                    "rpm": (500, 6000),
                    "coolant_temp": (80, 105),  # Celsius
                    "oil_pressure": (20, 80),   # PSI
                    "throttle_position": (0, 100)  # Percentage
                }
            },
            "transmission": {
                "parameters": ["gear", "fluid_temp", "torque"],
                "normal_ranges": {
                    "gear": (1, 8),
                    "fluid_temp": (70, 120),  # Celsius
                    "torque": (100, 500)  # Nm
                }
            },
            "braking": {
                "parameters": ["brake_pressure", "abs_active", "brake_temp"],
                "normal_ranges": {
                    "brake_pressure": (0, 2000),  # PSI
                    "abs_active": (0, 1),  # Boolean
                    "brake_temp": (20, 200)  # Celsius
                }
            },
            "emissions": {
                "parameters": ["o2_sensor", "cat_temp", "egr_position"],
                "normal_ranges": {
                    "o2_sensor": (0.1, 0.9),  # Voltage
                    "cat_temp": (300, 800),   # Celsius
                    "egr_position": (0, 100)  # Percentage
                }
            }
        }

        # Common ECU error codes and their meanings
        self.error_codes = {
            "P0301": "Cylinder 1 Misfire Detected",
            "P0420": "Catalyst System Efficiency Below Threshold",
            "P0171": "System Too Lean (Bank 1)",
            "P0172": "System Too Rich (Bank 1)",
            "P0128": "Coolant Thermostat (Coolant Temperature Below Thermostat Regulating Temperature)",
            "P0441": "Evaporative Emission Control System Incorrect Purge Flow",
            "P0700": "Transmission Control System Malfunction",
            "P0750": "Shift Solenoid 'A' Malfunction",
            "C1201": "Engine Control System Malfunction",
            "B1342": "ECM Battery Voltage Out of Range"
        }

    def generate_sample_ecu_logs(self) -> List[Dict[str, Any]]:
        """
        Generate sample ECU log data for demonstration.
        In a real scenario, this would come from vehicle diagnostic tools or CAN bus data.
        """
        import random

        logs = []
        base_time = datetime.now() - timedelta(hours=2)

        # Generate 500 log entries over 2 hours
        for i in range(500):
            timestamp = base_time + timedelta(seconds=i * 14.4)  # Every ~14 seconds

            # Simulate normal operation with occasional anomalies
            log_entry = {
                "timestamp": timestamp.isoformat(),
                "vehicle_id": "VIN123456789",
                "ecu_id": "ECU_ENGINE_01",
                "system": "engine",
                "parameters": {
                    "rpm": random.normalvariate(2000, 500) + (50 if i > 300 else 0),  # Anomaly after time 300
                    "coolant_temp": random.normalvariate(90, 5) + (20 if i > 350 else 0),  # Overheating
                    "oil_pressure": random.normalvariate(45, 8) - (20 if i > 400 else 0),  # Pressure drop
                    "throttle_position": random.uniform(10, 80)
                },
                "error_codes": [],
                "severity": "normal"
            }

            # Add some error conditions
            if i > 300 and random.random() < 0.1:  # 10% chance of error after entry 300
                error_code = random.choice(list(self.error_codes.keys()))
                log_entry["error_codes"].append(error_code)
                log_entry["severity"] = "warning" if error_code.startswith('P') else "critical"

            # Add transmission data occasionally
            if i % 50 == 0:
                log_entry["system"] = "transmission"
                log_entry["ecu_id"] = "ECU_TRANS_01"
                log_entry["parameters"] = {
                    "gear": random.randint(1, 6),
                    "fluid_temp": random.normalvariate(95, 10),
                    "torque": random.normalvariate(250, 50)
                }

            logs.append(log_entry)

        return logs

    def parse_log_patterns(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Parse ECU logs to identify patterns and anomalies.

        Args:
            logs: List of ECU log entries

        Returns:
            Dictionary with parsed patterns and anomalies
        """

        print("üîç Parsing ECU log patterns...")

        # Convert to DataFrame for easier analysis
        df_data = []
        for log in logs:
            row = {
                "timestamp": log["timestamp"],
                "vehicle_id": log["vehicle_id"],
                "ecu_id": log["ecu_id"],
                "system": log["system"],
                "severity": log["severity"],
                "error_codes": ";".join(log["error_codes"]) if log["error_codes"] else ""
            }

            # Flatten parameters
            for param, value in log["parameters"].items():
                row[param] = value

            df_data.append(row)

        df = pd.DataFrame(df_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        # Analyze patterns
        patterns = {
            "total_entries": len(df),
            "time_range": {
                "start": df['timestamp'].min().isoformat(),
                "end": df['timestamp'].max().isoformat(),
                "duration_hours": (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600
            },
            "systems_analyzed": df['system'].unique().tolist(),
            "severity_breakdown": df['severity'].value_counts().to_dict(),
            "error_frequency": Counter([code for codes in df['error_codes'] if codes for code in codes.split(';')]),
            "anomalies_detected": [],
            "parameter_statistics": {}
        }

        # Detect anomalies for each system
        for system in df['system'].unique():
            system_df = df[df['system'] == system]
            system_config = self.ecu_systems.get(system, {})
            normal_ranges = system_config.get("normal_ranges", {})

            for param in system_config.get("parameters", []):
                if param in system_df.columns:
                    values = system_df[param].dropna()

                    if len(values) > 0:
                        # Calculate statistics
                        patterns["parameter_statistics"][f"{system}_{param}"] = {
                            "mean": float(values.mean()),
                            "std": float(values.std()),
                            "min": float(values.min()),
                            "max": float(values.max()),
                            "median": float(values.median())
                        }

                        # Check for out-of-range values
                        if param in normal_ranges:
                            min_val, max_val = normal_ranges[param]
                            out_of_range = values[(values < min_val) | (values > max_val)]

                            if len(out_of_range) > 0:
                                patterns["anomalies_detected"].append({
                                    "system": system,
                                    "parameter": param,
                                    "anomaly_type": "out_of_range",
                                    "count": len(out_of_range),
                                    "percentage": (len(out_of_range) / len(values)) * 100,
                                    "normal_range": normal_ranges[param],
                                    "observed_range": (float(out_of_range.min()), float(out_of_range.max()))
                                })

                        # Detect sudden spikes (values > 2 standard deviations from mean)
                        if len(values) > 10:
                            mean_val = values.mean()
                            std_val = values.std()
                            spikes = values[abs(values - mean_val) > 2 * std_val]

                            if len(spikes) > 0:
                                patterns["anomalies_detected"].append({
                                    "system": system,
                                    "parameter": param,
                                    "anomaly_type": "statistical_spike",
                                    "count": len(spikes),
                                    "percentage": (len(spikes) / len(values)) * 100,
                                    "threshold": f">{mean_val:.2f} ¬± {2*std_val:.2f}",
                                    "spike_values": spikes.tolist()[:5]  # Show first 5 spikes
                                })

        return patterns

    def analyze_with_ai(self, patterns: Dict[str, Any], sample_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Use AI to analyze ECU patterns and provide insights.

        Args:
            patterns: Parsed patterns from log analysis
            sample_logs: Sample of original log entries

        Returns:
            AI analysis with insights and recommendations
        """

        # Prepare data for AI analysis
        analysis_summary = f"""
        ECU Log Analysis Summary:
        - Total entries: {patterns['total_entries']}
        - Time range: {patterns['time_range']['duration_hours']:.1f} hours
        - Systems: {', '.join(patterns['systems_analyzed'])}
        - Severity breakdown: {patterns['severity_breakdown']}
        - Error codes found: {dict(patterns['error_frequency'])}
        - Anomalies detected: {len(patterns['anomalies_detected'])}

        Key Anomalies:
        """

        for anomaly in patterns['anomalies_detected'][:5]:  # Top 5 anomalies
            analysis_summary += f"""
        - {anomaly['system']}.{anomaly['parameter']}: {anomaly['anomaly_type']}
          ({anomaly['count']} occurrences, {anomaly['percentage']:.1f}% of readings)
        """

        # Add sample of recent error codes
        recent_errors = [log for log in sample_logs[-50:] if log.get('error_codes')]
        if recent_errors:
            analysis_summary += "\n\nRecent Error Codes:\n"
            for log in recent_errors[:3]:
                for code in log['error_codes']:
                    analysis_summary += f"- {code}: {self.error_codes.get(code, 'Unknown error')}\n"

        # Create AI prompt for ECU analysis
        prompt = f"""
        As an automotive diagnostics expert, analyze this ECU log data and provide:

        1. Critical issues that need immediate attention
        2. Potential root causes for detected anomalies
        3. Predictive maintenance recommendations
        4. Risk assessment (safety, performance, emissions)
        5. Suggested diagnostic procedures
        6. Timeline for addressing issues (immediate, short-term, long-term)

        ECU Analysis Data:
        {analysis_summary}

        Respond in JSON format:
        {{
            "critical_issues": [
                {{
                    "issue": "description",
                    "system": "affected_system",
                    "severity": "critical/high/medium/low",
                    "immediate_action_required": true/false,
                    "potential_consequences": "description"
                }}
            ],
            "root_cause_analysis": [
                {{
                    "symptom": "observed_anomaly",
                    "likely_causes": ["cause1", "cause2"],
                    "diagnostic_steps": ["step1", "step2"]
                }}
            ],
            "maintenance_recommendations": [
                {{
                    "action": "recommended_action",
                    "timeframe": "immediate/1_week/1_month/3_months",
                    "priority": "high/medium/low",
                    "estimated_cost": "cost_category"
                }}
            ],
            "risk_assessment": {{
                "safety_risk": "low/medium/high",
                "performance_impact": "description",
                "emissions_compliance": "compliant/at_risk/non_compliant",
                "overall_vehicle_health": "excellent/good/fair/poor"
            }},
            "trending_issues": ["issue1", "issue2"],
            "summary": "overall_assessment"
        }}
        """

        try:
            # Make API call to Anthropic
            response = self.ai_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )

            # Parse the AI response
            ai_analysis = json.loads(response.content[0].text)
            return ai_analysis

        except Exception as e:
            print(f"AI analysis failed: {str(e)}")
            # Fallback to rule-based analysis
            return self.fallback_analysis(patterns)

    def fallback_analysis(self, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fallback rule-based analysis when AI is unavailable.

        Args:
            patterns: Parsed patterns from log analysis

        Returns:
            Basic analysis results
        """

        critical_issues = []
        maintenance_recs = []

        # Check for critical anomalies
        for anomaly in patterns['anomalies_detected']:
            if anomaly['percentage'] > 10:  # More than 10% out of range
                critical_issues.append({
                    "issue": f"{anomaly['system']} {anomaly['parameter']} anomaly",
                    "system": anomaly['system'],
                    "severity": "high" if anomaly['percentage'] > 20 else "medium",
                    "immediate_action_required": anomaly['percentage'] > 20,
                    "potential_consequences": "System performance degradation"
                })

        # Check error codes
        if patterns['error_frequency']:
            critical_issues.append({
                "issue": "Active error codes detected",
                "system": "multiple",
                "severity": "high",
                "immediate_action_required": True,
                "potential_consequences": "Vehicle reliability and compliance issues"
            })

        return {
            "critical_issues": critical_issues,
            "root_cause_analysis": [
                {
                    "symptom": "Multiple anomalies detected",
                    "likely_causes": ["Sensor degradation", "Component wear"],
                    "diagnostic_steps": ["Visual inspection", "Component testing"]
                }
            ],
            "maintenance_recommendations": [
                {
                    "action": "Comprehensive diagnostic scan",
                    "timeframe": "immediate",
                    "priority": "high",
                    "estimated_cost": "medium"
                }
            ],
            "risk_assessment": {
                "safety_risk": "medium",
                "performance_impact": "Potential degradation detected",
                "emissions_compliance": "at_risk",
                "overall_vehicle_health": "fair"
            },
            "trending_issues": ["Sensor anomalies"],
            "summary": "Rule-based analysis - AI analysis recommended for detailed insights"
        }

    def generate_diagnostic_report(self, patterns: Dict[str, Any],
                                 ai_analysis: Dict[str, Any]) -> str:
        """
        Generate a comprehensive ECU diagnostic report.

        Args:
            patterns: Parsed patterns from log analysis
            ai_analysis: AI analysis results

        Returns:
            Formatted diagnostic report
        """

        report = []
        report.append("=" * 70)
        report.append("üöó ECU LOG ANALYSIS - DIAGNOSTIC REPORT")
        report.append("=" * 70)
        report.append(f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Vehicle ID: {patterns.get('vehicle_id', 'Multiple vehicles')}")
        report.append(f"Analysis period: {patterns['time_range']['duration_hours']:.1f} hours")
        report.append(f"Total log entries: {patterns['total_entries']}")
        report.append("")

        # Overall health assessment
        risk_assessment = ai_analysis.get('risk_assessment', {})
        report.append("üè• OVERALL VEHICLE HEALTH:")
        report.append("-" * 30)
        report.append(f"Overall Status: {risk_assessment.get('overall_vehicle_health', 'Unknown').upper()}")
        report.append(f"Safety Risk Level: {risk_assessment.get('safety_risk', 'Unknown').upper()}")
        report.append(f"Performance Impact: {risk_assessment.get('performance_impact', 'Not assessed')}")
        report.append(f"Emissions Compliance: {risk_assessment.get('emissions_compliance', 'Unknown').upper()}")
        report.append("")

        # Critical issues
        critical_issues = ai_analysis.get('critical_issues', [])
        if critical_issues:
            report.append("üö® CRITICAL ISSUES REQUIRING ATTENTION:")
            report.append("-" * 45)
            for issue in critical_issues:
                report.append(f"‚ö†Ô∏è  {issue['issue']}")
                report.append(f"    System: {issue['system']}")
                report.append(f"    Severity: {issue['severity'].upper()}")
                report.append(f"    Immediate Action: {'YES' if issue.get('immediate_action_required') else 'NO'}")
                report.append(f"    Consequences: {issue.get('potential_consequences', 'Not specified')}")
                report.append("")

        # Detected anomalies
        if patterns['anomalies_detected']:
            report.append("üìä DETECTED ANOMALIES:")
            report.append("-" * 25)
            for anomaly in patterns['anomalies_detected']:
                report.append(f"‚Ä¢ {anomaly['system'].title()} - {anomaly['parameter']}")
                report.append(f"  Type: {anomaly['anomaly_type']}")
                report.append(f"  Occurrences: {anomaly['count']} ({anomaly['percentage']:.1f}% of readings)")
                if 'normal_range' in anomaly:
                    report.append(f"  Normal Range: {anomaly['normal_range']}")
                    report.append(f"  Observed Range: {anomaly['observed_range']}")
                report.append("")

        # Error codes
        if patterns['error_frequency']:
            report.append("üîç ACTIVE ERROR CODES:")
            report.append("-" * 25)
            for code, count in patterns['error_frequency'].most_common():
                description = self.error_codes.get(code, "Unknown error code")
                report.append(f"  {code}: {description} (occurred {count} times)")
            report.append("")

        # Root cause analysis
        root_causes = ai_analysis.get('root_cause_analysis', [])
        if root_causes:
            report.append("üî¨ ROOT CAUSE ANALYSIS:")
            report.append("-" * 27)
            for analysis in root_causes:
                report.append(f"Symptom: {analysis['symptom']}")
                report.append("Likely Causes:")
                for cause in analysis.get('likely_causes', []):
                    report.append(f"  ‚Ä¢ {cause}")
                report.append("Diagnostic Steps:")
                for step in analysis.get('diagnostic_steps', []):
                    report.append(f"  1. {step}")
                report.append("")

        # Maintenance recommendations
        maintenance_recs = ai_analysis.get('maintenance_recommendations', [])
        if maintenance_recs:
            report.append("üîß MAINTENANCE RECOMMENDATIONS:")
            report.append("-" * 35)

            # Sort by priority and timeframe
            priority_order = {"high": 0, "medium": 1, "low": 2}
            timeframe_order = {"immediate": 0, "1_week": 1, "1_month": 2, "3_months": 3}

            sorted_recs = sorted(maintenance_recs,
                               key=lambda x: (priority_order.get(x.get('priority', 'medium'), 1),
                                            timeframe_order.get(x.get('timeframe', '1_month'), 2)))

            for rec in sorted_recs:
                priority_icon = "üî¥" if rec.get('priority') == 'high' else "üü°" if rec.get('priority') == 'medium' else "üü¢"
                report.append(f"{priority_icon} {rec['action']}")
                report.append(f"    Timeframe: {rec.get('timeframe', 'Not specified')}")
                report.append(f"    Priority: {rec.get('priority', 'medium').upper()}")
                report.append(f"    Estimated Cost: {rec.get('estimated_cost', 'Not specified')}")
                report.append("")

        # Summary
        summary = ai_analysis.get('summary', 'No summary available')
        report.append("üìã EXECUTIVE SUMMARY:")
        report.append("-" * 22)
        report.append(f"  {summary}")
        report.append("")

        # Next steps
        report.append("üöÄ RECOMMENDED NEXT STEPS:")
        report.append("-" * 30)
        report.append("  1. Address all critical issues immediately")
        report.append("  2. Schedule diagnostic procedures for anomalies")
        report.append("  3. Plan maintenance based on recommendations")
        report.append("  4. Set up continuous monitoring for trending issues")
        report.append("  5. Update maintenance schedules based on findings")

        return "\n".join(report)

    def create_visualizations(self, patterns: Dict[str, Any], logs: List[Dict[str, Any]]):
        """
        Create visualizations of ECU data patterns.

        Args:
            patterns: Parsed patterns from log analysis
            logs: Original log data
        """

        # Convert logs to DataFrame for plotting
        df_data = []
        for log in logs:
            row = {
                "timestamp": log["timestamp"],
                "system": log["system"],
                "severity": log["severity"]
            }
            # Flatten parameters
            for param, value in log["parameters"].items():
                row[param] = value
            df_data.append(row)

        df = pd.DataFrame(df_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ECU Log Analysis Dashboard', fontsize=16)

        # Plot 1: Parameter trends over time (Engine RPM)
        engine_data = df[df['system'] == 'engine']
        if not engine_data.empty and 'rpm' in engine_data.columns:
            axes[0, 0].plot(engine_data['timestamp'], engine_data['rpm'], 'b-', alpha=0.7)
            axes[0, 0].set_title('Engine RPM Over Time')
            axes[0, 0].set_ylabel('RPM')
            axes[0, 0].tick_params(axis='x', rotation=45)

        # Plot 2: Severity distribution
        severity_counts = df['severity'].value_counts()
        axes[0, 1].pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%')
        axes[0, 1].set_title('Severity Distribution')

        # Plot 3: Temperature readings
        if not engine_data.empty and 'coolant_temp' in engine_data.columns:
            axes[1, 0].scatter(engine_data['timestamp'], engine_data['coolant_temp'],
                             c=engine_data['coolant_temp'], cmap='coolwarm', alpha=0.6)
            axes[1, 0].set_title('Coolant Temperature')
            axes[1, 0].set_ylabel('Temperature (¬∞C)')
            axes[1, 0].tick_params(axis='x', rotation=45)

        # Plot 4: System activity
        system_counts = df['system'].value_counts()
        axes[1, 1].bar(system_counts.index, system_counts.values)
        axes[1, 1].set_title('Log Entries by System')
        axes[1, 1].set_ylabel('Number of Entries')

        plt.tight_layout()
        plt.savefig('ecu_analysis_dashboard.png', dpi=300, bbox_inches='tight')
        print("üìä Visualization saved as: ecu_analysis_dashboard.png")

def main():
    """
    Main function to demonstrate ECU log analysis.
    """

    print("üöÄ ECU Log Analyzer Demo")
    print("=" * 40)

    # Initialize the analyzer
    analyzer = ECULogAnalyzer()

    # Generate sample ECU logs (in real app, this would read from log files)
    print("üìÑ Generating sample ECU logs...")
    logs = analyzer.generate_sample_ecu_logs()
    print(f"Generated {len(logs)} ECU log entries")
    print()

    # Parse log patterns and detect anomalies
    print("üîç Analyzing log patterns...")
    patterns = analyzer.parse_log_patterns(logs)
    print(f"Detected {len(patterns['anomalies_detected'])} anomalies")
    print()

    # Get AI analysis of patterns
    print("ü§ñ Performing AI-powered analysis...")
    ai_analysis = analyzer.analyze_with_ai(patterns, logs[-10:])  # Send last 10 logs as sample
    print("AI analysis complete!")
    print()

    # Generate comprehensive report
    report = analyzer.generate_diagnostic_report(patterns, ai_analysis)
    print(report)

    # Create visualizations
    print("\nüìä Creating visualizations...")
    analyzer.create_visualizations(patterns, logs)

    # Save analysis results
    output_file = "ecu_analysis_results.json"
    with open(output_file, 'w') as f:
        export_data = {
            "analysis_timestamp": datetime.now().isoformat(),
            "log_summary": {
                "total_entries": patterns["total_entries"],
                "time_range": patterns["time_range"],
                "systems_analyzed": patterns["systems_analyzed"]
            },
            "patterns": patterns,
            "ai_analysis": ai_analysis
        }
        json.dump(export_data, f, indent=2, default=str)

    print(f"\nüíæ Analysis results saved to: {output_file}")

    # Demonstrate integration possibilities
    print("\nüîó INTEGRATION POSSIBILITIES:")
    print("- Real-time CAN bus data integration")
    print("- OBD-II diagnostic tool connectivity")
    print("- Fleet management system integration")
    print("- Predictive maintenance scheduling")
    print("- Automated alert systems for critical issues")
    print("- Integration with dealer service systems")
    print("- Telematics and remote diagnostics")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/VN_user_test_summarizer.py

```python
#!/usr/bin/env python3
# /// script
# dependencies = ["anthropic", "python-dotenv", "pandas"]
# ///

"""
VN_user_test_summarizer.py
Student: VN - "summarizing common pain points in user tests"

This script demonstrates automated analysis of user testing feedback to identify
common pain points, themes, and actionable insights using AI.

Key Learning Objectives:
- Text analysis and pattern recognition
- User feedback processing
- AI-powered insight extraction
- Data aggregation and reporting
"""

import os
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any
from collections import Counter
from dotenv import load_dotenv
import anthropic

# Load environment variables
load_dotenv()

class UserTestSummarizer:
    """
    A class to analyze user testing feedback and extract common pain points.
    This simulates processing user test sessions, surveys, and feedback forms.
    """

    def __init__(self):
        """Initialize the user test summarizer with AI client."""
        # Initialize Anthropic client for AI-powered analysis
        self.ai_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # Define categories for pain point classification
        self.pain_point_categories = [
            "navigation", "usability", "performance", "content_clarity",
            "functionality", "accessibility", "visual_design", "mobile_experience",
            "onboarding", "error_handling"
        ]

        # Severity levels for pain points
        self.severity_levels = {
            "critical": "Prevents task completion",
            "high": "Significantly impacts user experience",
            "medium": "Minor frustration but task can be completed",
            "low": "Cosmetic or preference-based issue"
        }

    def get_sample_user_feedback(self) -> List[Dict[str, Any]]:
        """
        Generate sample user testing feedback for demonstration.
        In a real scenario, this would come from testing platforms or surveys.
        """
        sample_feedback = [
            {
                "session_id": "test_001",
                "user_id": "user_123",
                "test_date": "2024-01-15",
                "user_demographics": {
                    "age_range": "25-34",
                    "tech_savviness": "intermediate",
                    "device": "desktop"
                },
                "task": "Complete product purchase",
                "completion_status": "failed",
                "time_taken_minutes": 12,
                "feedback": "I couldn't find the checkout button after adding items to cart. The cart icon was too small and I didn't realize I needed to click it. When I finally found it, the payment form was confusing - too many fields and unclear which ones were required.",
                "frustration_level": 4,  # 1-5 scale
                "quotes": [
                    "Where is the checkout button?",
                    "This payment form is overwhelming",
                    "I'm not sure if I need to fill out all these fields"
                ]
            },
            {
                "session_id": "test_002",
                "user_id": "user_456",
                "test_date": "2024-01-15",
                "user_demographics": {
                    "age_range": "45-54",
                    "tech_savviness": "beginner",
                    "device": "mobile"
                },
                "task": "Find product information",
                "completion_status": "completed",
                "time_taken_minutes": 8,
                "feedback": "The search function worked well, but the product details page was hard to read on my phone. The text was too small and the images didn't zoom properly. I also couldn't find the product specifications easily.",
                "frustration_level": 2,
                "quotes": [
                    "The text is too small to read",
                    "I can't zoom in on the product images",
                    "Where are the technical specs?"
                ]
            },
            {
                "session_id": "test_003",
                "user_id": "user_789",
                "test_date": "2024-01-16",
                "user_demographics": {
                    "age_range": "18-24",
                    "tech_savviness": "advanced",
                    "device": "mobile"
                },
                "task": "Create user account",
                "completion_status": "completed",
                "time_taken_minutes": 3,
                "feedback": "Account creation was straightforward, but the email verification process was annoying. I didn't get the verification email for 10 minutes, and there was no way to resend it. The error messages were also generic and didn't help me understand what went wrong when I tried to use a password that was apparently too weak.",
                "frustration_level": 3,
                "quotes": [
                    "Why didn't I get the verification email?",
                    "The error message doesn't tell me what's wrong with my password",
                    "There's no resend button for the verification email"
                ]
            },
            {
                "session_id": "test_004",
                "user_id": "user_321",
                "test_date": "2024-01-16",
                "user_demographics": {
                    "age_range": "35-44",
                    "tech_savviness": "intermediate",
                    "device": "desktop"
                },
                "task": "Navigate to customer support",
                "completion_status": "failed",
                "time_taken_minutes": 15,
                "feedback": "I spent way too long trying to find customer support. I looked in the footer, header, and even tried the search function. Finally found it buried in a dropdown menu that wasn't labeled clearly. When I got there, the contact form had a captcha that was impossible to read.",
                "frustration_level": 5,
                "quotes": [
                    "Where is customer support?",
                    "This captcha is unreadable",
                    "The contact form is hidden too deep in the menu"
                ]
            },
            {
                "session_id": "test_005",
                "user_id": "user_654",
                "test_date": "2024-01-17",
                "user_demographics": {
                    "age_range": "55-64",
                    "tech_savviness": "beginner",
                    "device": "tablet"
                },
                "task": "Update profile information",
                "completion_status": "completed",
                "time_taken_minutes": 20,
                "feedback": "The profile page was cluttered and overwhelming. I couldn't tell which sections I could edit and which were just for display. The save button was at the bottom of a very long page, and I wasn't sure if my changes were saved because there was no confirmation message.",
                "frustration_level": 3,
                "quotes": [
                    "This page has too much information",
                    "Can I edit this section or not?",
                    "Did my changes save? I don't see any confirmation"
                ]
            },
            {
                "session_id": "test_006",
                "user_id": "user_987",
                "test_date": "2024-01-17",
                "user_demographics": {
                    "age_range": "25-34",
                    "tech_savviness": "advanced",
                    "device": "desktop"
                },
                "task": "Use advanced search filters",
                "completion_status": "completed",
                "time_taken_minutes": 5,
                "feedback": "The search filters worked well overall, but some of the filter options were confusing. The price range slider was hard to use precisely, and the category filters seemed to overlap in weird ways. Also, clearing all filters required clicking each one individually - there was no 'clear all' button.",
                "frustration_level": 2,
                "quotes": [
                    "The price slider is hard to control",
                    "Why do some products appear in multiple categories?",
                    "I need a way to clear all filters at once"
                ]
            }
        ]

        return sample_feedback

    def analyze_feedback_with_ai(self, feedback_batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Use AI to analyze a batch of user feedback and extract insights.

        Args:
            feedback_batch: List of user feedback dictionaries

        Returns:
            Dictionary with analyzed insights and pain points
        """

        # Prepare feedback for AI analysis
        feedback_text = ""
        for fb in feedback_batch:
            feedback_text += f"""
            Session {fb['session_id']}:
            Task: {fb['task']}
            Status: {fb['completion_status']}
            Device: {fb['user_demographics']['device']}
            Frustration Level: {fb['frustration_level']}/5
            Feedback: {fb['feedback']}
            User Quotes: {'; '.join(fb['quotes'])}
            ---
            """

        # Create AI prompt for comprehensive analysis
        prompt = f"""
        Analyze this user testing feedback and identify:

        1. Top 5 most common pain points across all sessions
        2. Categorize each pain point into: {', '.join(self.pain_point_categories)}
        3. Assign severity level: critical, high, medium, low
        4. Identify patterns by device type (mobile vs desktop vs tablet)
        5. Extract specific actionable recommendations
        6. Highlight any accessibility concerns
        7. Note which issues caused task failures vs completions

        User Feedback Data:
        {feedback_text}

        Respond in JSON format:
        {{
            "common_pain_points": [
                {{
                    "issue": "description",
                    "category": "category",
                    "severity": "level",
                    "frequency": "how often mentioned",
                    "affected_tasks": ["task1", "task2"],
                    "devices_affected": ["device1", "device2"],
                    "user_quotes": ["quote1", "quote2"]
                }}
            ],
            "device_specific_issues": {{
                "mobile": ["issue1", "issue2"],
                "desktop": ["issue1", "issue2"],
                "tablet": ["issue1", "issue2"]
            }},
            "task_failure_causes": ["cause1", "cause2"],
            "actionable_recommendations": [
                {{
                    "recommendation": "specific action",
                    "priority": "high/medium/low",
                    "impact": "expected improvement",
                    "effort": "implementation difficulty"
                }}
            ],
            "accessibility_concerns": ["concern1", "concern2"],
            "overall_sentiment": "summary of user satisfaction"
        }}
        """

        try:
            # Make API call to Anthropic
            response = self.ai_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )

            # Parse the AI response
            ai_analysis = json.loads(response.content[0].text)
            return ai_analysis

        except Exception as e:
            print(f"AI analysis failed: {str(e)}")
            # Fallback to rule-based analysis
            return self.fallback_analysis(feedback_batch)

    def fallback_analysis(self, feedback_batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Fallback rule-based analysis when AI is unavailable.

        Args:
            feedback_batch: List of user feedback dictionaries

        Returns:
            Dictionary with basic analysis results
        """

        # Basic keyword-based analysis
        all_feedback_text = " ".join([fb['feedback'] for fb in feedback_batch])
        all_quotes = [quote for fb in feedback_batch for quote in fb['quotes']]

        # Simple pain point detection based on keywords
        pain_point_keywords = {
            "navigation": ["find", "locate", "hidden", "menu", "where"],
            "usability": ["confusing", "unclear", "difficult", "hard"],
            "performance": ["slow", "loading", "wait", "delay"],
            "mobile_experience": ["small", "zoom", "mobile", "phone"],
            "error_handling": ["error", "message", "wrong", "failed"]
        }

        detected_issues = []
        for category, keywords in pain_point_keywords.items():
            if any(keyword in all_feedback_text.lower() for keyword in keywords):
                detected_issues.append({
                    "issue": f"Issues related to {category}",
                    "category": category,
                    "severity": "medium",
                    "frequency": "multiple mentions",
                    "affected_tasks": [],
                    "devices_affected": [],
                    "user_quotes": [q for q in all_quotes if any(kw in q.lower() for kw in keywords)][:3]
                })

        return {
            "common_pain_points": detected_issues,
            "device_specific_issues": {"mobile": [], "desktop": [], "tablet": []},
            "task_failure_causes": ["Rule-based analysis - limited insights"],
            "actionable_recommendations": [
                {
                    "recommendation": "Conduct detailed analysis with AI",
                    "priority": "high",
                    "impact": "Better insights",
                    "effort": "low"
                }
            ],
            "accessibility_concerns": [],
            "overall_sentiment": "Mixed - rule-based analysis"
        }

    def generate_pain_point_report(self, analysis: Dict[str, Any],
                                 original_feedback: List[Dict[str, Any]]) -> str:
        """
        Generate a comprehensive report of user testing pain points.

        Args:
            analysis: AI analysis results
            original_feedback: Original user feedback data

        Returns:
            Formatted report string
        """

        report = []
        report.append("=" * 70)
        report.append("üë• USER TESTING PAIN POINT ANALYSIS REPORT")
        report.append("=" * 70)
        report.append(f"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total user sessions analyzed: {len(original_feedback)}")

        # Calculate basic metrics
        failed_tasks = [fb for fb in original_feedback if fb['completion_status'] == 'failed']
        avg_frustration = sum(fb['frustration_level'] for fb in original_feedback) / len(original_feedback)

        report.append(f"Task failure rate: {len(failed_tasks)}/{len(original_feedback)} ({len(failed_tasks)/len(original_feedback)*100:.1f}%)")
        report.append(f"Average frustration level: {avg_frustration:.1f}/5")
        report.append("")

        # Top pain points
        report.append("üîç TOP PAIN POINTS IDENTIFIED:")
        report.append("-" * 40)

        for i, pain_point in enumerate(analysis.get('common_pain_points', []), 1):
            report.append(f"{i}. {pain_point['issue']}")
            report.append(f"   Category: {pain_point['category'].replace('_', ' ').title()}")
            report.append(f"   Severity: {pain_point['severity'].upper()}")
            report.append(f"   Frequency: {pain_point['frequency']}")

            if pain_point.get('user_quotes'):
                report.append("   User Quotes:")
                for quote in pain_point['user_quotes'][:2]:
                    report.append(f"   ‚Ä¢ \"{quote}\"")
            report.append("")

        # Device-specific issues
        device_issues = analysis.get('device_specific_issues', {})
        if any(device_issues.values()):
            report.append("üì± DEVICE-SPECIFIC ISSUES:")
            report.append("-" * 30)
            for device, issues in device_issues.items():
                if issues:
                    report.append(f"  {device.title()}:")
                    for issue in issues:
                        report.append(f"    ‚Ä¢ {issue}")
            report.append("")

        # Task failure analysis
        failure_causes = analysis.get('task_failure_causes', [])
        if failure_causes:
            report.append("‚ùå PRIMARY CAUSES OF TASK FAILURES:")
            report.append("-" * 40)
            for cause in failure_causes:
                report.append(f"  ‚Ä¢ {cause}")
            report.append("")

        # Actionable recommendations
        recommendations = analysis.get('actionable_recommendations', [])
        if recommendations:
            report.append("üí° ACTIONABLE RECOMMENDATIONS:")
            report.append("-" * 35)

            # Sort by priority
            priority_order = {"high": 0, "medium": 1, "low": 2}
            sorted_recs = sorted(recommendations,
                               key=lambda x: priority_order.get(x.get('priority', 'medium'), 1))

            for rec in sorted_recs:
                report.append(f"üéØ {rec['recommendation']}")
                report.append(f"   Priority: {rec.get('priority', 'medium').upper()}")
                report.append(f"   Expected Impact: {rec.get('impact', 'Not specified')}")
                report.append(f"   Implementation Effort: {rec.get('effort', 'Not specified')}")
                report.append("")

        # Accessibility concerns
        accessibility = analysis.get('accessibility_concerns', [])
        if accessibility:
            report.append("‚ôø ACCESSIBILITY CONCERNS:")
            report.append("-" * 30)
            for concern in accessibility:
                report.append(f"  ‚Ä¢ {concern}")
            report.append("")

        # Overall sentiment
        sentiment = analysis.get('overall_sentiment', 'Not available')
        report.append("üìä OVERALL USER SENTIMENT:")
        report.append("-" * 30)
        report.append(f"  {sentiment}")
        report.append("")

        # Next steps
        report.append("üöÄ RECOMMENDED NEXT STEPS:")
        report.append("-" * 30)
        report.append("  1. Prioritize high-severity pain points for immediate fixes")
        report.append("  2. Conduct follow-up testing on critical issues")
        report.append("  3. Create detailed user stories for recommended improvements")
        report.append("  4. Set up continuous user feedback collection")
        report.append("  5. Track metrics before and after implementing changes")

        return "\n".join(report)

    def export_to_csv(self, analysis: Dict[str, Any], filename: str = "user_pain_points.csv"):
        """
        Export pain points analysis to CSV for further analysis.

        Args:
            analysis: Analysis results dictionary
            filename: Output CSV filename
        """

        pain_points = analysis.get('common_pain_points', [])

        if not pain_points:
            print("No pain points to export")
            return

        # Convert to DataFrame
        df_data = []
        for pp in pain_points:
            df_data.append({
                'Issue': pp.get('issue', ''),
                'Category': pp.get('category', ''),
                'Severity': pp.get('severity', ''),
                'Frequency': pp.get('frequency', ''),
                'Affected_Tasks': '; '.join(pp.get('affected_tasks', [])),
                'Devices_Affected': '; '.join(pp.get('devices_affected', [])),
                'User_Quotes': '; '.join(pp.get('user_quotes', []))
            })

        df = pd.DataFrame(df_data)
        df.to_csv(filename, index=False)
        print(f"üìÅ Pain points exported to: {filename}")

def main():
    """
    Main function to demonstrate user test summarization and pain point analysis.
    """

    print("üöÄ User Test Pain Point Summarizer Demo")
    print("=" * 50)

    # Initialize the summarizer
    summarizer = UserTestSummarizer()

    # Get sample user feedback (in real app, this would come from testing platforms)
    print("üìä Loading user testing feedback...")
    feedback_data = summarizer.get_sample_user_feedback()
    print(f"Loaded {len(feedback_data)} user testing sessions")
    print()

    # Analyze feedback for pain points
    print("üîç Analyzing feedback with AI...")
    analysis = summarizer.analyze_feedback_with_ai(feedback_data)
    print("Analysis complete!")
    print()

    # Generate comprehensive report
    report = summarizer.generate_pain_point_report(analysis, feedback_data)
    print(report)

    # Export data for further analysis
    summarizer.export_to_csv(analysis)

    # Save full analysis to JSON
    output_file = "user_test_analysis.json"
    with open(output_file, 'w') as f:
        export_data = {
            "analysis_date": datetime.now().isoformat(),
            "sessions_analyzed": len(feedback_data),
            "analysis_results": analysis,
            "raw_feedback_sample": feedback_data[:2]  # Include sample for reference
        }
        json.dump(export_data, f, indent=2, default=str)

    print(f"\nüíæ Full analysis saved to: {output_file}")

    # Demonstrate integration possibilities
    print("\nüîó INTEGRATION POSSIBILITIES:")
    print("- Connect to UserTesting, Maze, or Hotjar APIs")
    print("- Automated analysis of survey responses")
    print("- Integration with product management tools (Jira, Asana)")
    print("- Real-time dashboard for UX teams")
    print("- Automated alerts for critical usability issues")
    print("- Trend analysis over time")

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/browser_automation_get_reviews_from_letterboxd.py

```python
# /// script
# dependencies = [
#   "playwright",
#   "openai",
#   "pyperclip",
# ]
# ///


import openai
import subprocess
import re
from playwright.sync_api import Playwright, sync_playwright, expect
import os
import sys
import pyperclip

def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://letterboxd.com/")
    page.get_by_label("Do not consent").click()
    page.get_by_role("link", name="Sign in").click()
    page.get_by_label("Username").fill(os.environ["LETTERBOXD_USER"])
    page.get_by_label("Username").press("Tab")
    page.get_by_label("Password").fill(os.environ["LETTERBOXD_PWD"])
    page.get_by_role("button", name="Sign in").click()
    page.locator(".navitem > .replace").click()
    page.get_by_label("Search:").fill(movie_to_search)
    page.get_by_role("button", name="Search").click()
    page.get_by_role("link", name=movie_to_search, exact=True).first.click()
    page.get_by_role("link", name="Popular reviews").click()
    try:
        page.get_by_role("link", name="more").click()
    except:
        print("No big reviews to expand")
    page.locator("#content").click()
    page.locator("body").press("ControlOrMeta+a")
    page.locator("body").press("ControlOrMeta+c")

    # ---------------------
    context.close()
    browser.close()

movie_to_search = sys.argv[1] 
with sync_playwright() as playwright:
    run(playwright)



# Get clipboard content
clipboard_content = pyperclip.paste()

# Extract reviews using OpenAI
from openai import OpenAI
client = OpenAI()

def process_movie_reviews(prompt_question):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "You are a movie review curator. \
                                                    Your task is to extract and format \
                                                    movie reviews from raw HTML content"},
                    {"role": "user", "content": prompt_question}]
    )
    
    return response.choices[0].message.content

# Save formatted reviews to file
reviews_filename = f"{movie_to_search}-reviews.md"
with open(reviews_filename, "w", encoding="utf-8") as f:
    f.write(process_movie_reviews(f"Extract the movie reviews from this raw html: \n {clipboard_content}"))

# Open the file
if sys.platform == "win32":
    os.startfile(reviews_filename)
else:
    opener = "open" if sys.platform == "darwin" else "xdg-open"
    subprocess.call([opener, reviews_filename])



    
```

---

## scripts/additional-scripts/browser_automation_imdb.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "playwright>=1.45",
#     "typer>=0.12",
#     "rich>=13.7",
# ]
# ///
"""
Usage:
  uv run imdb_fetch.py "The Matrix"
  uv run imdb_fetch.py "Interstellar" --show --json out.json

Notes:
  - First time only, install browsers: `uv run playwright install`
"""

from __future__ import annotations
import json
import re
from typing import Optional, List, Dict
import typer
from rich import print as rprint
from playwright.sync_api import sync_playwright, Playwright, Page, TimeoutError as PWTimeout

app = typer.Typer(add_completion=False)

def _click_consent(page: Page) -> None:
    """Best-effort click for IMDB/GDPR consent banners (varies by region)."""
    candidates = [
        # IMDB / Amazon consent variants
        {"role": "button", "name": re.compile(r"Accept all|Accept|Agree|OK|Got it", re.I)},
        {"role": "button", "name": re.compile(r"Reject all|Reject", re.I)},  # sometimes required before Accept
    ]
    for sel in candidates:
        try:
            page.get_by_role(sel["role"], name=sel["name"]).click(timeout=2000)
        except PWTimeout:
            pass
    # Some regions use shadow/iframes; ignore silently if not found.

def _text_or_none(page: Page, selector: str) -> Optional[str]:
    try:
        el = page.locator(selector).first
        el.wait_for(state="attached", timeout=1000)
        txt = el.inner_text().strip()
        return txt if txt else None
    except PWTimeout:
        return None
    except Exception:
        return None

def _all_text(page: Page, selector: str, limit: int | None = None) -> List[str]:
    try:
        loc = page.locator(selector)
        count = loc.count()
        items = []
        for i in range(min(count, limit or count)):
            txt = loc.nth(i).inner_text().strip()
            if txt:
                items.append(re.sub(r"\s+", " ", txt))
        return items
    except Exception:
        return []

def _parse_title_year_block(page: Page) -> Dict[str, Optional[str]]:
    # Title
    title = (
        _text_or_none(page, '[data-testid="hero__pageTitle"] h1')
        or _text_or_none(page, 'h1[data-testid="hero-title-block__title"]')
        or _text_or_none(page, "h1")
    )
    # Year (first item in hero metadata list)
    year = None
    try:
        metas = _all_text(page, '[data-testid="hero-title-block__metadata"] li', limit=3)
        # Usually: [YEAR, "Xh Ym", "Genre count/age rating"] ‚Äî pick a 4-digit year if present
        for m in metas:
            m_year = re.search(r"(19|20|21)\d{2}", m)
            if m_year:
                year = m_year.group(0)
                break
    except Exception:
        pass
    return {"title": title, "year": year}

def _parse_rating(page: Page) -> Optional[str]:
    # New IMDB selector (score like "8.7 /10")
    rating = _text_or_none(page, '[data-testid="hero-rating-bar__aggregate-rating__score"] span')
    if rating:
        rating = rating.replace("\n", " ").strip()
        return rating
    # Fallback older layouts
    rating = _text_or_none(page, '[itemprop="ratingValue"]')
    return rating

def _parse_genres(page: Page) -> List[str]:
    # Common genres container
    genres = _all_text(page, '[data-testid="genres"] a')
    if not genres:
        genres = _all_text(page, 'a[href*="/search/title/?genres="]')
    return genres

def _parse_plot(page: Page) -> Optional[str]:
    for sel in [
        '[data-testid="plot"] span',
        '[data-testid="plot-xl"]',
        '[data-testid="storyline-plot-summary"]',
        '.sc-16ede01-2 span',  # occasional summary block
    ]:
        txt = _text_or_none(page, sel)
        if txt:
            return re.sub(r"\s+", " ", txt)
    return None

def _parse_people_block(page: Page, role_labels: List[str]) -> List[str]:
    """
    Extract names from the principal credits box by role label (e.g., 'Director', 'Writers', 'Stars').
    IMDB often uses data-testid="title-pc-principal-credit".
    """
    names: List[str] = []
    try:
        credits = page.locator('[data-testid="title-pc-principal-credit"]')
        for i in range(credits.count()):
            block = credits.nth(i)
            label = block.locator("span").first.inner_text().strip()
            if any(lbl.lower() in label.lower() for lbl in role_labels):
                people_links = block.locator("a[href*='/name/']")
                for j in range(people_links.count()):
                    nm = people_links.nth(j).inner_text().strip()
                    if nm:
                        names.append(nm)
    except Exception:
        pass
    # Fallback: generic cast header sections
    if not names:
        names = _all_text(page, "a[href*='/name/']")[:5]
    return names

def fetch_movie(playwright: Playwright, query: str, show: bool = False) -> Dict[str, object]:
    browser = playwright.chromium.launch(headless=not show)
    context = browser.new_context()
    page = context.new_page()

    # 1) Go to IMDB and accept consent if present
    page.goto("https://www.imdb.com/", wait_until="domcontentloaded")
    _click_consent(page)

    # 2) Search for the query
    try:
        searchbox = page.get_by_role("combobox", name=re.compile(r"Search IMDb", re.I))
    except Exception:
        searchbox = page.locator("input[type='text'][name='q']").first
    searchbox.click()
    searchbox.fill(query)
    searchbox.press("Enter")

    # 3) On results page, click the first "Titles" result
    try:
        # Prefer the new results list (data-testid on result items)
        page.wait_for_selector('[data-testid="find-results-section-title"]', timeout=5000)
        # Try "Titles" section first
        titles_section = page.locator('section:has(h3:has-text("Titles"))')
        if titles_section.count() == 0:
            titles_section = page  # fallback: any result list
        first_result = titles_section.locator("a[href*='/title/']").first
        first_result.click()
    except PWTimeout:
        # Fallback: generic first /title/ link
        page.locator("a[href*='/title/']").first.click()

    page.wait_for_load_state("domcontentloaded")

    # 4) Extract details from the title page
    meta = _parse_title_year_block(page)
    rating = _parse_rating(page)
    genres = _parse_genres(page)
    plot = _parse_plot(page)
    directors = _parse_people_block(page, ["Director"])
    stars = _parse_people_block(page, ["Stars", "Cast"])

    url = page.url

    context.close()
    browser.close()

    return {
        "query": query,
        "url": url,
        "title": meta.get("title"),
        "year": meta.get("year"),
        "rating": rating,          # e.g., "8.7 /10"
        "genres": genres,          # list[str]
        "plot": plot,              # short summary
        "directors": directors,    # list[str]
        "stars": stars[:5],        # first few names
    }

@app.command()
def main(
    query: str = typer.Argument(..., help="Movie title to search on IMDB (e.g., 'The Matrix')"),
    show: bool = typer.Option(False, "--show", help="Show the browser (headless=False)"),
    json_out: Optional[str] = typer.Option(None, "--json", help="Output path to save JSON result"),
):
    """Fetch movie info from IMDB via Playwright and print JSON to stdout."""
    with sync_playwright() as pw:
        data = fetch_movie(pw, query=query, show=show)

    # Pretty print to console
    rprint(data)

    # Optional save to file
    if json_out:
        with open(json_out, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        rprint(f"[green]Saved JSON to[/green] {json_out}")

if __name__ == "__main__":
    app()

```

---

## scripts/additional-scripts/browser_automation_securing_login.py

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    page = context.new_page()
    page.goto("https://letterboxd.com/")
    
    input("Manually log in and press Enter to continue...")

    context.storage_state(path="auth.json")
    print("Auth state saved to auth.json")
    browser.close()
```

---

## scripts/additional-scripts/data_extraction_pdf_example_live.py

```python
# extraction.py
# /// script
# requires-python = ">=3.10"
# dependencies = ["pdfplumber>=0.11", "pandas>=2.0"]
# ///
#
# How to run (example):
#   uv run extraction.py "/path/to/input.pdf"            # writes alongside the PDF
#   uv run extraction.py "/path/to/input.pdf" output.csv # optional explicit output
#
# What it does:
#   1) Opens the PDF.
#   2) Scans pages for tables.
#   3) Finds the table with headers like: Month | Due Date | Amount ($).
#   4) Normalizes rows and writes a CSV.

from __future__ import annotations
import sys
import os
import re
from typing import List, Optional
import pandas as pd
import pdfplumber

REQUIRED_COLS = {"month", "due date", "amount"}  # relaxed match (case/spacing)

def usage_and_exit() -> None:
    print("Usage:\n  uv run extraction.py <input.pdf> [output.csv]")
    sys.exit(1)

# --- Parse CLI args (keep it simple) ---
if len(sys.argv) < 2:
    usage_and_exit()
in_path = sys.argv[1]
if not os.path.isfile(in_path) or not in_path.lower().endswith(".pdf"):
    print("Error: first argument must be a valid PDF file path.")
    sys.exit(1)
out_path = (
    sys.argv[2]
    if len(sys.argv) >= 3
    else os.path.splitext(in_path)[0] + ".csv"
)

# --- Helpers to lightly normalize headers/cells ---
def norm_head(s: str) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    s = s.replace("( $ )", "$")  # just in case weird spacing
    s = s.lower()
    s = s.replace("amount ($)", "amount").replace("amount($)", "amount")
    return s

def looks_like_header(row: List[str]) -> bool:
    heads = {norm_head(c) for c in row if c and str(c).strip()}
    # allow supersets (e.g., extra blank columns)
    return REQUIRED_COLS.issubset({h.replace(":", "") for h in heads})

def clean_money(val: str) -> str:
    # keep as text in CSV; strip currency formatting like "$5,000"
    s = (val or "").strip()
    s = s.replace("$", "").replace(",", "")
    return s

# --- Extract tables and pick the one matching required headers ---
rows: List[List[str]] = []
with pdfplumber.open(in_path) as pdf:
    for page in pdf.pages:
        # Try a "lines" strategy first (works for bordered tables),
        # then a gentler "text" fallback.
        for settings in (
            {"vertical_strategy": "lines", "horizontal_strategy": "lines"},
            {"vertical_strategy": "text", "horizontal_strategy": "text"},
        ):
            try:
                tables = page.extract_tables(table_settings=settings)
            except Exception:
                continue
            for tbl in tables or []:
                # find header row index
                header_idx: Optional[int] = None
                for i, r in enumerate(tbl):
                    if r and looks_like_header([str(c) if c is not None else "" for c in r]):
                        header_idx = i
                        break
                if header_idx is None:
                    continue

                header = [norm_head(str(c) if c is not None else "") for c in tbl[header_idx]]
                # map indices of columns we care about
                def col_idx(key: str) -> Optional[int]:
                    for j, h in enumerate(header):
                        if key == h or key in h:
                            return j
                    return None

                m_i = col_idx("month")
                d_i = col_idx("due date")
                a_i = col_idx("amount")
                if None in (m_i, d_i, a_i):
                    continue

                # collect body rows after header
                for r in tbl[header_idx + 1 :]:
                    if not any(x for x in r):
                        continue
                    month = (r[m_i] or "").strip()
                    due   = (r[d_i] or "").strip()
                    amt   = clean_money(r[a_i] or "")
                    # simple guard to skip junk lines
                    if not month or not due:
                        continue
                    rows.append([month, due, amt])

        if rows:
            break  # stop after first matching table found

# --- If nothing found, fail clearly (don‚Äôt guess) ---
if not rows:
    print("I cannot verify a matching table in this PDF (headers not found).")
    sys.exit(2)

# --- Write CSV (Month, Due Date, Amount) ---
df = pd.DataFrame(rows, columns=["Month", "Due Date", "Amount"])
df.to_csv(out_path, index=False)
print(f"Saved: {out_path}")
```

---

## scripts/additional-scripts/data_extraction_simple_pdf.py

```python
# /// script
# dependencies = [
#   "pymupdf",
#   "pdfplumber",
# ]
# ///

import fitz  # PyMuPDF
import pdfplumber
import csv
import argparse

def extract_text_from_pdf(pdf_path, text_output_path):
    """Extracts text from a PDF and saves it to a text file."""
    with fitz.open(pdf_path) as doc, open(text_output_path, 'w', encoding='utf-8') as text_file:
        for page in doc:
            text = page.get_text("text")  # Extract text from the page
            text_file.write(text + '\n' + '-'*80 + '\n')  # Separate pages
    print(f"Text extracted and saved to {text_output_path}")

def extract_tables_from_pdf(pdf_path, csv_output_path):
    """Extracts tables from a PDF and saves them as CSV files."""
    with pdfplumber.open(pdf_path) as pdf:
        tables = []
        for i, page in enumerate(pdf.pages):
            extracted_tables = page.extract_tables()
            if extracted_tables:
                tables.extend(extracted_tables)
    
    if tables:
        with open(csv_output_path, 'w', newline='', encoding='utf-8') as csv_file:
            writer = csv.writer(csv_file)
            for table in tables:
                for row in table:
                    writer.writerow(row)
        print(f"Tables extracted and saved to {csv_output_path}")
    else:
        print("No tables found in the PDF.")

def main():
    parser = argparse.ArgumentParser(description='Extract text and tables from PDF files')
    parser.add_argument('pdf_path', help='Path to the input PDF file')
    parser.add_argument('--text-output', default='extracted_text.txt', 
                       help='Path for the extracted text output (default: extracted_text.txt)')
    parser.add_argument('--csv-output', default='extracted_tables.csv',
                       help='Path for the extracted tables output (default: extracted_tables.csv)')
    
    args = parser.parse_args()
    
    extract_text_from_pdf(args.pdf_path, args.text_output)
    extract_tables_from_pdf(args.pdf_path, args.csv_output)

if __name__ == "__main__":
    main()
```

---

## scripts/additional-scripts/generate_synthetic_excel_data.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "pandas",
#     "numpy",
#     "openpyxl",
# ]
# ///

import os
import pandas as pd
from datetime import datetime, timedelta
import numpy as np

# Create input folder
input_dir = "./input"
os.makedirs(input_dir, exist_ok=True)

# Script expects columns A:Q ‚Üí 17 columns total
required_cols = [
    "Date",
    "Stock Symbol",
    "Close Price",
    "Moving Average (50-day)",
    "RSI (Relative Strength Index)",
    "MACD",
    "Signal Line",
]

# Create 10 extra dummy columns so total = 17 (A‚ÜíQ)
extra_cols = [f"Extra_{i}" for i in range(1, 17 - len(required_cols) + 1)]

all_cols = required_cols + extra_cols

start_date = datetime.today() - timedelta(days=30)
symbols = ["AAPL", "MSFT"]

for file_index in range(1, 3):
    data = []
    for symbol in symbols:
        date = start_date
        for i in range(30):
            row = {
                "Date": date,
                "Stock Symbol": symbol,
                "Close Price": 150 + i + (file_index * 2),
                "Moving Average (50-day)": 148 + i,
                "RSI (Relative Strength Index)": 40 + (i % 20),
                "MACD": 1.2 + (i * 0.05),
                "Signal Line": 1.0 + (i * 0.04),
            }
            # add empty columns
            for col in extra_cols:
                row[col] = np.nan
            data.append(row)
            date += timedelta(days=1)

    df = pd.DataFrame(data, columns=all_cols)
    df.to_excel(os.path.join(input_dir, f"sample_data_{file_index}.xlsx"), index=False)

input_dir
```

---

## scripts/additional-scripts/pdf_analysis.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "pandas>=2.0",
#     "matplotlib>=3.7",
# ]
# ///
"""
Usage:
  uv run pdf_analysis.py /path/to/stock-trading-data.csv
  (If no path is given, it tries './stock-trading-data.csv')

Output:
  ./stock_analysis_report.pdf
"""

# -----------------------------
# Imports & CLI argument parsing
# -----------------------------
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter

# -----------------------------
# Parse input CSV path (positional, optional)
# -----------------------------
parser = argparse.ArgumentParser(description="Generate a professional PDF dashboard report for stock trading data.")
parser.add_argument("csv", nargs="?", default="stock-trading-data.csv",
                    help="Path to the input CSV (default: ./stock-trading-data.csv)")
parser.add_argument("--out", default="stock_analysis_report.pdf",
                    help="Output PDF filename (default: stock_analysis_report.pdf)")
args = parser.parse_args()
csv_path = Path(args.csv)
out_pdf = Path(args.out)

# -----------------------------
# Theme & helper formatters
# -----------------------------
# A lightweight, modern visual theme (fonts, grids, colors, spacing).
mpl.rcParams.update({
    "figure.dpi": 120,
    "savefig.dpi": 300,
    "figure.facecolor": "white",
    "axes.facecolor": "white",
    "axes.edgecolor": "#E6E6E6",
    "axes.grid": True,
    "grid.color": "#ECECEC",
    "grid.linewidth": 0.8,
    "grid.alpha": 1.0,
    "axes.titleweight": "bold",
    "axes.titlesize": 13,
    "axes.labelsize": 11,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "legend.frameon": False,
    "font.family": "DejaVu Sans",
    "axes.prop_cycle": mpl.cycler(color=[
        "#0F609B", "#EF6C00", "#2F855A", "#805AD5", "#D53F8C", "#2D3748"
    ]),
})

def fmt_currency(x, _pos=None):  # Price axis
    return f"${x:,.0f}" if abs(x) >= 1 else f"${x:,.2f}"

def fmt_percent(x, _pos=None):   # Return axis
    return f"{x:.0f}%"

def fmt_thousands(x, _pos=None): # Volume axis
    # Show in millions when large
    if abs(x) >= 1_000_000:
        return f"{x/1_000_000:.1f}M"
    if abs(x) >= 1_000:
        return f"{x/1_000:.0f}K"
    return f"{x:.0f}"

def date_axis(ax):
    """Concise, professional date formatting with quarterly tick preference."""
    locator = mdates.AutoDateLocator(minticks=5, maxticks=8)
    ax.xaxis.set_major_locator(locator)
    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator))
    for label in ax.get_xticklabels():
        label.set_rotation(0)
        label.set_ha("center")

# -----------------------------
# Load data & light validation
# -----------------------------
required_cols = [
    "Date",
    "Stock Symbol",
    "Open Price",
    "Close Price",
    "Volume",
    "Moving Average (50-day)",
    "RSI (Relative Strength Index)"
]
df = pd.read_csv(csv_path)
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
df = df.dropna(subset=["Date"])
df = df.sort_values(["Stock Symbol", "Date"]).reset_index(drop=True)

# Pick primary ticker (AAPL if present, else first by alphabetical)
tickers = sorted(df["Stock Symbol"].unique().tolist())
primary = "AAPL" if "AAPL" in tickers else tickers[0]
primary_df = df[df["Stock Symbol"] == primary].copy()

# Convenience metrics
returns = df.groupby("Stock Symbol").apply(
    lambda x: (x["Close Price"].iloc[-1] / max(x["Close Price"].iloc[0], 1e-9) - 1) * 100
).sort_values(ascending=False)
avg_volume = df.groupby("Stock Symbol")["Volume"].mean().sort_values(ascending=False)

# -----------------------------
# Page decorations: header/footer
# -----------------------------
def decorate_page(fig, title_left: str, title_right: str, page_number: int | None = None):
    """Adds a subtle header and a footer with page number."""
    # Header bar
    fig.text(0.055, 0.985, title_left, ha="left", va="top", fontsize=10, color="#4A5568")
    fig.text(0.945, 0.985, title_right, ha="right", va="top", fontsize=10, color="#4A5568")
    # Footer line + page
    fig.text(0.5, 0.018, "Generated with Python ‚Ä¢ pandas ‚Ä¢ matplotlib", ha="center", va="bottom", fontsize=8, color="#A0AEC0")
    if page_number is not None:
        fig.text(0.98, 0.018, f"{page_number}", ha="right", va="bottom", fontsize=8, color="#A0AEC0")

# -----------------------------
# Helper: clean "text page" with real hierarchy
# -----------------------------
def add_text_page(pdf, title: str, blocks: list[tuple[str, list[str]]], subtitle: str | None = None, page_num: int | None = None):
    """
    Renders a simple, tidy 'document-like' page.
    blocks: [(heading, [bullet lines...]), ...]
    """
    fig = plt.figure(figsize=(10.5, 7.2), layout="constrained")
    gs = fig.add_gridspec(nrows=12, ncols=12)
    ax = fig.add_subplot(gs[:, :])
    ax.axis("off")

    # Title & optional subtitle
    y = 0.94
    ax.text(0.02, y, title, fontsize=22, fontweight="bold", color="#1A202C", va="top")
    if subtitle:
        ax.text(0.02, y-0.06, subtitle, fontsize=11, color="#4A5568", va="top")
    y -= 0.10

    # Content blocks
    for heading, lines in blocks:
        ax.text(0.02, y, heading, fontsize=14, fontweight="bold", color="#2D3748", va="top")
        y -= 0.045
        for line in lines:
            ax.text(0.04, y, f"‚Ä¢  {line}", fontsize=11, color="#2D3748", va="top")
            y -= 0.035
        y -= 0.02  # space between blocks

    decorate_page(fig,
                  title_left="Stock Trading Dashboard Report",
                  title_right=f"Universe: {', '.join(tickers)}",
                  page_number=page_num)
    pdf.savefig(fig, bbox_inches="tight")
    plt.close(fig)

# -----------------------------
# Charts
# -----------------------------
def page_price_and_ma(pdf, data: pd.DataFrame, page_num: int | None = None):
    fig = plt.figure(figsize=(10.5, 6.8), layout="constrained")
    ax = fig.add_subplot(111)

    ax.plot(data["Date"], data["Close Price"], linewidth=2.2, label="Close")
    ax.plot(data["Date"], data["Moving Average (50-day)"], linewidth=2.0, linestyle="--", label="50-day MA", alpha=0.9)

    ax.set_title(f"{primary} ‚Äî Closing Price & 50-day Moving Average")
    ax.set_ylabel("Price")
    ax.yaxis.set_major_formatter(FuncFormatter(fmt_currency))
    date_axis(ax)
    ax.grid(True, which="major")
    ax.legend(ncols=2, frameon=False, loc="upper left")

    # Light band to guide the eye around recent data
    if len(data) > 5:
        last_dt = data["Date"].iloc[-1]
        first_dt = data["Date"].iloc[max(0, len(data)-60)]
        ax.axvspan(first_dt, last_dt, alpha=0.04, color="#3182CE")

    decorate_page(fig,
                  title_left="1) Price & Trend",
                  title_right=f"Window: {data['Date'].min().date()} ‚Üí {data['Date'].max().date()}",
                  page_number=page_num)
    pdf.savefig(fig, bbox_inches="tight")
    plt.close(fig)

def page_rsi(pdf, data: pd.DataFrame, page_num: int | None = None):
    fig = plt.figure(figsize=(10.5, 5.2), layout="constrained")
    ax = fig.add_subplot(111)

    ax.plot(data["Date"], data["RSI (Relative Strength Index)"], linewidth=1.8, label="RSI")
    ax.fill_between(data["Date"], 70, 100, alpha=0.06, label="Overbought")
    ax.fill_between(data["Date"], 0, 30, alpha=0.06, label="Oversold")

    ax.axhline(70, linestyle="--", linewidth=1, color="#E53E3E")
    ax.axhline(30, linestyle="--", linewidth=1, color="#38A169")
    ax.set_ylim(0, 100)
    ax.set_title(f"{primary} ‚Äî RSI (Relative Strength Index)")
    ax.set_ylabel("RSI")
    date_axis(ax)
    ax.legend(ncols=3, frameon=False, loc="upper left")

    decorate_page(fig, title_left="2) Momentum (RSI)", title_right="Reference: 30 / 70 bands", page_number=page_num)
    pdf.savefig(fig, bbox_inches="tight")
    plt.close(fig)

def page_total_returns(pdf, ret: pd.Series, page_num: int | None = None):
    fig = plt.figure(figsize=(10.5, 6.0), layout="constrained")
    ax = fig.add_subplot(111)

    ret_sorted = ret.sort_values(ascending=False)
    bars = ax.bar(ret_sorted.index, ret_sorted.values)

    ax.set_title("Total Return by Stock (%)")
    ax.set_ylabel("Return (%)")
    ax.yaxis.set_major_formatter(FuncFormatter(fmt_percent))
    ax.set_xlabel("Stock Symbol")
    ax.grid(axis="y")

    # Annotate bars with values
    for b in bars:
        ax.text(b.get_x() + b.get_width()/2, b.get_height(),
                f"{b.get_height():.0f}%", ha="center", va="bottom", fontsize=9)

    decorate_page(fig, title_left="3) Cross-Section Performance", title_right="First‚ÜíLast available close", page_number=page_num)
    pdf.savefig(fig, bbox_inches="tight")
    plt.close(fig)

def page_avg_volume(pdf, volume_mean: pd.Series, page_num: int | None = None):
    fig = plt.figure(figsize=(10.5, 6.0), layout="constrained")
    ax = fig.add_subplot(111)

    vol_sorted = volume_mean.sort_values(ascending=False)
    bars = ax.bar(vol_sorted.index, vol_sorted.values)

    ax.set_title("Average Daily Trading Volume by Stock")
    ax.set_ylabel("Average Volume")
    ax.yaxis.set_major_formatter(FuncFormatter(fmt_thousands))
    ax.set_xlabel("Stock Symbol")
    ax.grid(axis="y")

    for b in bars:
        ax.text(b.get_x() + b.get_width()/2, b.get_height(),
                fmt_thousands(b.get_height()), ha="center", va="bottom", fontsize=9)

    decorate_page(fig, title_left="4) Liquidity Snapshot", title_right="Higher volume ‚Üí easier execution (usually)", page_number=page_num)
    pdf.savefig(fig, bbox_inches="tight")
    plt.close(fig)

# -----------------------------
# Build the PDF report
# -----------------------------
with PdfPages(out_pdf) as pdf:
    page = 1

    # Cover / Overview
    add_text_page(
        pdf,
        title="Stock Trading Dashboard Report",
        subtitle=f"Time window: {df['Date'].min().date()} ‚Üí {df['Date'].max().date()}",
        blocks=[
            ("Universe",
             [", ".join(tickers)]),
            ("What‚Äôs inside",
             [
                 "1) Price & 50-day Moving Average (primary ticker)",
                 "2) RSI momentum with overbought/oversold zones",
                 "3) Cross-section: Total Return by Stock (%)",
                 "4) Cross-section: Average Daily Trading Volume",
             ]),
            ("How to read",
             [
                 "Price vs. 50-day MA highlights medium-term trend shifts.",
                 "RSI >70 ‚âà overbought, <30 ‚âà oversold (context matters).",
                 "Total return = last close / first close ‚àí 1.",
                 "Higher average volume often implies tighter spreads.",
             ]),
        ],
        page_num=page,
    ); page += 1

    # 1) Price & 50-day MA (primary)
    add_text_page(
        pdf,
        title=f"1) {primary} ‚Äî Closing Price & 50-day Moving Average",
        blocks=[
            ("Why it matters",
             ["Trend + price context improves timing and risk framing."]),
            ("Reading tips",
             ["Watch crossovers and persistent gaps between price and MA.",
              "Recent period highlighted to focus attention."]),
        ],
        page_num=page,
    ); page += 1
    page_price_and_ma(pdf, primary_df, page_num=page); page += 1

    # 2) RSI
    add_text_page(
        pdf,
        title=f"2) {primary} ‚Äî RSI (Relative Strength Index)",
        blocks=[
            ("Why it matters",
             ["Momentum oscillator that can signal exhaustion or reversals."]),
            ("Reading tips",
             ["Zones shaded for quick scan; combine with price action."]),
        ],
        page_num=page,
    ); page += 1
    page_rsi(pdf, primary_df, page_num=page); page += 1

    # 3) Total Return
    add_text_page(
        pdf,
        title="3) Total Return by Stock (%)",
        blocks=[
            ("Why it matters",
             ["Cross-section snapshot to see relative winners/laggards."]),
            ("Method",
             ["Simple first‚Üílast close change (unadjusted for dividends/splits)."]),
        ],
        page_num=page,
    ); page += 1
    page_total_returns(pdf, returns, page_num=page); page += 1

    # 4) Average Volume
    add_text_page(
        pdf,
        title="4) Average Daily Trading Volume by Stock",
        blocks=[
            ("Why it matters",
             ["Liquidity affects execution quality, slippage, and viability of strategies."]),
            ("Reading tips",
             ["Consider volume alongside volatility and market microstructure."]),
        ],
        page_num=page,
    ); page += 1
    page_avg_volume(pdf, avg_volume, page_num=page); page += 1

print(f"Report written to: {out_pdf.resolve()}")
```

---

## scripts/additional-scripts/slide_deck_automation_ppt_example.py

```python
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "python-pptx",
#   "matplotlib",
#   "pillow",
# ]
# ///
"""
Generates a 5-slide PPTX on how LLMs augment productivity.
Slides:
1) Title
2) Chart (LLM usage/adoption) with sources
3) Diagram: How LLMs improve productivity (shapes + arrows)
4) Creative: ‚ÄúBefore vs After‚Äù workflow visual + callout
5) Conclusion
Data sources (fetched & verified by you earlier; cited on slides):
- Stack Overflow Dev Survey 2024 (AI tool usage 62% currently use / 76% use-or-plan)
  https://survey.stackoverflow.co/2024/ai
- McKinsey 'State of AI' 2024 (65% regularly using genAI)
  https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-2024
- Pew Research (Jun 25, 2025): 34% of U.S. adults have used ChatGPT
  https://www.pewresearch.org/short-reads/2025/06/25/34-of-us-adults-have-used-chatgpt-about-double-the-share-in-2023/
- VS Magazine summary of a Copilot study: ~55% faster on a coding task
  https://visualstudiomagazine.com/articles/2024/09/17/another-report-weighs-in-on-github-copilot-dev-productivity.aspx
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.enum.text import PP_ALIGN
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE
from pptx.enum.dml import MSO_LINE
from pptx.enum.text import MSO_AUTO_SIZE
from pptx.oxml.xmlchemy import OxmlElement

import io
import matplotlib.pyplot as plt

def add_metadata(slide, key, value):
    # tiny hidden textbox for metadata
    tx = slide.shapes.add_textbox(Inches(0.1), Inches(6.9), Inches(2.5), Inches(0.25))
    tf = tx.text_frame
    p = tf.paragraphs[0]
    run = p.add_run()
    run.text = f"{key}: {value}"
    run.font.size = Pt(1)
    run.font.color.rgb = RGBColor(255, 255, 255)

def add_footnote(slide, text):
    box = slide.shapes.add_textbox(Inches(0.5), Inches(6.8), Inches(9), Inches(0.6))
    tf = box.text_frame
    tf.clear()
    p = tf.paragraphs[0]
    p.text = text
    for run in p.runs:
        run.font.size = Pt(10)
        run.font.color.rgb = RGBColor(100, 100, 100)

def add_title(prs, title_text, subtitle_text=None, layout_idx=0):
    slide = prs.slides.add_slide(prs.slide_layouts[layout_idx])
    slide.shapes.title.text = title_text
    if subtitle_text is not None and len(slide.placeholders) > 1:
        slide.placeholders[1].text = subtitle_text
    return slide

def add_bullets(slide, title, bullets):
    slide.shapes.title.text = title
    body = slide.placeholders[1].text_frame
    body.clear()
    for i, b in enumerate(bullets):
        p = body.add_paragraph() if i > 0 else body.paragraphs[0]
        p.text = b
        p.level = 0

def add_matplotlib_chart_image(slide, fig, left_in=0.7, top_in=1.8, width_in=8.0):
    bio = io.BytesIO()
    fig.savefig(bio, format="png", dpi=200, bbox_inches="tight")
    plt.close(fig)
    bio.seek(0)
    slide.shapes.add_picture(bio, Inches(left_in), Inches(top_in), width=Inches(width_in))

def make_usage_chart():
    """
    Bars comparing different, reputable metrics (different populations):
    - Developers using AI tools (StackOverflow 2024): 62%
    - Organizations regularly using GenAI (McKinsey 2024): 65%
    - U.S. adults who have ever used ChatGPT (Pew 2025): 34%
    """
    labels = [
        "Devs using AI tools (2024)",
        "Orgs using GenAI (2024)",
        "U.S. adults used ChatGPT (2025)",
    ]
    values = [62, 65, 34]

    fig = plt.figure(figsize=(8, 3.8))
    ax = fig.add_subplot(111)
    bars = ax.bar(labels, values)
    ax.set_ylim(0, 100)
    ax.set_ylabel("Percent")
    ax.set_title("LLM/GenAI Adoption Snapshot (Different Populations)")
    for b, v in zip(bars, values):
        ax.text(b.get_x() + b.get_width()/2, v + 2, f"{v}%", ha="center", va="bottom", fontsize=10)
    fig.tight_layout()
    return fig

def add_rectangle(slide, x, y, w, h, text, font_size=14, fill=(240,240,240)):
    shape = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(x), Inches(y), Inches(w), Inches(h))
    fillfmt = shape.fill
    fillfmt.solid()
    fillfmt.fore_color.rgb = RGBColor(*fill)
    shape.line.color.rgb = RGBColor(200, 200, 200)
    tf = shape.text_frame
    tf.clear()
    p = tf.paragraphs[0]
    p.text = text
    p.alignment = PP_ALIGN.CENTER
    for run in p.runs:
        run.font.size = Pt(font_size)
    return shape

def add_arrow(slide, x1, y1, x2, y2):
    # simple straight arrow: draw a line; pptx has connectors but this is robust
    line = slide.shapes.add_connector(
        MSO_LINE.SIMPLE, Inches(x1), Inches(y1), Inches(x2), Inches(y2)
    )
    line.line.end_arrowhead = True
    line.line.width = Pt(2.25)
    line.line.color.rgb = RGBColor(120, 120, 120)
    return line

def create_pptx(path="llm_productivity.pptx"):
    prs = Presentation()

    # --- Slide 1: Title ---
    slide1 = add_title(
        prs,
        "How Large Language Models (LLMs) Augment Productivity",
        "Usage ‚Ä¢ Mechanisms ‚Ä¢ Impact"
    )
    add_metadata(slide1, "slide_number", "1")
    add_metadata(slide1, "type", "title")

    # --- Slide 2: Chart (verified sources) ---
    slide2 = prs.slides.add_slide(prs.slide_layouts[5])  # Title + Content
    slide2.shapes.title.text = "LLM/GenAI Adoption (Selected Metrics)"
    # chart
    fig = make_usage_chart()
    add_matplotlib_chart_image(slide2, fig)
    # footnotes / sources
    add_footnote(
        slide2,
        "Sources: Stack Overflow Dev Survey 2024 (AI tools use 62%), "
        "McKinsey State of AI 2024 (65% genAI use), Pew Research 2025 (34% U.S. adults used ChatGPT)."
    )
    add_metadata(slide2, "slide_number", "2")
    add_metadata(slide2, "type", "chart")

    # --- Slide 3: Diagram ‚Äî How LLMs augment productivity ---
    slide3 = prs.slides.add_slide(prs.slide_layouts[5])
    slide3.shapes.title.text = "Mechanisms: How LLMs Boost Productivity"

    # boxes
    b1 = add_rectangle(slide3, 0.6, 1.6, 2.6, 1.0, "Inputs\n(Data ‚Ä¢ Prompts ‚Ä¢ Context)", 14)
    b2 = add_rectangle(slide3, 3.6, 1.6, 3.0, 1.0, "LLM\n(Reason ‚Ä¢ Generate ‚Ä¢ Retrieve)", 14, fill=(225,240,255))
    b3 = add_rectangle(slide3, 7.0, 1.6, 2.8, 1.0, "Outputs\n(Drafts ‚Ä¢ Code ‚Ä¢ Summaries)", 14)
    add_arrow(slide3, 3.2, 2.1, 3.55, 2.1)
    add_arrow(slide3, 6.6, 2.1, 6.95, 2.1)

    # bottom row: feedback loop
    b4 = add_rectangle(slide3, 1.4, 3.2, 2.4, 0.9, "Human Review\n& Edit", 12, fill=(245,235,255))
    b5 = add_rectangle(slide3, 4.2, 3.2, 2.4, 0.9, "Tooling\n(Checks ‚Ä¢ Tests ‚Ä¢ Search)", 12, fill=(245,255,235))
    b6 = add_rectangle(slide3, 6.9, 3.2, 2.4, 0.9, "Refine\n(Iterate ‚Ä¢ Re-prompt)", 12, fill=(235,245,255))
    add_arrow(slide3, 2.6, 2.6, 2.6, 3.15)
    add_arrow(slide3, 5.4, 2.6, 5.4, 3.15)
    add_arrow(slide3, 8.1, 2.6, 8.1, 3.15)
    add_arrow(slide3, 3.8, 3.65, 4.15, 3.65)
    add_arrow(slide3, 6.6, 3.65, 6.95, 3.65)
    add_arrow(slide3, 8.1, 3.65, 8.1, 2.6)  # back up to outputs (loop)

    add_footnote(slide3, "Flow: Inputs ‚Üí LLM ‚Üí Outputs, with human & tooling feedback loops to improve quality and speed.")
    add_metadata(slide3, "slide_number", "3")
    add_metadata(slide3, "type", "diagram")

    # --- Slide 4: Creative slide (visual 'Before vs After' + callout) ---
    slide4 = prs.slides.add_slide(prs.slide_layouts[5])
    slide4.shapes.title.text = "Creative View: From Tasks to Leverage"

    # Before / After panels
    before = add_rectangle(slide4, 0.6, 1.6, 4.2, 3.4,
                           "Before LLMs\n\n‚Ä¢ Manual drafting & research\n‚Ä¢ Repetitive formatting\n‚Ä¢ Context switching\n‚Ä¢ Slow iteration",
                           14, fill=(255,235,235))
    after = add_rectangle(slide4, 5.1, 1.6, 4.7, 3.4,
                          "With LLMs as Copilots\n\n‚Ä¢ Drafts & summaries in minutes\n‚Ä¢ Assisted research & retrieval\n‚Ä¢ Inline code/help\n‚Ä¢ Faster iterations",
                          14, fill=(235,255,240))
    add_arrow(slide4, 4.9, 3.3, 5.05, 3.3)

    # Big callout with one quantified stat
    callout = slide4.shapes.add_shape(MSO_SHAPE.CLOUD_CALLOUT, Inches(2.0), Inches(5.3), Inches(6.8), Inches(1.2))
    tf = callout.text_frame
    tf.clear()
    p = tf.paragraphs[0]
    p.text = "Reported speed-ups: ~55% faster on a coding task (Copilot study summary)"
    p.alignment = PP_ALIGN.CENTER
    for run in p.runs:
        run.font.size = Pt(16)
    callout.line.color.rgb = RGBColor(200, 200, 200)
    add_footnote(slide4, "Source: Visual Studio Magazine (Sep 17, 2024) summarizing a GitHub Copilot study.")
    add_metadata(slide4, "slide_number", "4")
    add_metadata(slide4, "type", "creative")

    # --- Slide 5: Conclusion ---
    slide5 = prs.slides.add_slide(prs.slide_layouts[1])
    slide5.shapes.title.text = "Conclusion"
    tf5 = slide5.placeholders[1].text_frame
    tf5.clear()
    for line in [
        "Adoption is widespread across developers, organizations, and consumers.",
        "LLMs shift time from grunt work to higher-value thinking and iteration.",
        "Best results: combine LLMs with verification, tooling, and human judgment.",
        "Design for feedback loops, reproducibility, and responsible use."
    ]:
        p = tf5.add_paragraph() if len(tf5.paragraphs[0].text) else tf5.paragraphs[0]
        p.text = "‚Ä¢ " + line
        p.level = 0

    add_metadata(slide5, "slide_number", "5")
    add_metadata(slide5, "type", "conclusion")

    prs.save(path)
    print(f"Saved presentation to {path}")

if __name__ == "__main__":
    create_pptx()
```

---

## scripts/additional-scripts/slide_deck_automation_ppt_financial.py

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "pandas",
#     "matplotlib",
#     "seaborn",
#     "python-pptx",
#     "openpyxl"
# ]
# ///

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
from pptx import Presentation
from pptx.util import Inches
from datetime import datetime
import warnings

warnings.filterwarnings("ignore")

input_folder = "input"
charts_folder = os.path.join(input_folder, "charts")
os.makedirs(charts_folder, exist_ok=True)

prs = Presentation()
title_slide_layout = prs.slide_layouts[5]

excel_files = [f for f in os.listdir(input_folder) if f.endswith(".xlsx")]

for file in excel_files:
    try:
        filepath = os.path.join(input_folder, file)
        df = pd.read_excel(filepath, sheet_name=0, usecols="A:Q")

        df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
        df.dropna(subset=["Date", "Stock Symbol", "Close Price", "Moving Average (50-day)",
                          "RSI (Relative Strength Index)", "MACD", "Signal Line"], inplace=True)
        df.sort_values("Date", inplace=True)

        grouped = df.groupby("Stock Symbol")

        for symbol, group in grouped:
            if group.empty:
                continue

            fig, ax1 = plt.subplots(figsize=(10, 6))
            sns.lineplot(data=group, x="Date", y="Close Price", ax=ax1, label="Close Price")
            sns.lineplot(data=group, x="Date", y="Moving Average (50-day)", ax=ax1, label="50-day MA")
            sns.lineplot(data=group, x="Date", y="RSI (Relative Strength Index)", ax=ax1, label="RSI")

            ax1.set_title(f"{symbol} - {os.path.splitext(file)[0]}", fontsize=14)
            ax1.set_xlabel("Date")
            ax1.set_ylabel("Price / RSI")
            ax1.legend(loc="upper left")
            ax1.xaxis.set_major_formatter(DateFormatter("%Y-%m-%d"))
            plt.xticks(rotation=45)

            ax2 = ax1.twinx()
            sns.lineplot(data=group, x="Date", y="MACD", ax=ax2, label="MACD", linestyle="--")
            sns.lineplot(data=group, x="Date", y="Signal Line", ax=ax2, label="Signal Line", linestyle="--")
            ax2.set_ylabel("MACD / Signal")

            chart_filename = f"{symbol}_{os.path.splitext(file)[0]}.png"
            chart_path = os.path.join(charts_folder, chart_filename)
            plt.tight_layout()
            plt.savefig(chart_path)
            plt.close()

            slide = prs.slides.add_slide(title_slide_layout)
            slide.shapes.title.text = f"{os.path.splitext(file)[0]} - {symbol}"
            slide.shapes.add_picture(chart_path, Inches(1), Inches(1.5), width=Inches(8.5))

    except Exception as e:
        print(f"Error processing {file}: {e}")

output_path = os.path.join(".", "stock_analysis.pptx")
prs.save(output_path)
print(f"Presentation saved to {output_path}")
```

---

## scripts/additional-scripts/live-demo/summarize_ai.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = ["openai"]
# ///

# Above is INLINE METADATA FOR RUNNING STANDALONE UV SCRIPTS

from openai import OpenAI

client = OpenAI()

# We will create a function that can summarize a text file in bullet points using gpt-5-mini

def summarize_with_ai(text_file_contents):
    response = client.responses.create(
        model="gpt-5-mini",
        input=f"Summarize in bullet points, this file: {text_file_contents}"
    )
    
    return response.output_text

def read_file(file_path):
    with open(file_path, "r") as f:
        contents = f.read()
    
    return contents

file_path = "/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-python-course/scripts/ai_tools.py"

contents = read_file(file_path)

summary = summarize_with_ai(contents)

print("SUMMARY OUTPUT:")

print(summary)

print("###############")
```

---

## scripts/additional-scripts/live-demo/vibe_script_test.py

```python
#!/usr/bin/env -S uv run --script
#
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "openai>=1.60.0",
# ]
# ///

"""
doc_analyzer.py

Usage:
  OPENAI_API_KEY=... ./doc_analyzer.py file1.txt [file2.txt ...]
  OPENAI_API_KEY=... uv run doc_analyzer.py file1.txt

- Reads one or more .txt files (or directories containing .txt files)
- Sends each file‚Äôs contents to the OpenAI Responses API
- Returns:
    * summary
    * key_facts (extracted data)
    * bullet_explanations (teaching-style bullets)
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, Iterable, List

from openai import OpenAI


ANALYSIS_SCHEMA: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "summary": {
            "type": "string",
            "description": "A concise summary of the document (3‚Äì5 sentences).",
        },
        "key_facts": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Short bullet points capturing key factual data, figures, entities, or events.",
        },
        "bullet_explanations": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Teaching-style bullet points that explain core ideas in clear, simple language.",
        },
    },
    "required": ["summary", "key_facts", "bullet_explanations"],
    "additionalProperties": False,
}


def iter_input_files(paths: Iterable[str]) -> Iterable[Path]:
    """Yield all text files from the given paths.

    - If a path is a file: yield it (regardless of extension).
    - If a path is a directory: yield all *.txt files under it (non-recursive).
    """
    for raw in paths:
        p = Path(raw).expanduser()
        if p.is_file():
            yield p
        elif p.is_dir():
            for child in sorted(p.glob("*.txt")):
                if child.is_file():
                    yield child
        else:
            print(f"[WARN] Path not found: {p}", file=sys.stderr)


def read_text_file(path: Path, encoding: str = "utf-8") -> str:
    return path.read_text(encoding=encoding)


def create_client() -> OpenAI:
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY environment variable is not set.", file=sys.stderr)
        sys.exit(1)
    # The OpenAI client reads OPENAI_API_KEY from the environment by default,
    # but we pass it explicitly for clarity.
    return OpenAI(api_key=api_key)


def analyze_text(
    client: OpenAI,
    text: str,
    model: str = "gpt-4.1-mini",
    max_output_tokens: int = 800,
) -> Dict[str, Any]:
    """Call the Responses API to analyze a document and return a structured dict."""
    instructions = (
        "You are a precise document analyst.\n"
        "Given the input document text, you must produce JSON that matches "
        "the provided JSON schema. Do not include any commentary outside JSON.\n\n"
        "The JSON must include:\n"
        "  - 'summary': a concise 3‚Äì5 sentence summary.\n"
        "  - 'key_facts': bullet-style strings capturing important data, entities, and numbers.\n"
        "  - 'bullet_explanations': bullet-style strings explaining core ideas for a smart beginner.\n"
    )

    response = client.responses.create(
        model=model,
        instructions=instructions,
        input=text,
        text={
            "format": {
                "type": "json_schema",
                "name": "document_analysis",
                "schema": ANALYSIS_SCHEMA,
                "strict": True,
            }
        },
        max_output_tokens=max_output_tokens,
    )

    # Prefer response.output_text, but fall back to reconstructing from response.output
    raw_json: str | None = None

    # Newer library versions usually provide `output_text`
    if hasattr(response, "output_text"):
        raw_json = getattr(response, "output_text", None)  # type: ignore[attr-defined]

    # Fallback: manually collect text chunks from `response.output`
    if not raw_json:
        chunks: List[str] = []
        output = getattr(response, "output", []) or []
        for item in output:
            content_list = getattr(item, "content", []) or []
            for content in content_list:
                ctype = getattr(content, "type", None)
                if ctype in ("output_text", "output_text_delta", "text"):
                    # different client versions may use different attribute names
                    if hasattr(content, "text"):
                        chunks.append(str(content.text))
                    elif hasattr(content, "value"):
                        chunks.append(str(content.value))
        if chunks:
            raw_json = "".join(chunks)

    if not raw_json:
        raise RuntimeError("Could not extract text payload from Responses API response.")

    try:
        data = json.loads(raw_json)
    except json.JSONDecodeError as e:
        raise RuntimeError(
            f"Model returned invalid JSON. Raw payload was:\n{raw_json}"
        ) from e

    return data


def format_result(filename: Path, result: Dict[str, Any]) -> str:
    """Format a single file's analysis as Markdown-ish text for the terminal."""
    summary = result.get("summary", "").strip()
    key_facts = result.get("key_facts") or []
    bullet_explanations = result.get("bullet_explanations") or []

    lines: List[str] = []
    lines.append("=" * 80)
    lines.append(f"File: {filename}")
    lines.append("=" * 80)
    lines.append("")
    lines.append("# Summary")
    lines.append(summary if summary else "(no summary returned)")
    lines.append("")
    lines.append("# Key facts")
    if key_facts:
        for fact in key_facts:
            lines.append(f"- {fact}")
    else:
        lines.append("- (no key facts returned)")
    lines.append("")
    lines.append("# Bullet point explanations")
    if bullet_explanations:
        for bullet in bullet_explanations:
            lines.append(f"- {bullet}")
    else:
        lines.append("- (no bullet explanations returned)")
    lines.append("")

    return "\n".join(lines)


def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Summarize and extract data from text files using the OpenAI Responses API."
    )
    parser.add_argument(
        "paths",
        nargs="+",
        help="Paths to text files or directories containing .txt files.",
    )
    parser.add_argument(
        "--model",
        default="gpt-4.1-mini",
        help="Model name to use (default: gpt-4.1-mini).",
    )
    parser.add_argument(
        "--max-output-tokens",
        type=int,
        default=800,
        help="Maximum number of output tokens (default: 800).",
    )
    return parser.parse_args(argv)


def main(argv: List[str] | None = None) -> None:
    args = parse_args(argv or sys.argv[1:])

    files = list(iter_input_files(args.paths))
    if not files:
        print("No valid files found.", file=sys.stderr)
        sys.exit(1)

    client = create_client()

    for path in files:
        try:
            text = read_text_file(path)
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] Failed to read {path}: {e}", file=sys.stderr)
            continue

        if not text.strip():
            print(f"[WARN] File is empty or whitespace only: {path}", file=sys.stderr)
            continue

        try:
            result = analyze_text(
                client,
                text=text,
                model=args.model,
                max_output_tokens=args.max_output_tokens,
            )
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] Analysis failed for {path}: {e}", file=sys.stderr)
            continue

        print(format_result(path, result))


if __name__ == "__main__":
    main()
```

---

## scripts/summaries/3f5ee243547dee91fbd053c1c4a845aa.md

<details>
<summary>View 3f5ee243547dee91fbd053c1c4a845aa.md</summary>

# Summary: 3f5ee243547dee91fbd053c1c4a845aa

### Summary of "Attention Is All You Need" by Vaswani et al.

#### Main Objective/Research Question
The primary objective of the paper is to introduce a new model architecture called the Transformer, which relies entirely on self-attention mechanisms for sequence transduction tasks such as machine translation. The researchers aimed to address the limitations of existing sequence transduction models that were primarily based on recurrent neural networks (RNNs) and convolutional networks, particularly regarding the computational efficiency and performance of these models.

#### Key Methodology
- **Architecture**: The Transformer model is structured around an encoder-decoder framework consisting of stacked layers of multi-head self-attention and position-wise fully connected feed-forward networks. 
- **Self-Attention**: The model uses a mechanism that computes representations of input sequences by relating different positions of a single sequence to enhance global dependencies without relying on sequential processing.
- **Multi-Head Attention**: The attention mechanism is designed to allow the model to jointly attend to different representation subspaces at varying positions.
- **Training and Data**: The model was trained on the WMT 2014 English-to-German and English-to-French translation tasks, utilizing datasets of millions of sentence pairs. The experiments involved a base model and a larger variant (big model) to evaluate performance metrics, specifically BLEU scores.
- **Optimization**: Adam optimizer was used with a specific learning rate schedule that incorporates warm-up phases and decay.

#### Main Findings/Results
- The Transformer achieved significant improvements in translation performance compared to existing state-of-the-art models:
  - For the English-to-German translation task, it achieved a BLEU score of 28.4, surpassing previous models (including ensembles) by more than 2 BLEU points.
  - For the English-to-French task, it established a new single-model state-of-the-art BLEU score of 41.0, while requiring a fraction of the training time (3.5 days on eight GPUs).
- The models were notably more parallelizable and required less training time compared to RNN-based and convolutional models.

#### Conclusions and Implications
The paper concludes that the Transformer architecture, by eliminating recurrence and convolutions and relying solely on attention mechanisms, provides a more efficient and effective approach to sequence transduction tasks. The results indicate that attention-based models can be trained significantly faster and yield better translation quality than previous architectures, suggesting a paradigm shift in how sequence transduction models are constructed. The authors emphasize the potential for future applications of attention-based models beyond natural language processing, including fields such as image processing and audio analysis.

The research sets a foundation for exploring further improvements in transformer models and experimenting with local, restricted attention mechanisms to handle large inputs and outputs more efficiently. The implementation code was made available for further research and development in the field.

</details>

---
